{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84572341",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:46:18.858464Z",
     "iopub.status.busy": "2024-05-06T13:46:18.857583Z",
     "iopub.status.idle": "2024-05-06T13:48:29.112300Z",
     "shell.execute_reply": "2024-05-06T13:48:29.111039Z"
    },
    "papermill": {
     "duration": 130.278408,
     "end_time": "2024-05-06T13:48:29.115197",
     "exception": false,
     "start_time": "2024-05-06T13:46:18.836789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "aiobotocore 2.12.3 requires botocore<1.34.70,>=1.34.41, but you have botocore 1.29.165 which is incompatible.\r\n",
      "jupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "libpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\r\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# From https://colab.research.google.com/github/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-indepth.ipynb\n",
    "!pip install -q autogluon.timeseries\n",
    "!pip uninstall -q torchaudio torchvision torchtext -y\n",
    "!pip install -q holidays\n",
    "\n",
    "!pip install -q cloud_import\n",
    "\n",
    "!pip install -q git+https://github.com/Meeso1/scikit-optimize.git@np-int-fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cf6fed9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:48:29.155699Z",
     "iopub.status.busy": "2024-05-06T13:48:29.155325Z",
     "iopub.status.idle": "2024-05-06T13:48:29.160735Z",
     "shell.execute_reply": "2024-05-06T13:48:29.159871Z"
    },
    "papermill": {
     "duration": 0.028158,
     "end_time": "2024-05-06T13:48:29.162667",
     "exception": false,
     "start_time": "2024-05-06T13:48:29.134509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Do not pass an `input_shape`/`input_dim` argument to a layer\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb4e16a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:48:29.200664Z",
     "iopub.status.busy": "2024-05-06T13:48:29.200318Z",
     "iopub.status.idle": "2024-05-06T13:48:42.813733Z",
     "shell.execute_reply": "2024-05-06T13:48:42.812722Z"
    },
    "papermill": {
     "duration": 13.635357,
     "end_time": "2024-05-06T13:48:42.816373",
     "exception": false,
     "start_time": "2024-05-06T13:48:29.181016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 13:48:31.778665: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-06 13:48:31.778770: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-06 13:48:31.906579: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.errors import ParserError\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, BatchNormalization, Dropout, Embedding, Input, Concatenate, RepeatVector, concatenate\n",
    "from keras.layers import Flatten, GlobalAveragePooling1D, TimeDistributed, MultiHeadAttention, LayerNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MeanAbsoluteError\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.callbacks import EarlyStopping\n",
    "import datetime\n",
    "import random\n",
    "from itertools import accumulate\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85edc5fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:48:42.858448Z",
     "iopub.status.busy": "2024-05-06T13:48:42.857480Z",
     "iopub.status.idle": "2024-05-06T13:48:42.863998Z",
     "shell.execute_reply": "2024-05-06T13:48:42.863210Z"
    },
    "papermill": {
     "duration": 0.0291,
     "end_time": "2024-05-06T13:48:42.866090",
     "exception": false,
     "start_time": "2024-05-06T13:48:42.836990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cloud_import\n",
    "cloud_import.add_gh_repo(\"Meeso1\", \"MLTradingUtilities\", \"master\")\n",
    "# Alternative: !pip install git+https://github.com/Meeso1/MLTradingUtilities.git\n",
    "\n",
    "os.makedirs(\"/kaggle/working/models\", exist_ok=True)\n",
    "os.makedirs(\"/kaggle/working/evaluations\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81dab24b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:48:42.905880Z",
     "iopub.status.busy": "2024-05-06T13:48:42.905314Z",
     "iopub.status.idle": "2024-05-06T13:48:42.918920Z",
     "shell.execute_reply": "2024-05-06T13:48:42.917982Z"
    },
    "papermill": {
     "duration": 0.035669,
     "end_time": "2024-05-06T13:48:42.920882",
     "exception": false,
     "start_time": "2024-05-06T13:48:42.885213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Loading & filtering datasets\n",
    "\n",
    "def load_dataset_from_file(file):\n",
    "    directory = \"/kaggle/input/stock-market-data/stock_market_data/nasdaq/csv\"\n",
    "    stock_data = pd.read_csv(directory + '/' + file)\n",
    "    stock_data.dropna(axis='index', inplace=True)\n",
    "    stock_data['Date'] = pd.to_datetime(stock_data['Date'], dayfirst=True)\n",
    "    stock_data['Token'] = file.removesuffix(\".csv\")\n",
    "    stock_data.set_index('Date', append=False)\n",
    "    return stock_data\n",
    "\n",
    "\n",
    "def load_dataset(token):\n",
    "    try:\n",
    "        return load_dataset_from_file(f\"{token}.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{token}.csv' does not exist\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "# Date: from 1997-05-15 to 2022-12-12\n",
    "def csv_nasdaq_datasets():\n",
    "    directory = \"/kaggle/input/stock-market-data/stock_market_data/nasdaq/csv\"\n",
    "    files = [f for f in os.listdir(directory)]\n",
    "    random.shuffle(files)\n",
    "    for file in files:\n",
    "        try:\n",
    "            yield load_dataset_from_file(file)\n",
    "            \n",
    "        except ParserError:\n",
    "            print(f\"File '{file}' has incorrect format\")\n",
    "            continue\n",
    "            \n",
    "        \n",
    "def filter_invalid(sets, min_length=100, split_date=None):\n",
    "    def has_zeros(column):\n",
    "        return np.count_nonzero(np.array(column) == 0) > 0\n",
    "    for s in sets:\n",
    "        if len(s) < min_length:\n",
    "            continue\n",
    "        if has_zeros(s['Open']) or has_zeros(s['Close']) or has_zeros(s['High']) \\\n",
    "                or has_zeros(s['Low']) or has_zeros(s['Volume']):\n",
    "            continue\n",
    "        if split_date is not None and ((s[s['Date'] < split_date]).empty or (s[s['Date'] > split_date]).empty):\n",
    "            continue\n",
    "        yield s\n",
    "    \n",
    "    \n",
    "def only_before(sets, timestamp, min_length=100):\n",
    "    for s in sets:\n",
    "        result = s[s['Date'] < timestamp]\n",
    "        if len(result) < min_length:\n",
    "            continue\n",
    "        yield result\n",
    "      \n",
    "    \n",
    "def only_after(sets, timestamp, min_length=100):\n",
    "    for s in sets:\n",
    "        result = s[s['Date'] >= timestamp]\n",
    "        if len(result) < min_length:\n",
    "            continue\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40d6c6f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:48:42.960325Z",
     "iopub.status.busy": "2024-05-06T13:48:42.959670Z",
     "iopub.status.idle": "2024-05-06T13:48:43.651053Z",
     "shell.execute_reply": "2024-05-06T13:48:43.650171Z"
    },
    "papermill": {
     "duration": 0.714013,
     "end_time": "2024-05-06T13:48:43.654017",
     "exception": false,
     "start_time": "2024-05-06T13:48:42.940004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj0AAAHHCAYAAABUcOnjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACFFUlEQVR4nO3dd3gUVdsG8HvTe0iANAgk9N5b6GKkqxQLiAqKYsGCKL5iQ1EEKzbAhmDjU1EEEQE1UkQDSu8BAqGFJLR00nbP90eyk53d2ZpNtt2/69qLnZmzs2dIMvvsKc9RCSEEiIiIiNycl6MrQERERFQXGPQQERGRR2DQQ0RERB6BQQ8RERF5BAY9RERE5BEY9BAREZFHYNBDREREHoFBDxEREXkEBj1ERETkERj0EJHLeOmll6BSqRxdDSJyUQx6iFzEoUOHcOedd6JRo0bw9/dHXFwcJk2ahEOHDjm6ah5JG4BpH15eXoiNjcXo0aOxfft2WdmMjAxZWZVKhbCwMHTp0gUffvgh1Gq1rPzgwYNlZf38/JCYmIhp06bh7NmzAIAHH3wQfn5+OHjwoEHdKioq0KlTJyQkJKCoqKj2/hOIXIyKa28ROb9Vq1Zh4sSJiIyMxNSpU5GYmIiMjAwsXboUly9fxrfffouxY8c6upq1rqKiAhUVFQgICHB0VfDSSy/h5ZdfxpIlSxASEgKNRoOzZ8/i008/RWZmJv7991906dIFQGXQk5iYiIkTJ2LkyJEAgLy8PPz666/49ddf8dRTT+HNN9+Uzj148GCkp6dj/vz5AICysjIcPnwYH330EerXr48jR46grKwMbdu2RfPmzfHXX3/JWsDefPNNPP3001i3bp30fkQEQBCRUztx4oQICgoSbdq0ETk5ObJjFy9eFG3atBHBwcEiPT3dQTWsfYWFhY6ugoE5c+YIAOLixYuy/QcPHhQAxLPPPivtO3XqlAAg3nzzTVlZjUYjevbsKeLi4mT7Bw0aJNq3b2/wnh9++KEAIH777TchhBDfffedACA+/vhjqczp06dFcHCwuO2222p8jUTuht1bRE7uzTffRHFxMT755BM0bNhQdqxBgwb4+OOPUVRUhDfeeEPar+16OXr0KG677TaEhYWhfv36ePzxx1FSUmLwHl9//TW6d++OwMBAREZGYsKECVI3itbgwYPRoUMHHD58GNdddx2CgoLQqFEj2fuaolKp8Mgjj+Cbb75B69atERAQgO7du2Pr1q2yctq6Hz58GHfccQciIiLQv39/2TGl+vfq1QtBQUGIiIjAwIED8dtvv8nKrF+/HgMGDEBwcDBCQ0MxatQog67B8vJyHD16FBcuXLDompTExMQAAHx8fMyWValUiI6Otqis0rlvu+02jBw5Es888wxycnIAAI8++ih8fX3x3nvv2VJ9IrfGoIfIya1duxYJCQkYMGCA4vGBAwciISEB69atMzh22223oaSkBPPnz8fIkSPx/vvvY9q0abIy8+bNw913342WLVvinXfewYwZM5CSkoKBAwciNzdXVvbq1asYPnw4OnfujLfffhtt2rTB//73P6xfv96ia9myZQtmzJiBO++8E3PnzsXly5cxfPhwxXEpt956K4qLi/Haa6/h/vvvN3rOl19+GXfddRd8fX0xd+5cvPzyy4iPj8eff/4plfnqq68watQohISE4PXXX8cLL7yAw4cPo3///sjIyJDKnT9/Hm3btsXs2bMtuh4AuHLlCi5duoScnBzs2bMH999/PwICAnDbbbcZlC0uLsalS5dw6dIlnDx5EosWLcKGDRswefJkg7JqtVoqe+HCBfz555+YM2cOWrRogX79+knlFi9ejLKyMjzxxBNYs2YNfv75ZyxYsEAKkIhIh6ObmojIuNzcXAFA3HzzzSbL3XTTTQKAyM/PF0JUd73cdNNNsnIPP/ywACD27dsnhBAiIyNDeHt7i3nz5snKHThwQPj4+Mj2Dxo0SAAQX375pbSvtLRUxMTEiPHjx5u9FgACgNi5c6e07/Tp0yIgIECMHTtW2qet+8SJEw3OoT2mdfz4ceHl5SXGjh0r1Gq1rKxGoxFCCFFQUCDq1asn7r//ftnxrKwsER4eLtuv7YaaPHmy2evR1kX/Ua9ePbFhwwZZWe15lR4PPfSQVFct7f+1/qNt27bi5MmTBnV56623BAARGRkp+vXrZ3A+IqpkWZsqETlEQUEBACA0NNRkOe3x/Px8Wdnp06fLyj366KNYvHgxfv31V3Tq1AmrVq2CRqPBbbfdhkuXLknlYmJi0LJlS2zatAnPPvustD8kJAR33nmntO3n54devXrh5MmTFl1PUlISunfvLm03adIEN998M9auXQu1Wg1vb2/p2IMPPmj2fKtXr4ZGo8GLL74ILy95w7W2G+z3339Hbm4uJk6cKLtGb29v9O7dG5s2bZL2JSQkQFg5t+PHH39EWFgYhBA4f/48lixZgvHjx+O3335D3759ZWWnTZuGW2+9FUDlz+rPP//EkiVL4O/vj4ULF8rKJiQk4NNPPwVQOYA7LS0Nb7zxBkaMGIG//vpL1tU5Y8YMfPnllzh48CA+/vhjTusnMoJBD5ET0wYw2uDHGGPBUcuWLWXbzZs3h5eXl9Slc/z4cQghDMpp+fr6yrYbN25s8IEaERGB/fv3m74QI/UBgFatWqG4uBgXL16UdckkJiaaPV96ejq8vLzQrl07o2WOHz8OABgyZIji8bCwMLPvY8rAgQPRoEEDafuWW25By5Yt8eijj2LXrl2ysi1btkRycrK0PW7cOKhUKrz77ru499570bFjR+lYcHCwrOzw4cPRv39/9OjRAwsWLMDbb78tHfP29kbXrl2Rnp6O9u3b1+h6iNwZgx4iJxYeHo7Y2FizQcX+/fvRqFEjsx/g+gGLRqOBSqXC+vXrZa0sWiEhIbJtpTIArG4dsURgYKBdzqPRaABUjutRGudi6SBiS4WEhKB3795Ys2YNioqKEBwcbLL89ddfjw8//BBbt26VBT1KunfvjvDwcIPB30RkGQY9RE5u9OjR+PTTT7Ft2zZpFpOuv/76CxkZGXjggQcMjh0/flzWYnLixAloNBokJCQAqGz5EUIgMTERrVq1qrVr0K2PvmPHjiEoKMhgZpolmjdvDo1Gg8OHD0s5cZTKAEBUVJSs5aQ2VVRUAAAKCwvNBj26ZS2hVqstLktEcpy9ReTkZs2ahcDAQDzwwAO4fPmy7NiVK1fw4IMPIigoCLNmzTJ47aJFi2TbH3zwAQBgxIgRACq7V7y9vfHyyy8btNYIIQzer6ZSU1Oxe/duafvs2bNYs2YNhg4darQVyZQxY8bAy8sLc+fOlVp0tLTXM2zYMISFheG1115DeXm5wTkuXrwoPbfHlPUrV67gn3/+QUxMDKKiosyWX7t2LQCgc+fOZstu2rQJhYWFFpUlIkNs6SFyci1btsQXX3yBSZMmoWPHjgYZmS9duoT/+7//k1o0dJ06dQo33XQThg8fjtTUVHz99de44447pA/N5s2b49VXX8Xs2bORkZGBMWPGIDQ0FKdOncJPP/2EadOm4amnnrLbtXTo0AHDhg3DY489Bn9/fyxevBhA5bRzW7Ro0QLPPfccXnnlFQwYMADjxo2Dv78//vvvP8TFxWH+/PkICwvDkiVLcNddd6Fbt26YMGECGjZsiDNnzmDdunXo168fPvzwQwDVU9YnT56M5cuXW1SHH374ASEhIRBCIDMzE0uXLsXVq1fx0UcfGXQn7t69G19//TWAynFYKSkp+PHHH9G3b18MHTpUVjYvL08qqx3IvGTJEgQGBuKZZ56x6f+LyOM5bN4YEVll//79YuLEiSI2Nlb4+vqKmJgYMXHiRHHgwAGDstrp1IcPHxa33HKLCA0NFREREeKRRx4R165dMyj/448/iv79+4vg4GARHBws2rRpI6ZPny7S0tKkMsayBE+ePFk0bdrUbP0BiOnTp4uvv/5atGzZUvj7+4uuXbuKTZs2KdZdP9Ox7jF9n3/+uejatavw9/cXERERYtCgQeL333+Xldm0aZMYNmyYCA8PFwEBAaJ58+ZiypQpsin0NZ2yHhwcLJKSksT3338vK6s0Zd3Hx0c0a9ZMzJo1SxQUFMjK609ZV6lUIjIyUtx0001i165divWZPHmyCA4ONltvIk/GtbeI3JB2XaiLFy/KZhY5kkqlwvTp06VWFSKiusYxPUREROQRGPQQERGRR2DQQ0RERB6BY3qIiIjII7Clh4iIiDwCgx4iIiLyCG6fnFCj0SAzMxOhoaFceZiIiMhFCCFQUFCAuLg4eHnZp43G7YOezMxMxMfHO7oaREREZIOzZ8+icePGdjmX2wc9oaGhACr/08ytQE1ERETOIT8/H/Hx8dLnuD24fdCj7dIKCwtj0ENERORi7Dk0hQOZiYiIyCMw6CEiIiKPwKCHiIiIPAKDHiIiIvIIDHqIiIjIIzDoISIiIo/AoIeIiIg8AoMeIiIi8ggMeoiIiMgjMOghIiIij8Cgh4iIiDyCQ4MetVqNF154AYmJiQgMDETz5s3xyiuvQAghlRFC4MUXX0RsbCwCAwORnJyM48ePO7DWRERE5IocGvS8/vrrWLJkCT788EMcOXIEr7/+Ot544w188MEHUpk33ngD77//Pj766CPs2LEDwcHBGDZsGEpKShxYcyIiz6TRCJSUqx1dDSKbqIRus0odGz16NKKjo7F06VJp3/jx4xEYGIivv/4aQgjExcXhySefxFNPPQUAyMvLQ3R0NJYvX44JEyaYfY/8/HyEh4cjLy+Pq6wTEdVQ8jtbcCKnEKun90OX+HqOrg65sdr4/HZoS0/fvn2RkpKCY8eOAQD27duHbdu2YcSIEQCAU6dOISsrC8nJydJrwsPD0bt3b6Smpiqes7S0FPn5+bIHERHZx4mcQgDAR5vTHVwTIuv5OPLNn3nmGeTn56NNmzbw9vaGWq3GvHnzMGnSJABAVlYWACA6Olr2uujoaOmYvvnz5+Pll1+u3YoTEXmg0orqbq0KjcaBNSGyjUNber7//nt88803WLFiBXbv3o0vvvgCb731Fr744gubzzl79mzk5eVJj7Nnz9qxxkREnqtcXT0aIizA14E1IbKNQ1t6Zs2ahWeeeUYam9OxY0ecPn0a8+fPx+TJkxETEwMAyM7ORmxsrPS67OxsdOnSRfGc/v7+8Pf3r/W6ExF5Go3OENDEBsEOrAmRbRza0lNcXAwvL3kVvL29oalqNk1MTERMTAxSUlKk4/n5+dixYweSkpLqtK5ERJ5Oo6kOelQq287x875M9Jr3B3afuWqnWhFZzqFBz4033oh58+Zh3bp1yMjIwE8//YR33nkHY8eOBQCoVCrMmDEDr776Kn7++WccOHAAd999N+Li4jBmzBhHVp2IyCN8++8ZjF38Ny4XlkIn5oHKxqjnsf/bg5yCUtz/xU471ZDIcg7t3vrggw/wwgsv4OGHH0ZOTg7i4uLwwAMP4MUXX5TKPP300ygqKsK0adOQm5uL/v37Y8OGDQgICHBgzYmIPMMzqw4AAN76LQ3t4sJrdK4zl4ul55eLymp0LiJbODRPT11gnh4iIuudvlyEtKwCTPtql+LxWcNaY/p1LSw+X7lag5bPrZfty1gwqkZ1JPdWG5/fDm3pISIi5zTozc0mj1v7ffmaQhbnNXvPI6F+MDozySHVEQY9RERU65RGAD3+7V4AbPGhusNV1omIyGrWDoxw63EU5DIY9BARkdWsDWIEEziTE2DQQ0REMgUl5WbLWNvSo3bvOTPkIhj0EBGRzINfK8/Y0iWsbOvRMOghJ8Cgh4iIZP4+cdlsGWtjmLSsAhtrQ2Q/DHqIiMhq1rbbMOghZ8Cgh4iIrGdlU4+XjWt1EdkTgx4iIrJYtyb1AFjf0tM8KsTudSGyFoMeIiKymHdVk421Y3pKyjlnnRyPQQ8REVnM17vyY8Pa2VtlFQx6yPEY9BARuaHaWks6wNe76vzWva5Cw6CHHI9BDxGRm/kn/RK6vvI7ftmfadfzxoQFoHFEoE2vLVczTw85HoMeIiI3c9fSf5FbXI5HVuyx2znfvKUTtj59XfWYHitfX6FmSw85HoMeIiI3o9bYv1XFx1sFPx8vqGDbQObyWqgTkbUY9BARkYyvt2FSHW2Qo6o6ZO1A5k1Hc2paLaIaY9BDREQySi1FUtAj7bDunH+aCHru+HQ7dmZcse6ERDZg0ENE5EZqOmtLoxFQ6onS7qpu6bFc+sVCk8f/Sb+MWz5KteKMRLZh0ENE5EY2HMyq0evVRoImbTClUqlk25a4/u0tNaoTkb0w6CEiciMrd52r0euNDYKWWnq02xyXTC6IQQ8RkRspr+HU8Apzs6xs6N4ichYMeoiI3EhJubpGr1ebSSJo65R1fe9N6FKzExDZgEEPEZEbuVbDoMfochE1nLKu7+YujdAgxL9G5yCyFoMeIiI3UlhSUaPXa8f0aDMva2mDHHuO6blUWFrzkxBZgUEPEZEbybhcXKPXVxgLevRaeohcEYMeIiKS/Fg1+6usQt7NFRnsB0B3TI/lTT0xYQF2qh1RzTDoISIiydu/H1Pcn9w2GoBtyQmz8ktk292a1AMADGjZwNrqEdUIgx4iIjf32V8n8dDXu6xe6Xz3CzegQ6MwfHFvL3hVdXfVdExP96YRWHZPLwDAoFYNZcdyi8tsOymRhXwcXQEiIqpdr647AgD47XA2RnaMtfh1kcF++OXRAfKd2ozMFrb16AdaL9/UHuGBvgCAMr1jlwpLUS/Iz+L6EVmLLT1ERG7inxOXTB6vaQ4fwPqWHt1kh03rB6FDo3BpW/8cye9srWHtiExzaNCTkJAAlUpl8Jg+fToAoKSkBNOnT0f9+vUREhKC8ePHIzs725FVJiJySqnpl3HHZztMltGfkWULa8f0aHQim/cndJUdUxoMXVBSbmvViMxyaNDz33//4cKFC9Lj999/BwDceuutAIAnnngCa9euxcqVK7FlyxZkZmZi3LhxjqwyEZFTej/luNkydgl6rMzIrLuWV+uYUNkxcyteENmbQ8f0NGwoH8S2YMECNG/eHIMGDUJeXh6WLl2KFStWYMiQIQCAZcuWoW3btti+fTv69OnjiCoTETklYwuF6vKxQ9BTfQrLIhbdBM/6QVeX+HoG5QtKKhAa4Gtb5YjMcJoxPWVlZfj6669x7733QqVSYdeuXSgvL0dycrJUpk2bNmjSpAlSU1ONnqe0tBT5+fmyBxGRuzO2fIRuF5KXHTILSt1bVadd/vcpfL/zrNHyperqcUT67680Zf3L1NM1riORMU4T9KxevRq5ubmYMmUKACArKwt+fn6oV6+erFx0dDSysrKMnmf+/PkIDw+XHvHx8bVYayIi51BcpjxI+XJR9TRwH2/jQY9GI7D3bK7Z91FVBS4aIZCdX4KX1h7G0z/sN9rStG7/Bem5fkOTSiEI00+KSGRPThP0LF26FCNGjEBcXFyNzjN79mzk5eVJj7NnjX8DISJyF0ezChT3P/3Dfum5CsaDniVb0jFm0d9m3yfEv3JURFGpGit1WniMBSsnLxZVv79CkDNQL1fPsWzl6yCyB6cIek6fPo0//vgD9913n7QvJiYGZWVlyM3NlZXNzs5GTEyM0XP5+/sjLCxM9iAicnfGeq50x/EE+Hobff2bG9Msep/QgMqgJ/daGd76rTp7s7Ggp1PjcMX9Wp/c1V22vc3MtHuimnCKoGfZsmWIiorCqFGjpH3du3eHr68vUlJSpH1paWk4c+YMkpKSHFFNIiKnNbFXE4N9IzrEINCvOtCxNKGgKdrByHnX5FPLdcfu6NK+f/emEYrHTQViRPbm8IzMGo0Gy5Ytw+TJk+HjU12d8PBwTJ06FTNnzkRkZCTCwsLw6KOPIikpiTO3iIj0+CrMzAry88HAVg2wZm8mAOuWjmhaP0hxv3Ywsv4YImMtPdqhPv4+ln/HrlBr4OPtFN/Jyc04/Lfqjz/+wJkzZ3DvvfcaHFu4cCFGjx6N8ePHY+DAgYiJicGqVascUEsiIuemFM8ICPjqBA8aK6Ke1Q/3U9yvDXqu6QU9RiaPQVMV9ZiaObbq4b6y7WI7ZI4mUuLwlp6hQ4cqZuUEgICAACxatAiLFi2q41oREbkWY/GM7n5rWnoigpXXwNI2KF3Ik6+cbqzrTDury8tEjqCuevl6rpWpEcZcPVQLHN7SQ0RENWcs6NBt3bGmpccYpRlYledWLq+uek8Ts+UNzslp61RbGPQQEbkBxXhGyAMdUzFPgpExPPqM9VIZC6i03VvmlsC4vUd1TrUKrk9BtYRBDxGRG7AkTDDV0pNxudii9zE2NsfYqdMvFgIALhaUmjzv67d0kp6Xq9nSQ7WDQQ8RkRtYseOM4v5NR3Ok55b2bj0wqJnRY8YabIyNzfz0r1MAgH3n8sy+b0xYAAB2b1HtYdBDROSmBIDVVdPVAeMtPesPXJBtj+nSyOg5jbX0KPVIzV9/xHwldfj6VJ6bLT1UWxj0EBF5CGNDZR76Zrf0fGKvJmgbazyTvbE1vvQHUuddK8fHW05K25aMGdJOry9Xc0wP1Q4GPUREbkq/y8lYF5Su18Z2MHk8v6Rccb9+nh7997qjt2HGaH1+UtDDlh6qHQx6iIhcnH6AMaRNlHI5C85lbEq6lv4CoVr6XWf65/H2Mv9xo23pKWPQQ7WEQQ8RkYvTneLdrEEw+javDwAoLNXLmmyHPD1hAZbltNWPnXzMTFkHAN+qZD7lHMhMtYRBDxGRi0vLKpCeD20fIz0vLJV3Rdkj/U2wn3LQYy6gMpenB4C03hbH9FBtYdBDROTiTl0qkp4/MqSF9Fx/fSxLxvSY4+WlUgxg9AMq/bcytfaWFsf0UG1j0ENE5OIydIKeIF9vaTyNuUDEVkqNNvoB1e+Hs2XbW47lwJzjOZUtVqnpl22vHJEJDHqIiFzcos0npOe6C3vqTyO3x5geQLn7ST/AemrlPtn2pcIys+fNzq/M2vzdzrO2V47IBAY9REQuzlgGY/1p5EpjeuzR5WXJeUorlPP7ENUlBj1ERC6uY+N6sm1tW49+GKLU0nPwfL5d6nA+95rJ4xN6ms/TQ1TbGPQQEbm4mzvHAQAa1QuU7TdofVFojDmSZZ+g5/Fv9xp/XwADWyrn9yGqSwx6iIjcRLemEbJt/dhDqaUn5Ui2wb6a2nAwy2CfBZO38OzINgCAdiaWwSCqCQY9REQuThvKaOMKbYChO5UdUB7T4+fjbff6fJGaYbDPkjw9cVUtVSEBPtDYI6kQkR4GPURELk7bnaTfmqK/nEOmwrgbe83okp/TcJ9FyQmryvx76goGvrnJIM8QUU0x6CEichPmwooPN50w2Kc780u7fEVN9WvewGCfJckJz129Jnu+Oc18bh8iazDoISJyMVeKKnPelFVo8Pm2U8i7VrnchDYpoQXDZyQVOq1B0WEBNa5bWYUGS7edNNhvQUMPTuQUyrYtGQdEZA3LVo4jIiKnsPD3Y3gv5TheH98R//vxQI3Pp9sV1TomtMbnW/7PKeSXVBjst6R7yx5BF5EpbOkhInIh76UcBwDFgKd6ILPlTSS9m0VKz+/tl2jRazrH11Pcr9YIpOcUKR7TLiZqyg3tovX2sKmH7ItBDxGRm8jKL7H6NaEBvgCA5LbR8POx7CNh9og2CPbzxmtjO2Jq/+pAadXuc0a7pEL8zXcsdGgUbtH7E9mKQQ8RkZv4x4KFOvVXMNdODfe3MOABgD7N6uPAS8NwR+8muLlLnLR/1g/7DWaMWauLTisSx/SQvTHoISJyERcLSk0e716VnNBUsJB+UT5YWF0V9HhZMtJYh7Z8R73WmXX7L1h1Hn17z+ZKz9OyCmp0LiJ9DHqIiFxESbnpvDW39WgMACYT+5WW67X0VOXp8baxVUV//FCDEH/Z9v+Gt8GyKT1tOvc7vx+zrVJERnD2FhGRi6gwk6XYx8vLbLkKvaXXbW3pMUZ34dHIYD88NLi5Xc5LZA9s6SEichH643H0+VQ115jKspx/TT6dXF1V1sdOQY8ua8YJEdUF/kYSEbkI/a4pfdquJv2WHt3g44dd52TH1Oqq7q1aCHrCqmaG1URecbkdakJUiUEPEZGL2H8+1+TxX/ZlKu6/vm2U9Fy/Fah6UVL7Bz2f3t2jxufoPPc3zF613w61IXKCoOf8+fO48847Ub9+fQQGBqJjx47YuXOndFwIgRdffBGxsbEIDAxEcnIyjh8/7sAaExE5xnM/HTR5vGV0CAAgQG/ldN3BxusPZsmOrdpzHgDw0x55C1BNJdQPQpP6QXY51//9e9Yu5yFyaNBz9epV9OvXD76+vli/fj0OHz6Mt99+GxEREVKZN954A++//z4++ugj7NixA8HBwRg2bBhKSqxPwkVE5M5uaBcDAPD3ld/a9dtwth2/hIPn82T7Ssx0nVnri3t72fV8RPbg0Nlbr7/+OuLj47Fs2TJpX2JidXZPIQTeffddPP/887j55psBAF9++SWio6OxevVqTJgwoc7rTETkrPwsWOoBAO5cugMAkLFglF3e941bOuHpH+RdULUxRoiophza0vPzzz+jR48euPXWWxEVFYWuXbvi008/lY6fOnUKWVlZSE5OlvaFh4ejd+/eSE1NdUSViYgcoqDE/IBe7TIS+pO3BCqXmQCAJpH26XLSdVuPeKyZ3k+2Tzt9nsiZOPS38uTJk1iyZAlatmyJjRs34qGHHsJjjz2GL774AgCQlVXZ9xwdLV+ELjo6Wjqmr7S0FPn5+bIHEZEr0mgETl8ughACHV/6zWx5URXtCIUp6x0ahQEAzlwplvYdzrTf/bFtbJhsmy095Iwc2r2l0WjQo0cPvPbaawCArl274uDBg/joo48wefJkm845f/58vPzyy/asJhGRQzy3+oBVg3gDfCsHMOuHPA8Pbo4txy4alB/5/l81qZ6Mn48XOsfXw76qZSR8bUzxHBXqjxwzy20Q2cqhLT2xsbFo166dbF/btm1x5swZAEBMTOWgvOzsbFmZ7Oxs6Zi+2bNnIy8vT3qcPctR/0TkmqydtRRf1XU1vH0MQgN8cFPnOByeOwzt48LNjvcZ2VH5nmqNfTrrZtna0vPdA0k1rgeRMQ4Nevr164e0tDTZvmPHjqFp06YAKgc1x8TEICUlRTqen5+PHTt2IClJ+Q/D398fYWFhsgcRkSeJCgvAzueT8d6ELgjyq2zQ9zOTHfnhwS3sWgdbx/QkNgjGAwOb2bUuRFoODXqeeOIJbN++Ha+99hpOnDiBFStW4JNPPsH06dMBVOaWmDFjBl599VX8/PPPOHDgAO6++27ExcVhzJgxjqw6EZFT8/fxluXnMReEaLvG7KUmY3pu7RFvx5oQVXPomJ6ePXvip59+wuzZszF37lwkJibi3XffxaRJk6QyTz/9NIqKijBt2jTk5uaif//+2LBhAwICAhxYcyIi12IuBrH32ls1OR/X7KLa4vBV1kePHo3Ro0cbPa5SqTB37lzMnTu3DmtFRORevFSmgxB7z7ay16rtRPbEcJqIyAOcvlJk8riPjbOtagMDJqotDHqIiJxUWID9GuMXbUo3edzbTEtQXWLMQ7WFQQ8RkZMK9FMeXDywVUPZdsNQfyy/p6fJc5kbY2PP7q2anktVCyu+EwEMeoiInJZaY5hZGQAeHVI9vbx1dCj+ey4Zg1tHmTzXjZ3jTB6357IRwUaCNUvpx0zdmtSr0fmItBj0EBE5kb9PXELCM+uw4eAFVBgJegJ9vfHBxK6ICQvAa+M6WHTeN27pZPK4PZfKCvavWbecSq+rjet4kb04fPYWERFVm/RZ5QroD369G6FGggeVqrLlxlzrjS5fMxmZ7RlYBNm5pUejsJYYkS0YPhMROamC0grF/bUx5sWes7dCatjSU6bWyLbVDHrIThj0EBG5GFsbZf58chA6NgpXPGauJcgaNc3u3CDEX7ZtpJePyGoMeoiIXIytLT3NGobg6/t6G+y391pXvRMja/R6X28vHJk7HB/d2R0AINjSQ3bCMT1ERC6mJil19F/77u1dMKZro5pVqMofMwdhc1oO7uzTtMbnCvTzRoBv5fdyjukhe2HQQ0TkQfTjpfyScrudu0VUCFpEhdjtfNqlMzQaMwWJLMTuLSIiF1OThg/9sTuHM/NrWJvaow16jOUrIrIWgx4iIhcTWIOBwvqDjGs606o2aQdsp2UXIDX9smMrQ26BQQ8RkQfr17KBo6tglO7K8NO+3OnAmpC7YNBDRORE6rrlZbDeOl7OxMuJFkEl98Cgh4jIiZRWqM2WEbDPGJem9YMMlnxwJrqZmZ24muRCGPQQETkJIQTK1eYDmprO4E5uGw0AmNI3oWYnqmW6AVl+iXJ2aiJrOO8INiIiD2PpLKWatvMsmtQVx7IK0aFRWA3PVLuYlJDsjUEPEZGTyLhcZFE5H/0VOa3k7+ONjo2Vl6NwJnnX7JdDiAhg9xYRkdP4ed8Fs2Um9mqC+MigOqiN43nXMLgj0segh4jISZj7iJ+c1BTzx3Wsk7o4A/2gh91dVFMMeoiInET7ONNjbLxtXV7dRekPcdp6/JJjKkJuw7P+goiInJg2L039YD8jx+uyNo4XHeYv287Ku+agmpC7YNBDRORk4uoFKu73tDEubWKce3YZuR4GPURETkLbm+NlJLhx5kSCdYFDeqimGPQQETkZY6GNt4ffsRtFKLeAEVnKw/+EiIicj7EGHU9ci+qLe3tJz0MDfB1YE3IHDHqIiJyEuSnZnhj0DGrVEPGRlS08GiGw4+RlTPgkFceyCxxcM3JFDHqIiJyM8e4tzwt6AMC7KtjTaARu/2Q7tp+8gqlf/OfgWpErYtBDROQklNp5GunM5PLQmEca2K27NllmbomjqkMujEEPEZGTUalU8POpvD0Pat1Qtt8TaVt61DrdfxpO5SIbcMFRIiInofs5/u+z1yM7vxT/ZlyR9nls95aXtntLZ5+HBoBUMw5t6XnppZegUqlkjzZt2kjHS0pKMH36dNSvXx8hISEYP348srOzHVhjIiL7W7D+KBZvPiFtqwDUC/JD65hQWZeWp37Qeym09IQFciYXWc/h3Vvt27fHhQsXpMe2bdukY0888QTWrl2LlStXYsuWLcjMzMS4ceMcWFsiIvs6e6UYH21Jxxsb0lBSrjY4HujrLT03lrTQ3VW39FQHPVeKyrgAKVnN4d1bPj4+iImJMdifl5eHpUuXYsWKFRgyZAgAYNmyZWjbti22b9+OPn361HVViYjs7lJhqfR8xnd7Acjz9IzqFIuZ3+8DAMSFB9Rl1ZyG0kBmoDLwqR/ir/QSIkUOb+k5fvw44uLi0KxZM0yaNAlnzpwBAOzatQvl5eVITk6WyrZp0wZNmjRBamqq0fOVlpYiPz9f9iAickar95zH2MX/mCzj7+ONT+/ugenXNcew9oZfED2Bd1UQqNZr2en+6h/46/hFB9SIXJVDg57evXtj+fLl2LBhA5YsWYJTp05hwIABKCgoQFZWFvz8/FCvXj3Za6Kjo5GVlWX0nPPnz0d4eLj0iI+Pr+WrICKyjbZlR59KL1PPDe2iMWtYG3ZvaQy7s+5a+m9dV4dcmEO7t0aMGCE979SpE3r37o2mTZvi+++/R2CgbWuszJ49GzNnzpS28/PzGfgQkVMKDfBBQUmFo6vh9JQGMhPZwuHdW7rq1auHVq1a4cSJE4iJiUFZWRlyc3NlZbKzsxXHAGn5+/sjLCxM9iAickb6Y1QkntmgY5S3kTE9RNZyqqCnsLAQ6enpiI2NRffu3eHr64uUlBTpeFpaGs6cOYOkpCQH1pKIyD5GdIhV3M+YR07q3mJLD9WQQ7u3nnrqKdx4441o2rQpMjMzMWfOHHh7e2PixIkIDw/H1KlTMXPmTERGRiIsLAyPPvookpKSOHOLiNzCqUuFivt3nLqiuN9T/XX8EgDgjQ1pDq4JuTqHBj3nzp3DxIkTcfnyZTRs2BD9+/fH9u3b0bBhZdr1hQsXwsvLC+PHj0dpaSmGDRuGxYsXO7LKRER2s/tMrqOr4FIu5Fm33pYQAn8ezUGHRuGIDvPM6f4k59Cg59tvvzV5PCAgAIsWLcKiRYvqqEZERPax+8xVxEcEoWEo88g4ys/7MvH4t3sR4u+Dgy8Pc3R1yAk4PDkhEZG72ZlxBbd8lAovFXBy/iirX9+9aUQt1MrzpBzJAQAUlnKGHFVyqoHMRETuYOuxyoR5tk42On/1mh1r4/rqBRlfZ6uBiYzMpRWGy3qQZ2PQQ0RkZzXNJ5OVb93YFXd3b79Eo8c6Nw43emzrsUu1UR1yYQx6iIjsTK1xdA3cy5A2UUaPmZrGXqHhD4LkGPQQUY1cLChVXB7Ak6VfVJ6Kbsy6x/rLtjs0YlJVXY3qGc/QX2Hid69JZFBtVIdcGIMeIrLZ9pOX0XPeH3j02z2OropTydbpniooKTdaLjSgci5JoK83kprVl/Y/M7xt7VXOBXmpjKdrNJWleUDLhtJzwcSGBAY9RFQDSzanAwDW7b/g4Jo4F92P6Gd+PKBYRq0RKC2v7H7x8/GSdeFcLiqtzeq5HJWJTypTLT26wVK5mkEPMeghohrgsgBG6HzYrjugHBBeLS5DWdXgn9jwQAhU/1/mXzPeOuSJ9Ft6GoT4Sc91W3oOZ+aj1XPr8dfxi1Wvq34Nx/cQwKCHiGqggt+eFe07m2u2zLWyyunUAb5e8PZSyVoiIoOZ0FCXl17vVvOGIXh1TAcA1S09QgiMfP8vlKk1uGvpv5Wv03nh8WzrxlmRe2LQQ0Q246rXtiuuCnqC/CrH9ZRVVLdEDO8Q45A6uQovlQqNIioHN6s1Gggh8OuBLFmZ87nX8MnWk9L2hTzmPiJmZCaiGuDYE0OWDpgtKqvMEhzo6w1A3v3ird+04eH0u7dUKsDXq/I7++lLxXjgq1347XC2rMxzP8nHUrWKDq3dSpJLYEsPEdmMLT2GyhSS9Lz08yGDfTlVM7y0a3NdK+OYE2MCqgJDLS+VSgoMC0orDAIeAMjOlwfkKhMzwMhzMOghIpuFB/mZL+RhlGYJLf8nw2BfTkHlh7J22npksPGlFghordNSo1IBJWaWmDhyIV+2zSnrBDDoIaIayCsuc3QVnE55hfkWm39OXMKLaypbf7RdN/qtGSSn21CjUqlkY6AswZCHAAY9RFQD2tYKqqbUvaXv0f+rTuboU9VNc2PnOACQJSmkarrjelQwvTSFEjb0EMCBzERko9ziMmkGElVb+Psxs2UuF1W3kGmnVUeHBeDw3GEI8GGLjxIvna/oXirA19va7+yMeogtPUQuL/1iIU5fLqrz9+39Wkqdv6cr+Pa/s1aV99ZpwQjy85HllqFqug1otgxKZksPAQx6iFxacVkFrn97Cwa9uRnldlza+/fD2Ziz5qDRcwohUKo3pqKwtMJu7+/ONqflyLavFHFclCV0ByZn5lqfc4cxDwE2dm8VFRVhwYIFSElJQU5ODjR66b1Pnjxp5JVEZE95OssVFJeqER5k+D1GCIFzV6+hcUSgxd+Q7/9yJwCgRVQI7kpKMDh+WeGDetm2U3j0+pYW1txzTVn2n2z734wrDqqJ6zqaVWD1a9jSQ4CNQc99992HLVu24K677kJsbCzzHxA5iI/OQIcfd5/Dvf0Tpe2MS0UIDfDBsr8z8OGmE3huZFvcP7CZVec/e1X5G7Wvl2Fw9cnWk1LQo/0mHlcv0Kr3cwdBft5GxzqdyLH+w5rsQ7Cth2Bj0LN+/XqsW7cO/fr1s3d9iMgKut83Nh+7iPHdGuPrHafRp1l9jF/yj6zsvF+PWB30WDMteEjbytk0BSXl6LvgTwDAsVdHwM/Hs3rRE+oH47BejhitzNySOq4NabGlhwAbg56IiAhERkbauy5EZCXdVc5LytQY+u4Wg0y0NWFsTI/S6uotGoYAgJR/BqjsftNmHLZWRdXCkYkNg/Ha2I42ncMRSk0kzQsLNExA2CuR99K64ClBjxAChzLz0axhsLSuG1Wz6SvYK6+8ghdffBHFxcX2rg8RWUF3OF1JhdpkwBPkZ/1UaGuCnvKqJSl+2nPe7OstsWr3eaSevIwVO864VDbdknLj16zWGB5r5IFdgI7gKd1b6w5cwOgPtmH8klRHV8Up2RQGvv3220hPT0d0dDQSEhLg6yv/9rJ79267VI6ITJO19JSbzpkTGx5g9fm/33kOb9zSWeF9DctWKAQ4GZeKbB7X8/La6hajgtIKhAW4xjIN+rPatIQQqFBYooJDIu2nf4sG2HbikuIxF4qba2TV7sovHfrLcFAlm4KeMWPG2LkaRGQL3aCnTUwYjmUXGi1rzwkHSt+a0y8WIr+kXLbv4RW7sffFoTa9R5HOYGBLlnZwpE1pOfgq9TQWjOsodW/d2r0xVu46J5XRCKCiKlqMDQ/AtIHN8PX205g1rLVD6uxqXhvbEc/qrZyuz8ebEST/B0yzKeiZM2eOvetBRDbQXdyyU+Nw/Lwv02jZEzmFSHhmHdJeHQ7/Gmb9VfrWvPFQNjRin2xfbnG5YUEL7D+XK9t29sXc76mahv7imkMoreremnFDK1nQo9YIKeiJDPbDPf0ScU+/RMOTkc1CA3xxU+c4xb8DT2npYcuhaTZPq8jNzcVnn32G2bNn48qVyjwTu3fvxvnz5828kojs5afd1R+qlqz5BABvbUyzeIxMl/h6ivuVxvQAlUkN7eGmD/+WbV8pKsP2k5edfmzPhkNZ0s8hwMcLf8wcJB3TCCGN6fFh1mWr9W1evSZZ4wjlLtNnR7bB7T3jFY8JCKg1wul/h2qOv1um2BT07N+/H61atcLrr7+Ot956C7m5uQCAVatWYfbs2fasHxGZkJVfPQX6moXrYH361yn0mZ9idJBxkU5m5RM5hXhjw1Es2nRCVib/mmXZl+/o3cSicuYMe3crJnyyHRsOZtnlfHUhwNdbNkhZrRHYdvwyAGDfuTxHVctlJTQIxis3t0ebmFAsndwTAPB/9/eRlYkND0S/Fg3QNjbM4PWr92Si+bO/GiSHdDeMp02zKeiZOXMmpkyZguPHjyMgoHpw5MiRI7F161a7VY6ITNPNo2Np0AMA2fml2HMmV/HYV9tPS88LSyuweHM63tyYJpuKPexdy/7OQ/yt70HXXYFc30PfuM4kCX8fL9kimWoh8PnfpxxXITdwV1ICNswYiNYxoQCApOb1MU0h91Sr6BCDfdr/+y3HLtZuJR2M3Vum2RT0/Pfff3jggQcM9jdq1AhZWa7zTYzI1el2J10zM3tLn9JsK8D4OBxrgiotjRWDcYQQeHHNQaw1MS4JAD77yzWWufHx9pItJqrRCDQIsS1nERkXoJD80pNXqlexe8skm4Ief39/5OcbToc7duwYGjZsWONKEZF5QgjZDCdrg55yIwGJsSzMP+gMytUVEWR8KrnuO+SXlBsNtADgwPk8fJl62uhxrVfXHTFbxll46/Q1qDUC47o1AgCM6RLnqCq5nakDmqFT43A8N7KttC/A1/RHmzuP62FLj2k2BT033XQT5s6di/Lyym+EKpUKZ86cwf/+9z+MHz/epoosWLAAKpUKM2bMkPaVlJRg+vTpqF+/PkJCQjB+/HhkZ9tnoCSRq9PPB1NqIimeEmMByKlLytPejQUbiyZ1M/oe2gHPFwtK0eml3zD6g21Gy6qtaBUyl5PIWahUKmmMhVojpICykZGBuGS98EBf/PxIf9kSK4Wlpn8/Kpx9OmANeDHqMcmmoOftt99GYWEhoqKicO3aNQwaNAgtWrRAaGgo5s2bZ/X5/vvvP3z88cfo1KmTbP8TTzyBtWvXYuXKldiyZQsyMzMxbtw4W6pM5Ha++CdDtq2fI8ccYwOZjS0yCgDnrhbj31PVq4K/MLodWkeHGi2v/UK9OS0HgOnVsa0JeqwpWxf0s11/PqWH9Fzb2qMWQgpUa5oygEz7cbdyq6SWsQSSboExj0k25ekJDw/H77//jm3btmH//v0oLCxEt27dkJycbPW5CgsLMWnSJHz66ad49dVXpf15eXlYunQpVqxYgSFDhgAAli1bhrZt22L79u3o06ePsVMSeYS0bHkA8ddx5Uy0xhi78U/q3QQvrz2seKz/65tk262iQ+BrYkFRbTeC7vpbxWUVBmsClas1uOUjy9Pmq52seyLAV76yelKzBtLzym/eQtbS42mLsDqbX/ZlorC0AvcNsG4BXlfAmMe0Gq1G1r9/f/Tv379GFZg+fTpGjRqF5ORkWdCza9culJeXywKpNm3aoEmTJkhNTTUa9JSWlqK0tHr9IaWxR0TuIM1Eq4kljHWHWRNPBPp6w8/b+Ae4tkEmXGehzWtlaoOgp+Vz6y1/UwDCwV/Ul247BbVGg2kDmwOo/H/QpTuWR/tco6lejNSfQU+dmdAzHt/+d1a275lVlZmdeyfWR8fG4Y6oFjmIzX95KSkpGD16NJo3b47mzZtj9OjR+OOPP6w6x7fffovdu3dj/vz5BseysrLg5+eHevXqyfZHR0ebnCE2f/58hIeHS4/4eOVEVUSu7lBmzQL64jLlXDvGEg8qCfD1NvkBrj2X7hIYliZRNMWaOtpbuVqDV345jNd+PYrzuZVdgfr/B7Kgp+raLxeVsqWnjqy4r7f03FRX6KUi4wv0knuy6S9v8eLFGD58OEJDQ/H444/j8ccfR1hYGEaOHIlFixZZdI6zZ8/i8ccfxzfffCPL9VNTs2fPRl5envQ4e/as+RcReaBghRw6n/110qrZUYF+3ibX9NJ+3OjOljE2O8wajuze0l009FqZGleKynDyUpGsjG7QU1CV7HHs4n/wW1WKAVuX5yDL9G1R3b2Ye60cfZpFKpbjoF/PY1PQ89prr2HhwoX4v//7Pzz22GN47LHHsGLFCixcuBCvvfaaRefYtWsXcnJy0K1bN/j4+MDHxwdbtmzB+++/Dx8fH0RHR6OsrEzK9qyVnZ2NmJgYo+f19/dHWFiY7EHkbqzJf2PMGxvTZNtCCKungwdUdeu0jDJMBqc9JwBcLS6T9l0qtP3btfYzypEtPboBl0oF3P/lTqvP4e4J8pzJ74ez8fFdPRSPZegFq+T+bAp6cnNzMXz4cIP9Q4cORV6eZenVr7/+ehw4cAB79+6VHj169MCkSZOk576+vkhJSZFek5aWhjNnziApKcmWahO5jV1nrtb4HBcLKoOPrLwSqDXCptYHbY6ep4e3UTxetdQU3tx4TNr32P/tlZXJu2b5+2q/mTtyHLNut2BpuQa7Tlv/s7iXC43WKd0xZboOnudyIJ7G5jw9P/30k8H+NWvWYPTo0RadIzQ0FB06dJA9goODUb9+fXTo0AHh4eGYOnUqZs6ciU2bNmHXrl245557kJSUxJlb5PEs7SJ6amgr6fkfMweiecNg2fG/jl9En/kpeGTFbqNT2E3RDkj28VbuJtC2yBy5UD3+SDsORis1/bJF77Xj2eul8TGOnLKek1/dUmVtQkgtPx92q9SV0Kpu3C/u7WVwrGuTiLquTq0z1d1MNs7eateuHebNm4fNmzdLrS7bt2/H33//jSeffBLvv/++VPaxxx6zuXILFy6El5cXxo8fj9LSUgwbNgyLFy+2+XxE7sLSD33dYi2iQtEoIgjpF6ub9D/ZWrmkw/qDWZhzY3ur6tA+rrrruGMj+QyYUR1jse7ABRirpRBCujlbeo+ODgtwiu4t3bc2lWFa6/o2UUg5mlOLNSIl2llbr4zpAAAY1KohpvRNwHKd/FaukuTSHtYfuIBmDUOkdcs8lU1Bz9KlSxEREYHDhw/j8OHqfB716tXD0qVLpW2VSmVV0LN582bZdkBAABYtWmTx4GgiT2Fpq4x+PBGol55fN7fMMr3FMKf0TcCRC/lIbhuNeb8ajvXRHazbIMQfd/Zpgq+3nwFQnZfHWHCy7sAFjO4UV1Un44n6pl/XHIs2pePhwZVTw52he0v3msrV5ivirbDsdZ9m9e1aJzI0f1xHzEhuhZjw6okyfZrVlwU9jgye69L2k5elxXozFoxycG0cy6ag59SpypvjpUuVydAaNGhgqjgR2Zm57q3nR7VF/5YNEBsWiFV7zmNUx1gAhmMbDmVWj2n4eKt8Ic8u8fXw0k2VrT9KQY/+zJc2MdUtP9oPemOfKZuOXpSCHlNjemYNa4PHrm8pZTDWdicVlChPt68Luh+U+tP+F0/qht6J8plCJQo/K84aqn0qlUoW8ADArtNXjJR2b4drmN7CnVg9pic3NxfTp09HgwYNEB0djejoaDRo0ACPPPKIwUwrIrLcrtNX8OuBCxaVjQz2k23HR1av5fTu7V1w34BmaBMThvAgX2x6ajCeGtYaAPDEDa1kLQ8lJtbruqFdtMk66Ldg6C6DoT22KS1HSsinq2dC9VgK3QDuozsN1/FSWrJh5Pt/maxbbTqeU7022bSvdsmOjewYi/p6K6mfv1pscA7GPI6hH4R7SksPVbMq6Lly5Qp69+6NL774AuPHj8fbb7+Nt99+G+PGjcPy5cuRlJSEq1drPquEyBONX5KKh7/ZbVGm5ZCA6kbaY6+OwO9PDAJQOZZmTNdGRl8XGx6IzU8NNnv+RvUCFfP46NLvtTl7pXqAsjboyS0uxy1LUtG3ubw7x0vnxdoPnsGtG2J4h1jMvbmydalXgnJuFUcqLK3A0z/st+o1St1bbOlxjFnDW8u2P9+W4ZiKkMNY1b01d+5c+Pn5IT09HdHR0QbHhg4dirlz52LhwoV2rSSRJ3ljw1EsndLTZJmrRdWtKtrsvpb21Vvy7VZYUOa/DP0vONWv8dH5oD+gMC04X6dLSzsoW/uWdyclYEzXRtKsG2dyqcD6HEMXcksM9jHocQx/H29kLBiFhGfWAQCy8g1/NuTerGrpWb16Nd566y2DgAcAYmJi8MYbbyhOZSciZWqNwPK/T8nG1ujO9MnOL8G5qu6Rf05cwufbTkEIgX3ncm1+zwoLZn6V2zAlXGPFjHfdJIif/lU5lkg3YV9YgK9TTr01NjUfkC99oEubkVmXQuMPEdUBq75KXbhwAe3bG5/W2qFDB5PrYhGR3DM/7sfKXecUjwkh0Pu1yuScB18ehjs+2wEAaB0Tijf1sinbmyVTsfU108kBZMmspu0nL6NPs/qyKfTmTO2fiKXbKidSnMgpQIuoup1+62ticVXdpQ/MccaAzpPc2y8Rn+vNVnQXSr9Z/HWrZlVLT4MGDZCRkWH0+KlTpxAZ6Xz98ETOyljAA8gX5tTN+nv2iuHAWHursCBoaaOX72NCzybS85Qj2WZfP+GT7VbXa2Kv6gWEk9/ZWudJCm0Z+No4ItB8IapTIztWLmWUUD/IwTWhumZV0DNs2DA899xzKCsrMzhWWlqKF154QXF5CiKynu7MqnnrqvNh/X7YfEBhSnyE+Rv9qE6xsu0lk+SzqnolRuKjO7vL9oUH+eLhwc0xbWAzdGpcT/G8j1zXQrZ94Fx1t95zI9uarZc2A7SWPRYvtYYtS3XEhttvQWWyD2kc3OVizP/1iEVj2FyF+1xJ7bB6IHOPHj3QsmVLTJ8+HW3atIEQAkeOHMHixYtRWlqKr776qrbqSuRRSnWyxZbqfLgHmEjmZwntDd+YWcNa455+CbJ9IzrKg6DvH1Be/067BtfB83n4cbdhK5b+mJgbP9wmPQ/0M39d+jOhdp2+iv4t6y5P2Ij3lKfK//nkIKOvUSl2OJAj6c5M/HjrSfRv2QADWjZ0YI2orlgV9DRu3Bipqal4+OGHMXv2bCk6VqlUuOGGG/Dhhx8iPj7ezFmICDCfAl830Dl9ubpLq7Zzi9zeM96gRcVazRsqr7q++0yu0dcoTe3Wp3/pdy7dgY0zBuJKURmSmjsuy3EzI9cLcDyFMwrRmxl4qdD6WXnOytyvm0YjZCkjPI3Vd7bExESsX78eV69exfHjxwEALVq04FgeIittTjO9HpOxoEh3kcs+zWz7u+vfogG2nbikeMzbDp/SxlptturM0LLlfZW6s4a9uxUA8NfT1yE+0vnGaDw7si1uXvS3o6tBOvRzUFkyhs1dHMzMM9r97Als/joXERGBXr0MV60lIvPSLxaaLVNqZLzK5rTqwMHWgbxBJrqSatp9ZitLvn02CPUzeizjcpFTBj2d4+sh7dXhKCnXoPPLv6EpB886nI/e71pdD4iva7pX6+7Xao7zZf8icnP/nLgkTT83Zt3+C5i+YrfZc4UF+Joto8TUfc9YK03aq8Mx8/t9eHBgc4veI8jPW7ag6b4Xh+K+L/9TSGpYycRscJ1z+mBs10b4ac95i+pgT/rrbFnD38cb/j7eOPTyMLNjqqj26SeHtCR3latgd6ppDHqI6pglH9iWBDyAfDkKazQIqW4xGdy6oaz1yBh/H28susNwbSxjDr08DLnF5agX5AshKlty5o3tiKELtyqW9/ayLBh457bOiv+Htf3BtWq3/D0jgnxx1crZXOaW9qC6oT9+zJa8VK7KfcI72/AvkKgOqDUCU7/4D62jQ81+028fF4ZDJlZF7hxfD/vO5krntcWsYa2RlV+C23vEIyzQ16Kgx1oqlQoRVQujar99too2nkzQ0rFExhL7XS0yTKVhT3N/OSzb3vL0dXh9/VGTa52RczIIetyopcfcPAdzEyjcHYMeojqw/eRlbE67aFFw0UBvlW59ut9Kbb1V1w/xx/J7nGtMniXdW6aYyqFTWFoBtUYgPNC27kDAcBB1WIAv5o3taPP5yHm4U9Bjzh2f7rB4nT53xKCHqA5Y0yJjqpUHAM7nVq9m7lfTSMGJ1HQRzmtGvsEKIdBhzkYAwNFXhjtsoDY5L3fq3lL6M9K/+wghPHYpFPe5YxI5MVMLVeozlzNEt0VjlF7SQFfw33PJivstWa9L64GBzQz2nchRnhH3ydaT0vMzNVjCY0rfBJtfS87nozurx6dZ87vnivS7vNwoAbXVGPQQ1QFTLTK2zubxUgHXt42ytUqKhrSx7/mUNAxV7r4rUliN3Jgb2kUb7Ptpz3mUVhi29sxff1R6Xl6Db/T+vrxdupMWUdUJJd19GrdBS49DauEc+FdMVAdM5aApq9Ag/bWR6JkQYdU5ezSNtFsT9f/d3wcjOsTgNQeOUYkMNp6DR5+x7M3510wHTjX5Rv/xluoWo0/u6m6iJLmChqHVa6KVa9yne0uJ/tpi7rTWmLUY9BA5AW8vFR6wMP+N7mvsJal5fSy5sztiHLg4ZvtGYRaX9TEyvb3CzIeXvZbwGNo+xi7nIccJD/RFrwTPXEnAc0MeBj1EtUJ/YKQln7XeVoz7AYDca9av+O3MYsMDLS7brGGw4n5zq67r/hyEEG7frUGmdY4Pr3zi5r8Gtb1enyth0ENkZy/9fAjdX/0DWXkl0r4/j2abfZ1+anxzyhTGr3iKYH8fLL+np8F+Y0t3aOne/Kev2I2+C1JQUGJZ8Ni/ReVq7hzQ7D603cPuHhJwIHM1Bj1Edrb8nwzkXSvHZ39VjwFZtCnd7OusXejTlaecBupNG7+1e2Orz9E21rA77LmfDmDX6StGX6Nt2RFC4NcDWcjOL8W6/Rcsej/tmCNnXN+LbKP9C/K0MS7C7cM84xj0ENWSz7adsqq8tWN0XDfkAe5Oaio9n5zUFG/c0snqc0QG+yEmLECWcPC/jKsYvyQVa/dlIjX9ssFrtEFPypHqFe7zLOwm1I4X8rWyG5KcmBv+KJUuSb8X18NiPBkGPUROwlgunwk94xX323Mgc137ZscZ6fldSQk2tVr5envht5kDse1/1yFUbw2yR/9vDyZ+ut1gzI52WvyvB6pbdywd1qOd+WVsEDW5LncPAjy5ZUcf/3qJ7OTvE5fw1Mp9isfiLJgVZSwjsbEU+a48OFE3YKtJ7BYW4IvQAF8UlChPVb+itx6XdiFX3f+7q8WWrdmlHZxu7dgrcl4quPeYnrGL/8bGQ1kI9pN/KahJvipXx6CHyA4Ons/DpM924Idd5xSPZ+oMajbGWAvCD7vO4cbOcUhuGyWbteTKE4/m3txeel6bY5N6zvtDtq1trVm9N1Pap5ux2RRt8GlNdm1ybi48LM4ie87k4oGvdiFW70uXsSVbPAHX3iKyg0OZeUaPFZdZlmnYVK/JBxO7AgAyc6+h74I/Abh2S8/gVtWZn13lOrTfjn3caL0zquQiv4I2C9NbaLekjC09RFQDKhMjIo3lgrmnXwIAYM30fgAsGysSV686l40r36h1W0v8bVyGoy5UqDXSzB7tz9GX3VtuQ/uTPH25yKH1qGts6SGiGjHWTG4qAd6cG9vjmRFt4O9TOX3b2MBkY4n4XKWFREmwvw9GdoxBWYVAo3qWJyWsSylHsjH1i52IjwzESze2x38ZVwGwpcedqKv+hlKO5uBamRqBft5mXuEelNao8xQO/etdsmQJOnXqhLCwMISFhSEpKQnr16+XjpeUlGD69OmoX78+QkJCMH78eGRnm0/yRlTXjI1LUWuEbCDynX2aAABiwir72LUBD2A86DEWFLhy0AMAiyd1x2eTezhtvqGpX+wEAJy9ck0aAA1wTI87KS6t/vCfvWo/rhZZNqjdmVny9+TK4wFryqFBT+PGjbFgwQLs2rULO3fuxJAhQ3DzzTfj0KFDAIAnnngCa9euxcqVK7FlyxZkZmZi3LhxjqwykSJjyc3Uei09L45uj7+evg6bnhpsUNbYrKAmRpLhGZuxRKbZsvRESXn1GAhfTll3GxsPZUnPV+/NxJNGZl+6OsOMzJ4b9Tj0r/fGG2/EyJEj0bJlS7Rq1Qrz5s1DSEgItm/fjry8PCxduhTvvPMOhgwZgu7du2PZsmX4559/sH37dkdWm8hAmZEpoGpNddDj5+MFPx8vxEcGKTaj67f0LJ7UDeO6NsLTw9sonju32L3W3qqJFff1trjs//17xnwhE9jS4z6ulcm7ef48mmOkpHvJ8LAxTLqc5iuLWq3Gt99+i6KiIiQlJWHXrl0oLy9HcnKyVKZNmzZo0qQJUlNTHVhTIkO6zeS6Sss1UtBjLr+LftAzoGUDvHN7F1nGYV2d4+tZX1E31T0hwuKyz68+WKP3YkZm99Ew1N/RVagT564Wy7af+M50i9aJnEIcuZBfm1VyGIcPZD5w4ACSkpJQUlKCkJAQ/PTTT2jXrh327t0LPz8/1KtXT1Y+OjoaWVlZyicDUFpaitLSUmk7P989f3DkXPKNLFq5/mAWkprXB2B+bS39oMhY3/yeF27Az/sycVPnOBtq6p50x0ZZq01MqFXlK9Se2zXgdvT+xNw1oJ31w36Ly6o1AsnvbAEA7H9pKMIClL90uSqHt/S0bt0ae/fuxY4dO/DQQw9h8uTJOHz4sM3nmz9/PsLDw6VHfLxyCn8iezI27ubZnw6grEKb38X0DdVPb+q2sZahiGA/TO6bgIiqBTCpZvT/383x4pR1t6G/8G3vxPoOqkndyzKSMFW7xhwAXC50/YHd+hwe9Pj5+aFFixbo3r075s+fj86dO+O9995DTEwMysrKkJubKyufnZ2NmJgYo+ebPXs28vLypMfZs2dr+QqITM+kulxY2fJYP8R0U7p+a4Uvp0bXCWsHNof4O7yBnOxk1rDWsu0KjcZjBvn2mZ8CjcLvvrtfvtPdVTUaDUpLS9G9e3f4+voiJSVFOpaWloYzZ84gKSnJ6Ov9/f2lKfDaB1FtM7WUzYaqGSJBZnKA6Detu/KCoq7EWNBjrKujUYRz5hUi6w1uHSXb3n7yCu5Z/p+DalP3Vu89b/K4O96BHPqVZfbs2RgxYgSaNGmCgoICrFixAps3b8bGjRsRHh6OqVOnYubMmYiMjERYWBgeffRRJCUloU+fPo6sNpHMqUtFePanA0aPf5l6GoD5gczOmq/GVRx9ZTjavLDB6tcZa6Xr0Cgce87kyvaN7drI7cY4kNzmtIuOrkKNWHMXOZFTWGv1cFYODXpycnJw991348KFCwgPD0enTp2wceNG3HDDDQCAhQsXwsvLC+PHj0dpaSmGDRuGxYsXO7LKRAaue2uzReUsWWaCbBfga9tgZmMtPUqxUPs4thx7gsLSCo/oxvTE71kO/akuXbrU5PGAgAAsWrQIixYtqqMaEVlnc5rleT2Y36X2JTWrj9STl616jbEhPUq7r7hBxl4yr8iFgx5rhuTkXfO8XF/86klko6tFZZiyTN7/3zMhAsfnjcDKBw3HnXGMTu27rk1Dxf3jujYy+hqjA5kVmnoWb063qV7kvN65rbPBPldf4sVSX283TNSpe+nu2BLEoIfIRleKDb/1q1Qq+Hp7oWdCpMExzsaqfcZWu3/zVsMPNq0zV4pRorDqtGd87NGw9oazgV055rFnnGLs78mV8S5MZCOlqa26U0D1k96xpaf2Gftm6u2lQqSJvEa6azBpufIHH1ku2N8Hn0/pIdvnKT96T7wlMeghspHSh6JaZ6f+GB5Lsr0+M0J5nS2yjNIMuG5N6gEAnhvZVrZf94av9LNUGr/zys3ta1Q/ck76kwyU8te4owEtlbuD3RmDHiIbXCtT490/jhvs171ZeuvdSP86fsnseaf0TcDAVg0x58Z2Na+kB1IKK7WzusZ3byzbv3Ryz+rXKbzwfO41g329PChjryfxtzIrt1OzovVGaTybcPN2Ltccnk7kYO+lHMe6AxcM9h/Lrs57oZ+Xp6Ckwux5A3y98eW9vWpeQQ+lFLwY66ZqrJNk0MuCEZt9mkWitZXrdJFraBkt/7m68kDmcivWhjOXjXzP2atoUr96iZ1ytQYpR7LRO7G+yy6D40bhLVHdWbHjtOL+azoDYs0lIyT7U/ofn35dC8NyKvkaWtrxVkoDmrWGtjO+/A25tshgP3x8V3dp24VjHqzdl2lxWXPB3ePf7pWNXfzgzxN48OvdGL/kH5vr52gMeohskG9Bqw3z8tQ93dY0Px8v/PdcMvq3bCDt0w4uv75NlGzVey8VsPvMVbR5YQPm/3pE8dwDW3ne+AdPMqx9DEKrcvO4ckuPNSy5Tt0i6/ZXBlQnLxXVVpVqHYMeIjvSXW1df3BkWAB7k2vbJ3+dlJ6XVWjQMFS+yOuXU3vh+VFt8datnWWz6VQqFRasPwoA+HjrSdlrBrRsgG3/uw4tokJqsebkDLRxsGeEPEbG9Ojtcrf/CwY9RBa6XFhqdlbHPf0SpOf63Vucsl77zI2bigoNwH0DmqFekJ+se8tLpcK/p65I27pN+o9c1wKNI4JA7k87+89TVlq3ZJKau/1fMOghssC+s7no/uofeODrXUbLrLi/N+5OSpC2y/SWXtefzUX2FxMWID0f3Nry7qj953Jl2xfyShBYNesrrh5XVfcU2jjYzT7nJbf3iMfU/onStlL3lv4e3W13WBSZd2EiC3zxTwYA4PfD2UbL9G3eQNaao990bEmeHqqZ5lHB0vNFd3Sz+HXbTsjTCZzPvSYNSmcLnefQfqi7a5oePx8v3KKTukEjBGZ8uwfTv9kttejot+zobrpDqw+DHiIzftx1Dqv2nLf6dfoflvzwrH0VOtN1g80sGKl7A99zJld2bOlfp6TnnIXnOaSWHhcbyXKlqAwVei3LSrafvIxWOtPzrxaVY/XeTKw7cAFZ+SWKr3G1/wtzGPQQmfHkyn0G+5KamU9Sx6Cn7pnLO2KptOwC6Tl/bp5EO6bHwdWwwp4zV9Htld/R4rn1Bsc6Nw6XbR/PKYS3lwo/VC2IXFpRnaJB+6dj0L0lW4DU9f8WGPQQWelKURlST142W85bxaCnro2pWk1df90zJVGhAUaPndKZkqs/C4/cl/ZP1JWmrI9drJwzp0+zSDw4qLlsn/aW5O9TOV6ttLy6dcgduq4swTm0RFb6fudZi8p56bf0uMG3JGd3R68maBEVgvZxYWbL+lm49IA3x2J5DJXOQObSCjVyi8sRHWY8OHY2QgiEBvigoKQCs4a1MVhKRZt5PMC38ne/VKFLTD/2caUA0BIMeoispM3nYo5+ww5bemqfl5cKfSzoerQGg1XP4aWq7t4a/OZmXMgrwfVtorB0Sk8zr3QOGgGE+FcGPX7eXiivkAc12u5fbUtPWYVuS4/yOd0s5mH3FlFt0Q9yLFnfiZwPZ915Du1PWkDgQl7lwN6UozmOq5AFdBNwaoSQWmZUKqBCozy42d/XxEc/kxMSkS30g5weCREOqgnZKjLYDz7evE16CmNT1n8/nO20Y16u08lHJYS8ZcbYDMaAqpYeSzjrdduKf81EtUQ36HlqaCv8b3gbB9aGbHGlqMzRVaA6pJ3NdCyrQLb//i93YlOac7b4qHSW2dWI6gnmKhWQ3DYaPRW+bCm19Ahp9pZenh7Ze7k+Bj1EtUS3e+uRIS3N5o2husdFREnXpcLKIPfpH/cbHPsv42pdV8dqui09XioVAny98cHE6iSdTw9vDQDwVxjEP/DNTUbPKT23X1UdhndhohpoVC/QYIaEFsfwOD9zTfePDWlRRzUhZ2dNl5CjVLbSVI/pAYCoUH/0SoiElxfwUNUUdlP5dgz+JNwh0tHBoIfIBHMfiq+O7YDc4jL0aBppcIxDQZyf2em4DFypysI/juGx61s4XYI+3e4ojU5Lj7bby8tLhe8e6GNxvQ1jnuo9znXltuFtmcgEcwl++7dogLFdGyM+0nAVbrb0OD8jk1skKy3MyUSeodPLv1m03IOjCL3ZW1o1CdSKy9TmC7kQBj1EJqw2s+aWr4nmHP3khOR8OsXL0/THhcsT0WmnLRMBQEFJBdbuz3R0NYzSiOqWGnO3nzt6N1Hcr9+6/euBC3aomfNg0ENkwuZjF21+LZPaOb8Z17eSbT/NGXZkRsoR55rFpb8KevW26ftPswbBFp0/NjzQtoo5KQY9RCboZzS1Bht6nF+gn7ds8djwIF8H1oZcwS/7navl42pxufT8SlGZ1FJj63cu/R798ED3+ptg0ENkgrGMpgAwpE2Uydeye8s1HNNZUd1H72c2sVd8XVeHHOi5kW0dXQWr/XEkW3o+5O0tyC+pAGB+0LGl43zUTE5I5DnK1cb/4M0tasnuLddwWScBoQoqRIdVp/W/vafyuAdyT5P7Jji6CnZj6+BlUwuOusMtjUEPkQnaDK1KzE135gKjrkelArLzS6VtBq6exdJ11g5n5kvPNeameDqIvW4/znp9tmLQQ4TK1YcPZeZJqxBrbT95xehrOjeuZ/KcSllPybnpf0548UfoUSxtHXkv5RjK1RoMf3crbv8k1SnXp1KZ6eBSOlqu1hgsQ+FmMQ+DHiIAuPWjfzDq/W14ee0hi19zQ7tok8edLYkZWUDvR+bDqMfjKc1y2ngoG2euFONoVgH+y7iKa+XOl8vGltvP5cIyg5HM+l8EXZ1D/6Lnz5+Pnj17IjQ0FFFRURgzZgzS0tJkZUpKSjB9+nTUr18fISEhGD9+PLKzs42ckch6pRVq7D6TCwD4MvU0jusMbB3VKdbo68wFNe51q/AQej80ZtX2PB0aycfq9W1RX7Hc6ctF0nNTY/9cidKgZWdsxaoJh/5Jb9myBdOnT8f27dvx+++/o7y8HEOHDkVRUfUv0xNPPIG1a9di5cqV2LJlCzIzMzFu3DgH1prcjf5K2jcs3Co9bxsTavuJ3exm4a6S21a32KmFwMwbqnP3MKu255k+WL7emtLvwICWDXDv8p3StjNmaTY3e1TpV1uj0e/ccr/ZWw5de2vDhg2y7eXLlyMqKgq7du3CwIEDkZeXh6VLl2LFihUYMmQIAGDZsmVo27Yttm/fjj59+jii2uRm/BUWEly95zz+OJKNxhGVy0tM6t0Em9MuGl1clFyX7tgrtUZgSJsovPP7MQAcjO6JQgLkH4tKQU96TqFs21EtPaZaYWz5zVU6nW7vlrlxQq7AqRpv8/LyAACRkZWLN+7atQvl5eVITk6WyrRp0wZNmjRBamqq4jlKS0uRn58vexCZonTjmPHdXvyy/wI+2pIOoHK5iW+nVQfZyW1N5+gB2L3lKnRn7GiEQIBvdRDMlh7PE+wvD3rGd2sMAGij0+rbpL58rb1yB7X0mBpuY8uvroAwnLKu8yaG7UCux2mCHo1GgxkzZqBfv37o0KEDACArKwt+fn6oV6+erGx0dDSysrIUzzN//nyEh4dLj/h4Jhcj0yz5M/bxUiE+Mgi7nk/Gq2M64N0JXc2f1/XvDx7BT9bSU5mlWYstPZ4nRC/o6dg4HNtnX4+1j/bHiA4xAAxbPA5lOubLtalBxrbM3lI6nbsNZHZo95au6dOn4+DBg9i2bVuNzjN79mzMnDlT2s7Pz2fg40HUGoFZK/ehfaNwnMgphFqjwevjO5kcdGxJcOJd1RpQP8Qfd/ZpalFd3OFbkSfw8dYNejSyDz13u+GTefUUliKJqVqIVtvyFxYo/+g8eD4Pw6sCorpkKleYLS09GiEM7ltlOq1Y7tC95RRBzyOPPIJffvkFW7duRePGjaX9MTExKCsrQ25urqy1Jzs7GzExyr9g/v7+8Pf3VzxG7u/L1Ays2nMeq3RWR+/bvAHGdG1k9DWWBCe+nLrstnQ/N9QaIFQn6PFjriWP0zCk+vPjvQld5AerPvMr9MbwfLjpBPo2r4+8a+UY0dH4jE97M/WFzZag55d9F3Bbz8ayfZluNo7RoX/RQgg88sgj+Omnn/Dnn38iMTFRdrx79+7w9fVFSkqKtC8tLQ1nzpxBUlJSXVeXnNx7fxzHy2sPG+yf8d3eGp/bx8JMrbrYveV61ELAy0uFxZO6YcG4jogOC3B0laiOqVQqvDC6HW7t3hijO8XJj1X9m3ut3OB1d3y2Aw99sxsncgoMjtWWmsysUmr9XvjHMYP71gd/nrD5PZyRQ1t6pk+fjhUrVmDNmjUIDQ2VxumEh4cjMDAQ4eHhmDp1KmbOnInIyEiEhYXh0UcfRVJSEmdukYGFfxyz7YUW3Dd8bUjYEuRnOCuMnFH1L4B20ObIOvy2Ts5nav9Exf3aQKG4zHgywlOXitEiyjDVxYxv9+Dc1Wv47oEku40VM9W9VVpun8HV/YzkKXJVDg16lixZAgAYPHiwbP+yZcswZcoUAMDChQvh5eWF8ePHo7S0FMOGDcPixYvruKbkLpb/fQrF5Wo8rJOLw5LvSrbcpKb0S8TWY5cwomPd9/WT5eTdW2yeI+O0t4FSExmYY8MNWweFEFi9NxMAcCgzD53MLGFjKd2ZVTd1jsPP+zKl7dAA2z7e9f8COjaqJz13h8mMDg16LMn0GBAQgEWLFmHRokV1UCNyZ+VqDV6q6v4a17WxNDjRkhZiU9/sjAnx98H3D7Ib1tnp/vyvtyAVAXku7Wf+yUuVCXT9vL1kA32NKdFpdVHKC2Yr3SBdf6p9vSA/k681FsDofy6724QMjtIjt+dT9fVM9wZRXFYhPbfkj9rUauvk2nR//uY+KMiz6ScnVQp4lL5EXSoslZ7bs9tbtz4DWjaw23lldJMTukFTD4MecnsVGgGNRuChr3dJ+3T7wi1p6Qn2c4qJjlQLOOCcLPVfxlWzZZS+RBWWVn/J+if9kunXC4H8EsOB0krqBVYH6SM6xCA80HC6vTHGwhf9vwfdTdcPeRj0kIfYevwiNqVdlLZT0y+jqLQCaoW1ZpTUD2ELgLtyt7WFqO7c1acpMhaMku1TXsqheufGQ6YXzJ71w350euk3bD952eJ6BPt5Q6VSYefzlasX3NK9sZlXeC4GPeQRZq86INt+Yc0htJ+zEcPe3WrZ2DI79sOTcyksqTBfiEjBsyPbGuxTuptM/2a39PzPozkmz/nDrnMAgAmfbDf7/vpf2Xy9vZCxYBTeurWz2deezy1R3J9bLG9l0l1iww16txj0kOsqKVfjwz+PY8+ZyibnyGDjrTEX8pT/wE/kFFrUvXFj5zjzhcglaX+2bWPDHFwTcnb6ySq1S5b8++z10j6DgcBCIONyca3UR/tWtoy10a4rqO+XA5my7WV/Z0jP3SHo4UAFclnPrjqAVXvO463fjqF70whcKSozWrZ5w2CkXyxSPFZhYpryoZeHGcyKIPcyulMs4iOD0CIqxNFVISf324yBGPzWZoP9UWEBaBIZhDNXig3Wr0qc/WvdVM4OAn298fGWkwb7z1wuRpP6QW6xDAVbeshl6S41seu08gDD9nGV395NzcrJLTYeLDHgcX8qlQpd4usZLDRJpC+hQbDRY9WtINVRT3a+cguzpS4WlJo8rn0ne4Ui14zkHyrXVHZxuUNLD4Mecmva5mhT68fkGxnT0TraMKsqEXm2u5MqFxyePaKNbL82HtDt3dIfHwOpjGWD5/84YnrQs3SeWg5G/GzISO+s3OdKyKNsOJhlUTnvqq8mxsb0AMCcNQel5z8+VJ1M8NlRhoMUicizzb25AzIWjMIDg5rL9mvH1eiGM+VGEheeu2r8S5ivzjp/+hMwHM0NGnoY9JBrelAn544+3cGGXha0x+oOMozQ6Qbr29y91pwhotqj1NJjLOh5UeeLlj79maLpFwuNlq1J95Y1y1RIiV3doH+LQQ+5nUMvD8OM5JZInT0EXlb+hjeJDEKTyCC0jQ2TMjkTEZlVdbvQ7boqVyt3Y+0+k2v0NAWl8u72eeuOGC1bkxRTr4/vZHFZbS4rd7gjcuQeuR1fby/MSG4FwPqFQn28vbDpqcFQwT1SrhNR3ZBaenT2/XZIuRt+YKuGivs1CjNJi0rN55Gy5V7VqF6gxWWV6uWq2NJDbu3vE/Kspl3i65l9jbeXCl5s5SEiK2i70rWtL+dzr+GzbacUyxYaWWaitMKwO+xioakZXFUtMDbcrkyl+NBqUJWJXtvS4w6hD4Me8iickUVEtaGkalFibffWsyYGIesuiaNLdyFkaV+p8cWOtQFWabn5ld71+Xibj5S0gZx2TI+ls86cGYMe8iijO8fiketa4It7ezm6KkTkRs5eqZyRdcdnO7D3bK7JNBlK3e7bjl9C91f/MNivMRFonL1aOQnDWH4dU3wsGPCorWdVmh63WJyXY3rIrbw3oYvJ40WlFXhqWOu6qQwReaQxi/5WXBYn2M8bRWVqdGtSz+DYnUt3KJ7L1HCa9QcsS92hJCzQ/Me/1NIjdW9VV0YI4ZLjHtnSQ04rO78ECc+swz3L/rWo/OwRbXBzl0Ymy+RfMz4oMD7S8oF9RESm3NzFcL0+n6okf8VllrfMmOpSMrWEjjnt9Naaa1o/yKCMNv2Hduq9blU2pZleONVZMeghp9X7tRQAhv3fOXqp3X99bABOzR9pkCxMydD20UaP3dsv0YZaEhEZ2plRuTROctsoaZ+2V+uaFUFPx8bhRo/1aRZpW+VgOONrjMIXRm0m5rIKw6DnUoH5gdDOiEEPOSW1iW8wvaqCIQD49O4eaBcXZnEzq6k1uCxJZEhEZAnteJvGEdUtKFerlqU4eakIFUYSF+rrEGc86NEGIbqBla2Ubn9p2QUAgC3HKr94usGQHgY95Hw+++skmj8rX5n4fNWgQP1Wnu5NIyw+r7m8FIx5iMhWj13fUrbdvGEIABhNcqq7rpb+l7zuTSPQvGHl4qamBjJru7eszUemtePZ6wEAU/snmvzS98nWypXXdbvahIuGQAx6yOm8qpCB9LWqfbd/sl2239y0ywDf6l9x3eUpACAiyFe2zZiHiGylXYhUSxvIeHurMLJjDADgp4f7Ssdf+aX6Pqe/XEVMeICUwNBUaKF9D0tmYimJDgvAyddG4oXR7Uze/0Z1jLXp/M6IQQ85FWMZTC9VJeg6e6VYtt/c6r/fTateQPSjO7vLjv0+c5Bs2zW/txCRM2gQ4i/b3ns2F0BlS8/iSd2RsWAUujapbpku0Zlmrh/0lFdoDJId6sspKMGcnw8BsL2lB4CUiFWpoeeuPpWBnCWtTq6CU9bJqUz7Snkh0R2nruC/jCsGsxUCfL0Vy2t1jq+HjAWjFI/p36Tc4O+ZiBxoXNdGWLXnvGyft5FWGN3cOtn58qzL5WqNzrIWyjemtzcek56byglkqZYKiVu1LekVUnLC6mOuer9kSw+5jHX7L9Tq+d0h2ygROc7bt3U22GdsTI/utPXnfpJnby5Ta6T8PB9vOan4+u92npWe7zx91dqqGhjaznBmq7buUtBT43dxPAY95FRMzUIotGDhvZpwozX1iMgBlGaRWtL1tOPUFdn2XX0SsPP0FSOla4dKpUKbmOrWnvcmdJHyClWoDZehcNXbJbu3yKk0iQw2euySyYX3bH2/IJypGifkDv3VRORczlwuNl9Ix59PDkKzhiF4fcNRo2VO5BTUtFqKVj6YhLSsAnRrEgEvLxXSf0sDAFRUrUPhDndItvSQU9H+cSk5f1Xebx3kZ3o8jyVC/KvjfqUVjomIrKGddaWl2w0FADd1NszUrKtZ1VR33Wns+jl9jlyQBz1zbmxndT2VhAb4okdCpDS4WTseSRpLyTE9RPalP4sBqJ45kHetXNoXFuCDdY8NqPH73dqjsfR8o5GZY0RElvrs7h6y7egw+YSJl25qLz3XJv1rHGGYQ+yMzkzVEr0vZGV620Pbx9hWWTOkgcxV9+WTl4pq5X3qEoMecirlasOvD+kXK//Qcgqqu7eeH9UOiQ2Md4VZ6u6kBOk5u7eIqKb084FN6i3P36O7EOnkzyvXFUxuG11VtoniOUv1VlHXb5UO8Kmdj3L9gcy6mJyQqIaKSivww65zAICHBzdH78RIvKMwGwKoWV4K/fNokxS+P6GrXc5JRJ5Ndzxz5/h6Fr8uwsgyObotPX+fuIRndWZ7dWgUpriiuz1o77NK3we9XTSFPQcyk9NoP2ej9DwmPADfPVCZWHDm9/sMyqZfLLTb++55cajdzkVEdF//RHz61ykAQK8E04uCCiFwtbhy8U5j3+V0u7MmfbZDduyXR2vezW+Mdjaa0lqI+i1arsKhtd66dStuvPFGxMXFQaVSYfXq1bLjQgi8+OKLiI2NRWBgIJKTk3H8+HHHVJbqlLlvLg1D/U0eJyJyFF+dTPGBChMuXhvbUXr+0s+HsGZvJgBArdOk8sHE6pbnY9m1M1vLHO0qP2qFph4GPTYoKipC586dsWjRIsXjb7zxBt5//3189NFH2LFjB4KDgzFs2DCUlJQolifXpT8FMyff9PR0e8zcIiKqDX2a1Td5/KYu1TO4vkg9LT3ffrI6N8+NOrO8jE1fn9o/0dYqWkTbvaVRaOkxtUCpM3No99aIESMwYsQIxWNCCLz77rt4/vnncfPNNwMAvvzyS0RHR2P16tWYMGFCXVaVapEQApM//0+2z9z0cbb0EJGzGtiqIT66s5s0/Vyfv5FWEt0Zqrq0DS36WeO7Nqlncx0toZ26rtS9pZvuw5U4bfvUqVOnkJWVheTkZGlfeHg4evfujdTUVKOvKy0tRX5+vuxBzm3zsYs4r7d2TMdG4dLz0ADDP67BrYxnbiYicrThHWLRSmE9K8D40hQncpTHKmqDncWb0/X216CCFtC25ihlq3fNuVtOHPRkZVXmTImOlq8HEh0dLR1TMn/+fISHh0uP+Pj4Wq0n1dw9y+StPK+N7Yj+LRtI2/q5e1Y+mCR9AyEicjVKy1UAQExYgOL+hKr0HIs2nZDtr61ZW1raGVrnrhpmlXbVtQqdNuix1ezZs5GXlyc9zp49a/5F5DD662k1jgjEHXq5KvSbUXuamQ1BROSKFozvKNtOqB8EAOjbvHKMkO4ipQDQMkq5+8xerlTNKjuaVYDiMvm92kVjHucNemJiKjNMZmdny/ZnZ2dLx5T4+/sjLCxM9iDnlZYlH8D81dTeBmU+1ctwSkTkjgbpLWHRpH5lC8/fJy4rlvfxrt2P8PUHLkjPs/NL0ahedeZoJie0s8TERMTExCAlJUXal5+fjx07diApKcmBNSN7KKnKMPr536ekfRkLRilmWe7aJKLO6kVE5AgZC0YZdHttrVqmQrtchT57JWk15ppOJugPUo7Lxld+/9+5Wn3v2uLQoKewsBB79+7F3r17AVQOXt67dy/OnDkDlUqFGTNm4NVXX8XPP/+MAwcO4O6770ZcXBzGjBnjyGqTlYQQ6DBnIx77vz0AgGd+3I/OL/+Gr1IzsG7/BTOvrnSjmUX6iIhciW7SwvhIw7W3AGBw64aK+7VqO+h5ZkQb6fmqPedlS/VscNG1Ch0652znzp247rrrpO2ZM2cCACZPnozly5fj6aefRlFREaZNm4bc3Fz0798fGzZsQECA8mAvck4/78tEYWkFft6XiZJyNX47XNll+cKaQxaf44XRbXG1qAx39lFem4aIyJU8NLg5/l1emZfn7JVrimVGdozF5rSLuM5I8FPb08Y7xIXLtpVmcbkahwY9gwcPNjkCXKVSYe7cuZg7d24d1orsrUJnEVFtwGOtqNAAfH2f4XgfIiJXNLCV6VYcwPSCn3UhSm82mTssyuy0Y3rIfczWWRzPmHaxHHBORJ7Dkq4p7UBl3S+OjuQGMQ+DHjKvsLQCuVVTF02pUGsMWu6EELLF8owZ162RzfUjInJHvlJLj/l7aF3Qb+lRWp7C2THoIbNu/nAber2Wgqw842uelVVo0GPeH+i34E/Z/qx88+ukzbmxHab0TahpNYmIXFKDEOVldbx1urcs+eJZ2/SDnpIKtZGSzotBD5mk0QikXyxCWYUGf5+4JDu24eAFvLjmIH7YdQ7HsguQW1yOzLwSpF+sTqW+YL3yQnlaCfWDcE+/xFrPN0FE5Ky6xNdT3O+r07214t8zsmN1teiy7tAD/QanolIGPeRmRn+wTXp+4Hye9Px4dgEe/Ho3vkw9jadW7pNlVk6vWj9m4e/HsGZvprT/1PyROPjyMKS9Olza98YtnWuz+kRETuv2HvHw9Vbhf8NbKx738a5s6SlXa2TjaRbd0Q2bnxpcBzUEPrm7u/Rcf43Ea2WuF/S45jKpVGcOX6hesHX5Pxl46ab2AIAbFm6VlZvwyXbp+bSvduHtWzvjvZTjsjIqlUqaYvnHzEE4dakIvRK5pAQReaYF4zvixRvbIdjI1HMfr8p2iaNZBTialSbtH9kxxuj6XfYW4Gu8RalIb2kKV8CWHlJUUq7GPcv+Ndj/x+Fsi/qWn1y5T7b92xMDZdstokJwQzv5YrJERJ5EpVIZDXiA6pYeXXUZ8ACmgx799bhcAYMeUvRV6mlsSjNMfX7flzvxydaTVp8vPiLIHtUiIvIYPgrT2kP9feu0DqYSIKZfLKrDmtgHgx5SNO/XI0aPXS0ut+pc3ZtGILCOBt0REbkLX4UJHocu5CmUdIynf9jv6CpYjWN6yGo5FkxD15p+XXPMGtbGfEEiIpLx9zEMeg6ez1coSZZiSw8ZUFoaZHy3xtLzlKM50vP6wX7o3LhyfZYv7+2Ft26Vz8aanJRQO5UkInJzpsbT1KWp/RNl27W95ldtct2aU6159qeD0vNh7aPx+vhOOHA+Dz/uPicr1ysxEt9N62MwqG58t0bYlJaDDnHhBmu3EBGRZbwUxvR0a1KvzuvRvGGIbPvF0e3w9I/7ER2mnFTRmTHoIZmi0gr8n04SrI/v6gHA8JceAJ4b2VZxFoFKpcKQNpyZRURUE0qt7lGhdf9FUn9oUYNQPwBAdn4pzl4pRnyk60xUYfcWyfSc94fifqXsn21iQ2u7OkREpGNQa/Ors9tbSbk8FbPushnjl/xT19WpEQY9JNlwMAvFRjJsKs2+8vdxjv5mIiJPcXuP+Dp/T92lhQCgZVT1F96cgtK6rk6NsHuLcOBcHiZ8kooivYDnj5mDpOd+Ou2bf8wchBZRht1dRERkP0rDB5TG+dS2QL0B1a6cgoRBj5O4WFCKn/acQ4uoENy7fCcA4N3bu2BM10a1/t43frjNYN+X9/aSBTYqlQpH5g6HgECQH39tiIhqW3hg3SYiNGbawGb4uCop7bSBzRxcm5rhp5cTUGuE4liaGd/txdD20Qjy80GFWiOtRF5SrsYrvxxGr8RI3NzF+qBo/7lcrNmbiUeHtICfQh4IAEhsEGywz5WjeyIiV6M/NfzvZ4Y4pB71Q/yx54UbEOjn7TTT6G3FoMcJ/Jdxxeixdi9uRM+ECPyXcRUAMDmpKf7NuIojF/LxzY4ziAjyw8BWlQPbikor0H7ORgDAwZeHIdjP26B5NK+4HDd9+DcAYOm2U4rv+fj1LV1qND4RkbtqEOKHS4WV6x02qhfosHpEBPvJtn95tD9Gf1DZS5BfUo6wAOdolTKHA5mdgO4K5Uq0AQ8AfJF6Gkd0Vj6/+/N/seFgFp5ffUAKeACgw5yN6PjSbwZTHid8avq9AOCJG1pZWnUiIqpVdT+GxxLt48Kk5zd9YDhEwlkx6HGwDQezanyOB7/eha+3nzHYX1hagZsX/S3bpxsw6fvhwSTsf2lojetDRET20aNpBADlJSkcSbcXIeNyMS7kXXNgbSzH7i0HKVdrsPD3Y1i8OV3x+OanBmPwW5tr/D77z+XhuZ8OYHz3xtBoDBNd6eqREFnj9yMiIvt5bVxHNKkfhNt6NDZf2IFOXSxCbLjjut8s5Vyhowf5KvW0QcCz8/lkbJ11HX58KAkJDYKx7rH+iq9tFxuGw3OHKR6b2Cse3au+GWh9s+MMxi3+B7d8lCrtG9EhBk3rB2Hh7ZVrZf355CAQEZFziQz2w7Mj26JFlPMlgx3VMVZ6npZd4MCaWE4llPJcu5H8/HyEh4cjLy8PYWFh5l9QB0Z/8JfBSrmvjOmAu/o0VSyv0QiohcCgNzYhM68Ef8wcKP0BrNt/AdNX7AZQObCsQ6PKxT+FEEic/avROmQsGGWPSyEiIg9Vodag9Qsb0CshEi/d1B6tY+wbmNXG5zeDnlq0/1wuNALoEl8PFwtKsTktB0cuFODzvw1nTe1/aajZ0e8l5WpcLS4zaEI8d7UYMWEB0pR2rYxLRYpdZA8MbIbZI9taf0FERER1pDY+vzmmp5Y88NVObDyUbbbc/4a3wX0DEuGrv6KbggBfb8U+08YRytPLExoEY830frLBzF9N7YX+LRqYfS8iIiJ3w6CnFuQUlFgU8Mwa1hoPDW5eq3XRnVZ49JXhLp9YioiIyFYMeuxICIGf92Xi8W/3mi1bV2NqfLy9OH6HiIgIDHrs5qvtp/HC6oOyfUsmdcOr647gfG5l/oLvpvVB86gQ1NfLbElERES1j0GPjTQagV8OXMCw9tFo/fwGg+MDWjbAiI6xGNExFiXlapSpNS6TppuIiMgdMeix0f1f7kTK0RzFY1P6JuClm9pL2wG+rr9IGxERkatzieSEixYtQkJCAgICAtC7d2/8+++/jq4S+inMgPr3ueuRsWCULOAhIiIi5+D0Qc93332HmTNnYs6cOdi9ezc6d+6MYcOGISdHuZWlrkzumyA979u8Pk6+NhJRoQGOqxARERGZ5PTJCXv37o2ePXviww8/BABoNBrEx8fj0UcfxTPPPGP29c6YkZmIiIhMq43Pb6du6SkrK8OuXbuQnJws7fPy8kJycjJSU1NNvJKIiIhIzqkHMl+6dAlqtRrR0dGy/dHR0Th69Kjia0pLS1FaWipt5+fnK5YjIiIiz+LULT22mD9/PsLDw6VHfHy8o6tERERETsCpg54GDRrA29sb2dnyJR2ys7MRExOj+JrZs2cjLy9Pepw9e7YuqkpEREROzqmDHj8/P3Tv3h0pKSnSPo1Gg5SUFCQlJSm+xt/fH2FhYbIHERERkVOP6QGAmTNnYvLkyejRowd69eqFd999F0VFRbjnnnscXTUiIiJyIU4f9Nx+++24ePEiXnzxRWRlZaFLly7YsGGDweBmIiIiIlOcPk9PTTFPDxERkevxuDw9RERERPbCoIeIiIg8AoMeIiIi8ggMeoiIiMgjMOghIiIij8Cgh4iIiDyC0+fpqSntjHwuPEpEROQ6tJ/b9sys4/ZBT0FBAQBw4VEiIiIXVFBQgPDwcLucy+2TE2o0GmRmZiI0NBQqlcrq1+fn5yM+Ph5nz551++SGvFb35EnXCnjW9fJa3ROvtZIQAgUFBYiLi4OXl31G47h9S4+XlxcaN25c4/N40uKlvFb35EnXCnjW9fJa3ROvFXZr4dHiQGYiIiLyCAx6iIiIyCMw6DHD398fc+bMgb+/v6OrUut4re7Jk64V8Kzr5bW6J15r7XH7gcxEREREAFt6iIiIyEMw6CEiIiKPwKCHiIiIPAKDHiIiIvIIHhH0bN26FTfeeCPi4uKgUqmwevVq2fHs7GxMmTIFcXFxCAoKwvDhw3H8+HFZmfT0dIwdOxYNGzZEWFgYbrvtNmRnZ0vHMzIyMHXqVCQmJiIwMBDNmzfHnDlzUFZWVheXKKmLa9Vat24devfujcDAQERERGDMmDG1eGWG5s+fj549eyI0NBRRUVEYM2YM0tLSZGVKSkowffp01K9fHyEhIRg/frzBtZw5cwajRo1CUFAQoqKiMGvWLFRUVMjKbN68Gd26dYO/vz9atGiB5cuX1/blydTltWr9/fff8PHxQZcuXWrrshTV5bV+88036Ny5M4KCghAbG4t7770Xly9frvVr1LLXtT722GPo3r07/P39FX9emzdvxs0334zY2FgEBwejS5cu+Oabb2rz0gzU1bUClZl833rrLbRq1Qr+/v5o1KgR5s2bV1uXZsAe17pv3z5MnDgR8fHxCAwMRNu2bfHee+8ZvJc73JssvVatGt2bhAf49ddfxXPPPSdWrVolAIiffvpJOqbRaESfPn3EgAEDxL///iuOHj0qpk2bJpo0aSIKCwuFEEIUFhaKZs2aibFjx4r9+/eL/fv3i5tvvln07NlTqNVqIYQQ69evF1OmTBEbN24U6enpYs2aNSIqKko8+eSTbnetQgjxww8/iIiICLFkyRKRlpYmDh06JL777rs6vdZhw4aJZcuWiYMHD4q9e/eKkSNHyq5FCCEefPBBER8fL1JSUsTOnTtFnz59RN++faXjFRUVokOHDiI5OVns2bNH/Prrr6JBgwZi9uzZUpmTJ0+KoKAgMXPmTHH48GHxwQcfCG9vb7Fhwwa3u1atq1evimbNmomhQ4eKzp0718UlSurqWrdt2ya8vLzEe++9J06ePCn++usv0b59ezF27FiXulYhhHj00UfFhx9+KO666y7Fn9e8efPE888/L/7++29x4sQJ8e677wovLy+xdu3a2r5ESV1dq7ZM69atxZo1a8TJkyfFzp07xW+//Vablydjj2tdunSpeOyxx8TmzZtFenq6+Oqrr0RgYKD44IMPpDLucm+y5Fq1anpv8oigR5d+IJCWliYAiIMHD0r71Gq1aNiwofj000+FEEJs3LhReHl5iby8PKlMbm6uUKlU4vfffzf6Xm+88YZITEy0/0VYqLautby8XDRq1Eh89tlndXMhFsrJyREAxJYtW4QQlfX29fUVK1eulMocOXJEABCpqalCiMog0cvLS2RlZUlllixZIsLCwkRpaakQQoinn35atG/fXvZet99+uxg2bFhtX5JRtXWtWrfffrt4/vnnxZw5c+o86NFXW9f65ptvimbNmsne6/333xeNGjWq7UsyypZr1WXNz2vkyJHinnvusUu9bVFb13r48GHh4+Mjjh49Wmt1t1ZNr1Xr4YcfFtddd5207S73JiX616pV03uTR3RvmVJaWgoACAgIkPZ5eXnB398f27Ztk8qoVCpZ8qSAgAB4eXlJZZTk5eUhMjKylmpuPXtd6+7du3H+/Hl4eXmha9euiI2NxYgRI3Dw4ME6vBpDeXl5ACD9n+/atQvl5eVITk6WyrRp0wZNmjRBamoqACA1NRUdO3ZEdHS0VGbYsGHIz8/HoUOHpDK659CW0Z7DEWrrWgFg2bJlOHnyJObMmVMXl2JWbV1rUlISzp49i19//RVCCGRnZ+OHH37AyJEj6+rSDNhyrTV5L0fen2rrWteuXYtmzZrhl19+QWJiIhISEnDffffhypUr9r0AK9jrWvV/Zu5ybzJ2Hv3fT3vcmzw+6NH+58+ePRtXr15FWVkZXn/9dZw7dw4XLlwAAPTp0wfBwcH43//+h+LiYhQVFeGpp56CWq2Wyug7ceIEPvjgAzzwwAN1eTkm2etaT548CQB46aWX8Pzzz+OXX35BREQEBg8e7LAbi0ajwYwZM9CvXz906NABAJCVlQU/Pz/Uq1dPVjY6OhpZWVlSGd0PRu1x7TFTZfLz83Ht2rXauByTavNajx8/jmeeeQZff/01fHwcvx5xbV5rv3798M033+D222+Hn58fYmJiEB4ejkWLFtXyVSmz9Vpt8f333+O///7DPffcU5Mq26w2r/XkyZM4ffo0Vq5ciS+//BLLly/Hrl27cMstt9jzEixmr2v9559/8N1332HatGnSPne5N+lTulZ73Zs8Pujx9fXFqlWrcOzYMURGRiIoKAibNm3CiBEjpKXsGzZsiJUrV2Lt2rUICQlBeHg4cnNz0a1bN8Xl7s+fP4/hw4fj1ltvxf3331/Xl2SUva5Vo9EAAJ577jmMHz8e3bt3x7Jly6BSqbBy5UqHXNv06dNx8OBBfPvttw55/7pUW9eqVqtxxx134OWXX0arVq3sem5b1ebP9fDhw3j88cfx4osvYteuXdiwYQMyMjLw4IMP2v29LFFXv8ObNm3CPffcg08//RTt27ev1fcypjavVaPRoLS0FF9++SUGDBiAwYMHY+nSpdi0aZPBANu6YI9rPXjwIG6++WbMmTMHQ4cOtWPt7Ku2rtWe9ybHf5VzAt27d8fevXuRl5eHsrIyNGzYEL1790aPHj2kMkOHDkV6ejouXboEHx8f1KtXDzExMWjWrJnsXJmZmbjuuuvQt29ffPLJJ3V9KWbZ41pjY2MBAO3atZNe4+/vj2bNmuHMmTN1e0EAHnnkEfzyyy/YunUrGjduLO2PiYlBWVkZcnNzZd8ysrOzERMTI5X5999/ZefTzirQLaM/gyQ7OxthYWEIDAysjUsyqjavtaCgADt37sSePXvwyCOPAKj8ABFCwMfHB7/99huGDBlSy1dYrbZ/rvPnz0e/fv0wa9YsAECnTp0QHByMAQMG4NVXX5V+z+tCTa7VGlu2bMGNN96IhQsX4u6777ZH1a1W29caGxsLHx8f2Ydj27ZtAVTO6GvdunXNL8JC9rjWw4cP4/rrr8e0adPw/PPPy465y71Jy9i12vXeZPUoIBcHvcG9So4dOya8vLzExo0bjZZJSUkRKpVKNlju3LlzomXLlmLChAmioqLCXlW2WW1da15envD395cNZC4rKxNRUVHi448/tkvdLaHRaMT06dNFXFycOHbsmMFx7QC6H374Qdp39OhRxQGv2dnZUpmPP/5YhIWFiZKSEiFE5WDBDh06yM49ceLEOh0sWBfXqlarxYEDB2SPhx56SLRu3VocOHBANhujNtXVz3XcuHHitttuk537n3/+EQDE+fPna+PSDNjjWnWZGty5adMmERwcLD788EO71d8adXWtGzduFADEiRMnpH179+4VAERaWpp9LsYMe13rwYMHRVRUlJg1a5bi+7jLvUkI09dqz3uTRwQ9BQUFYs+ePWLPnj0CgHjnnXfEnj17xOnTp4UQQnz//fdi06ZNIj09XaxevVo0bdpUjBs3TnaOzz//XKSmpooTJ06Ir776SkRGRoqZM2dKx8+dOydatGghrr/+enHu3Dlx4cIF6eFu1yqEEI8//rho1KiR2Lhxozh69KiYOnWqiIqKEleuXKmza33ooYdEeHi42Lx5s+z/u7i4WCrz4IMPiiZNmog///xT7Ny5UyQlJYmkpCTpuHZq89ChQ8XevXvFhg0bRMOGDRWnrM+aNUscOXJELFq0qM6nhdbVtepzxOyturrWZcuWCR8fH7F48WKRnp4utm3bJnr06CF69erlUtcqhBDHjx8Xe/bsEQ888IBo1aqVdA/QzlT7888/RVBQkJg9e7bsfS5fvux216pWq0W3bt3EwIEDxe7du8XOnTtF7969xQ033OBS13rgwAHRsGFDceedd8rOkZOTI5Vxl3uTJdeqz9Z7k0cEPZs2bRIADB6TJ08WQgjx3nvvicaNGwtfX1/RpEkT8fzzzxtM4f3f//4noqOjha+vr2jZsqV4++23hUajkY4vW7ZM8T3qujGtLq5ViMqWnSeffFJERUWJ0NBQkZycLJsKXxeM/X8vW7ZMKnPt2jXx8MMPi4iICBEUFCTGjh1rEIhmZGSIESNGiMDAQNGgQQPx5JNPivLyclmZTZs2iS5dugg/Pz/RrFkz2XvUhbq8Vl2OCHrq8lrff/990a5dOxEYGChiY2PFpEmTxLlz5+riMoUQ9rvWQYMGKZ7n1KlTQgghJk+erHh80KBBbnetQghx/vx5MW7cOBESEiKio6PFlClT6jTAs8e1zpkzR/EcTZs2lb2XO9ybLL1WXbbem1RVlSYiIiJyax4/e4uIiIg8A4MeIiIi8ggMeoiIiMgjMOghIiIij8Cgh4iIiDwCgx4iIiLyCAx6iIiIyCMw6CEiIiKPwKCHiBxuypQpUKlUUKlU8PX1RXR0NG644QZ8/vnn0Gg0Fp9n+fLlskUNiYh0MeghIqcwfPhwXLhwARkZGVi/fj2uu+46PP744xg9ejQqKiocXT0icgMMeojIKfj7+yMmJgaNGjVCt27d8Oyzz2LNmjVYv349li9fDgB455130LFjRwQHByM+Ph4PP/wwCgsLAQCbN2/GPffcg7y8PKnV6KWXXgIAlJaW4qmnnkKjRo0QHByM3r17Y/PmzY65UCJyGAY9ROS0hgwZgs6dO2PVqlUAAC8vL7z//vs4dOgQvvjiC/z55594+umnAQB9+/bFu+++i7CwMFy4cAEXLlzAU089BQB45JFHkJqaim+//Rb79+/HrbfeiuHDh+P48eMOuzYiqntccJSIHG7KlCnIzc3F6tWrDY5NmDAB+/fvx+HDhw2O/fDDD3jwwQdx6dIlAJVjembMmIHc3FypzJkzZ9CsWTOcOXMGcXFx0v7k5GT06tULr732mt2vh4ick4+jK0BEZIoQAiqVCgDwxx9/YP78+Th69Cjy8/NRUVGBkpISFBcXIygoSPH1Bw4cgFqtRqtWrWT7S0tLUb9+/VqvPxE5DwY9ROTUjhw5gsTERGRkZGD06NF46KGHMG/ePERGRmLbtm2YOnUqysrKjAY9hYWF8Pb2xq5du+Dt7S07FhISUheXQEROgkEPETmtP//8EwcOHMATTzyBXbt2QaPR4O2334aXV+VwxO+//15W3s/PD2q1Wrava9euUKvVyMnJwYABA+qs7kTkfBj0EJFTKC0tRVZWFtRqNbKzs7FhwwbMnz8fo0ePxt13342DBw+ivLwcH3zwAW688Ub8/fff+Oijj2TnSEhIQGFhIVJSUtC5c2cEBQWhVatWmDRpEu6++268/fbb6Nq1Ky5evIiUlBR06tQJo0aNctAVE1Fd4+wtInIKGzZsQGxsLBISEjB8+HBs2rQJ77//PtasWQNvb2907twZ77zzDl5//XV06NAB33zzDebPny87R9++ffHggw/i9ttvR8OGDfHGG28AAJYtW4a7774bTz75JFq3bo0xY8bgv//+Q5MmTRxxqUTkIJy9RURERB6BLT1ERETkERj0EBERkUdg0ENEREQegUEPEREReQQGPUREROQRGPQQERGRR2DQQ0RERB6BQQ8RERF5BAY9RERE5BEY9BAREZFHYNBDREREHoFBDxEREXmE/wf32ZZYe5e+JAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_open(dataset):\n",
    "    plt.plot(dataset[\"Date\"], dataset[\"Open\"])\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Open\")\n",
    "    plt.title(f\"Open price: {dataset['Token'].iloc[0]}\")\n",
    "    plt.show()\n",
    "\n",
    "stock_data = load_dataset(\"BBBY\")\n",
    "plot_open(stock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bb0e0f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:48:43.703675Z",
     "iopub.status.busy": "2024-05-06T13:48:43.702803Z",
     "iopub.status.idle": "2024-05-06T13:48:43.712077Z",
     "shell.execute_reply": "2024-05-06T13:48:43.711123Z"
    },
    "papermill": {
     "duration": 0.034939,
     "end_time": "2024-05-06T13:48:43.714197",
     "exception": false,
     "start_time": "2024-05-06T13:48:43.679258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Scaler:\n",
    "    \"\"\"\n",
    "    Scales data to given range\n",
    "    \"\"\"\n",
    "    def __init__(self, min_output=0, max_output=1):\n",
    "        if max_output - min_output <= 0:\n",
    "            raise ValueError(\"'max_output' must be bigger than 'min_output'\")\n",
    "        \n",
    "        self._eps = 1e-9\n",
    "        self._min = min_output\n",
    "        self._max = max_output\n",
    "        self._min_input = None\n",
    "        self._max_input = None\n",
    "        \n",
    "    def fit(self, data):\n",
    "        self._min_input = np.quantile(data, 0.05)\n",
    "        self._max_input = np.quantile(data, 0.95)\n",
    "        \n",
    "    def scale(self, data):\n",
    "        return (data - self._min_input) / (self._max_input - self._min_input + self._eps) \\\n",
    "                * (self._max - self._min) + self._min \n",
    "    \n",
    "    def descale(self, data):\n",
    "        return (data - self._min) / (self._max - self._min) \\\n",
    "                * (self._max_input - self._min_input + self._eps) + self._min_input\n",
    "    \n",
    "    def get_range(self):\n",
    "        return self._min_input, self._max_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adfa47a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:48:43.755341Z",
     "iopub.status.busy": "2024-05-06T13:48:43.754917Z",
     "iopub.status.idle": "2024-05-06T13:48:43.764633Z",
     "shell.execute_reply": "2024-05-06T13:48:43.763628Z"
    },
    "papermill": {
     "duration": 0.032977,
     "end_time": "2024-05-06T13:48:43.766806",
     "exception": false,
     "start_time": "2024-05-06T13:48:43.733829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Getting Fear-Greed index data\n",
    "\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "def load_fear_greed_index_data():\n",
    "    current_data = pd.read_csv('/kaggle/input/fear-greed-data/fear-greed.csv')\n",
    "    current_data = current_data[['Date', 'Fear Greed']]\n",
    "\n",
    "\n",
    "    url = \"https://production.dataviz.cnn.io/index/fearandgreed/graphdata/2020-09-19\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        my_data = response.json()\n",
    "        historical_data = my_data[\"fear_and_greed_historical\"][\"data\"]\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "\n",
    "    for entry in historical_data:\n",
    "        timestamp = entry['x'] / 1000\n",
    "        date = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
    "        entry['Date'] = date\n",
    "\n",
    "    historical_data = pd.DataFrame(historical_data)\n",
    "\n",
    "    historical_data.drop(columns=['rating'], inplace=True)\n",
    "    historical_data.drop(columns=['x'], inplace=True)\n",
    "    historical_data.rename(columns={'y': 'Fear Greed'}, inplace=True)\n",
    "    \n",
    "    fear_greed_data = pd.concat([current_data, historical_data], ignore_index=True)\n",
    "    \n",
    "    return fear_greed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35a38b29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:48:43.807847Z",
     "iopub.status.busy": "2024-05-06T13:48:43.807328Z",
     "iopub.status.idle": "2024-05-06T13:48:43.976495Z",
     "shell.execute_reply": "2024-05-06T13:48:43.975691Z"
    },
    "papermill": {
     "duration": 0.192354,
     "end_time": "2024-05-06T13:48:43.978865",
     "exception": false,
     "start_time": "2024-05-06T13:48:43.786511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import holidays\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Factory class for `Features` - needed because we don't want to calculate general features multiple times\n",
    "    \"\"\"\n",
    "    def __init__(self, all_datasets, fear_greed_data):\n",
    "        self._general_features = GeneralFeatures(all_datasets, fear_greed_data)\n",
    "        self.token_to_index = {token: idx for idx, token in enumerate(set(token for dataset in all_datasets.values() for token in dataset[\"Token\"]))}\n",
    "    \n",
    "    def get_features(self, dataset):\n",
    "        return Features(dataset, self._general_features, self.token_to_index)\n",
    "    \n",
    "    def get_merged_features_dataframe(self, datasets):\n",
    "        features_list = [Features(dataset, self._general_features) for dataset in datasets.values()]\n",
    "        return Features.merge(features_list)\n",
    "    \n",
    "    \n",
    "class GeneralFeatures:\n",
    "    \"\"\"\n",
    "    Features that are not token-specific\n",
    "    \"\"\"\n",
    "    def __init__(self, all_datasets, fear_greed_data):\n",
    "        self._datasets = all_datasets\n",
    "        self._fear_greed_data = fear_greed_data\n",
    "\n",
    "        all_days = {day for dataset in self._datasets.values() for day in dataset[\"Date\"]}\n",
    "        self.for_day = self._calculate_daily_general_features(all_days)\n",
    "            \n",
    "    def _calculate_daily_general_features(self, days):\n",
    "        traded_value_by_day = self._calculate_traded_values_by_day(days)\n",
    "        fear_greed_index_by_day = self._calculate_fear_greed_index_by_day(days)\n",
    "        general_features = {day: GeneralFeatures.DailyGeneralFeatures(traded_value_by_day[day], fear_greed_index_by_day[day]) for day in days}\n",
    "        return general_features\n",
    "    \n",
    "    def _calculate_traded_values_by_day(self, days):\n",
    "        traded_value_by_day = {day: 0 for day in days}\n",
    "        dataset_count = len(self._datasets)\n",
    "        for dataset in self._datasets.values():\n",
    "            for _, row in dataset.iterrows():\n",
    "                traded_value_by_day[row[\"Date\"]] += row[\"Open\"] * row[\"Volume\"]\n",
    "        for day in traded_value_by_day:\n",
    "            traded_value_by_day[day] /= dataset_count\n",
    "        return traded_value_by_day\n",
    "\n",
    "    \n",
    "\n",
    "    def _calculate_fear_greed_index_by_day(self, days):\n",
    "        fear_greed_index_by_day = {}\n",
    "        self._fear_greed_data[\"Date\"] = pd.to_datetime(self._fear_greed_data[\"Date\"])\n",
    "        fear_greed_data_dates = self._fear_greed_data[\"Date\"]\n",
    "        fear_greed_data_dates = fear_greed_data_dates.sort_values()\n",
    "\n",
    "        for day in days:\n",
    "            if day in fear_greed_data_dates.values:\n",
    "                fear_greed_index_by_day[day] = self._fear_greed_data.loc[self._fear_greed_data[\"Date\"] == day, \"Fear Greed\"].iloc[0]\n",
    "            else:\n",
    "                fear_greed_index_by_day[day] = 50.0\n",
    "        return fear_greed_index_by_day\n",
    "\n",
    "    \n",
    "    class DailyGeneralFeatures:\n",
    "        def __init__(self, traded_value, fear_greed_index):\n",
    "            self.traded_value = traded_value\n",
    "            self.fear_greed_index = fear_greed_index\n",
    "\n",
    "        \n",
    "def relative_change(array):\n",
    "    return np.diff(array) / array[:-1]\n",
    "    \n",
    "\n",
    "class Features: \n",
    "    \"\"\"\n",
    "    Data that is used as model input (after transformations)\n",
    "    \"\"\"\n",
    "    def __init__(self, data, general_features, token_to_index):\n",
    "        self.open = np.array(data[\"Open\"].pct_change()[1:])\n",
    "        self.close = np.array(data[\"Close\"].pct_change()[1:])\n",
    "        self.high = np.array(data[\"High\"].pct_change()[1:])\n",
    "        self.low = np.array(data[\"Low\"].pct_change()[1:])\n",
    "        self.volume = np.log(np.array(data[\"Volume\"]))[1:]\n",
    "        self.day_of_week = (np.array(data[\"Date\"].dt.dayofweek) / 6)[1:]\n",
    "        self.day_of_year_sin = np.sin(2* np.pi * data[\"Date\"].dt.dayofyear / 366)[1:]\n",
    "        self.day_of_year_cos = np.cos(2* np.pi * data[\"Date\"].dt.dayofyear / 366)[1:]\n",
    "\n",
    "        self.fear_greed_index = np.array([general_features.for_day[day].fear_greed_index for day in data[\"Date\"]])[1:]\n",
    "        \n",
    "        self.token = data.iloc[0, data.columns.get_loc(\"Token\")]\n",
    "        self.token_index = token_to_index[self.token]\n",
    "        \n",
    "        # Additional data\n",
    "        self.date = np.array(data[\"Date\"])[1:]\n",
    "        \n",
    "    @staticmethod\n",
    "    def create_dataframe_for_autogluon(datasets, general_features):\n",
    "        next_id = 0\n",
    "        \n",
    "        token_ids = []\n",
    "        tokens = []\n",
    "        series_datasets = []\n",
    "        \n",
    "        for token, dataset in datasets.items():\n",
    "            token_id = f\"ID{next_id}_{token}\"\n",
    "            \n",
    "            tokens.append(token)\n",
    "            token_ids.append(token_id)\n",
    "            next_id += 1\n",
    "            \n",
    "            series_datasets.append(\n",
    "                pd.DataFrame(data={\n",
    "                    \"token_id\": [token_id for _ in range(len(dataset[\"Date\"]) - 1)],\n",
    "                    \"open_change\": dataset[\"Open\"].pct_change()[1:],\n",
    "                    \"close_change\": dataset[\"Close\"].pct_change()[1:],\n",
    "                    \"high_change\": dataset[\"High\"].pct_change()[1:],\n",
    "                    \"low_change\": dataset[\"Low\"].pct_change()[1:],\n",
    "                    \"volume_log\": np.log(np.array(dataset[\"Volume\"]))[1:],\n",
    "                    \"date\": dataset[\"Date\"][1:]\n",
    "                })\n",
    "            )\n",
    "            \n",
    "        merged_series = pd.concat(series_datasets, ignore_index=True, sort=False)\n",
    "        \n",
    "        past_covariates = lambda days: pd.DataFrame(data={\n",
    "            \"traded_value_change\": [0, *relative_change(np.array([general_features.for_day[day].traded_value for day in days]))],\n",
    "            \"fear_greed_index\" : [0, *(np.array([general_features.for_day[day].fear_greed_index for day in days])[1:])]\n",
    "        })\n",
    "        \n",
    "        static_features = pd.DataFrame(data={\n",
    "            \"token_id\": token_ids,\n",
    "            \"token\": tokens\n",
    "        })\n",
    "        \n",
    "        return merged_series, past_covariates, static_features\n",
    "            \n",
    "    @staticmethod\n",
    "    def get_holidays_data(dates, individual=False):\n",
    "        country_holidays = holidays.country_holidays(\n",
    "            country=\"US\", \n",
    "            years=range(dates.min().year, dates.max().year + 1)\n",
    "        )\n",
    "        \n",
    "        holidays_df = pd.get_dummies(pd.Series(country_holidays)).astype(float).reindex(dates.date).fillna(0)\n",
    "        \n",
    "        if individual:\n",
    "            return {name: holidays_df[name] for name in holidays_df.columns}\n",
    "        \n",
    "        return {\"holiday\": holidays_df.max(axis=1).values}\n",
    "        \n",
    "    @staticmethod\n",
    "    def dates_to_known_covariates(dates):\n",
    "        return pd.DataFrame(data={\n",
    "            \"day_of_week\": np.array(dates.dayofweek) / 6,\n",
    "            \"day_of_year_sin\": np.sin(2* np.pi * dates.dayofyear / 366),\n",
    "            \"day_of_year_cos\": np.cos(2* np.pi * dates.dayofyear / 366),\n",
    "            **Features.get_holidays_data(dates, individual=False)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "821dc0a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:48:44.019757Z",
     "iopub.status.busy": "2024-05-06T13:48:44.019362Z",
     "iopub.status.idle": "2024-05-06T13:48:44.037314Z",
     "shell.execute_reply": "2024-05-06T13:48:44.036421Z"
    },
    "papermill": {
     "duration": 0.040649,
     "end_time": "2024-05-06T13:48:44.039292",
     "exception": false,
     "start_time": "2024-05-06T13:48:43.998643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeatureEncoder:\n",
    "    \"\"\"\n",
    "    Transforms token datasets into model input (as a day-indexed array)\n",
    "    \"\"\"\n",
    "    \n",
    "    num_of_features = 9\n",
    "    \n",
    "    def __init__(self, feature_extractor):\n",
    "        self._feature_extractor = feature_extractor\n",
    "        self._open_scaler = Scaler(0, 1)\n",
    "        self._close_scaler = Scaler(0, 1)\n",
    "        self._high_scaler = Scaler(0, 1)\n",
    "        self._low_scaler = Scaler(0, 1)\n",
    "        self._volume_scaler = Scaler(0, 1)\n",
    "        self._fear_greed_index_scaler = Scaler(0, 1)\n",
    "        \n",
    "    def fit(self, datasets):        \n",
    "        features = [self._feature_extractor.get_features(dataset) for dataset in datasets.values()]\n",
    "        self._open_scaler.fit(np.concatenate([f.open for f in features]))\n",
    "        self._close_scaler.fit(np.concatenate([f.close for f in features]))\n",
    "        self._high_scaler.fit(np.concatenate([f.high for f in features]))\n",
    "        self._low_scaler.fit(np.concatenate([f.low for f in features]))\n",
    "        self._volume_scaler.fit(np.concatenate([f.volume for f in features]))\n",
    "        self._fear_greed_index_scaler.fit(np.concatenate([f.fear_greed_index for f in features]))\n",
    "    \n",
    "    def encode(self, data):\n",
    "        \"\"\"\n",
    "        Dataset => 2D array with features\n",
    "        \"\"\"\n",
    "        features = self._feature_extractor.get_features(data)\n",
    "        return np.column_stack((\n",
    "            self._open_scaler.scale(features.open),\n",
    "            self._close_scaler.scale(features.close),\n",
    "            self._high_scaler.scale(features.high),\n",
    "            self._low_scaler.scale(features.low),\n",
    "            self._volume_scaler.scale(features.volume),\n",
    "            features.day_of_week,\n",
    "            features.day_of_year_sin,\n",
    "            features.day_of_year_cos,\n",
    "            self._fear_greed_index_scaler.scale(features.fear_greed_index)\n",
    "        ))\n",
    "        \n",
    "    def encode_merged_dataframe_inplace(self, data):\n",
    "        data[\"open_change\"] = self._open_scaler.scale(np.array(data[\"open_change\"]))\n",
    "        data[\"close_change\"] = self._close_scaler.scale(np.array(data[\"close_change\"]))\n",
    "        data[\"high_change\"] = self._high_scaler.scale(np.array(data[\"high_change\"]))\n",
    "        data[\"low_change\"] = self._low_scaler.scale(np.array(data[\"low_change\"]))\n",
    "        data[\"volume_log\"] = self._volume_scaler.scale(np.array(data[\"volume_log\"]))\n",
    "        \n",
    "    def decode(self, outputs):\n",
    "        \"\"\"\n",
    "        Days x 3 array with predicted values => Days x 3 array with close/high/low %-changes\n",
    "        \"\"\"\n",
    "        result = outputs.copy()\n",
    "        result[:,0] = self._close_scaler.descale(result[:,0])\n",
    "        result[:,1] = self._high_scaler.descale(result[:,1])\n",
    "        result[:,2] = self._low_scaler.descale(result[:,2])\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def decode_many(self, many_outputs):\n",
    "        \"\"\"\n",
    "        N x (Days * 3) array with predicted values (raw model output) => N x Days x 3 array with close/high/low %-changes\n",
    "        \"\"\"\n",
    "        return np.array([self.decode(output.reshape((-1, 3))) for output in many_outputs])\n",
    "    \n",
    "    def print_scaler_ranges(self):\n",
    "        print(f\"Open: {self._open_scaler.get_range()}\")\n",
    "        print(f\"Close: {self._close_scaler.get_range()}\")\n",
    "        print(f\"High: {self._high_scaler.get_range()}\")\n",
    "        print(f\"Low: {self._low_scaler.get_range()}\")\n",
    "        print(f\"Volume: {self._volume_scaler.get_range()}\")\n",
    "        print(f\"Feer-Gread index: {self._fear_greed_index_scaler.get_range()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e735289",
   "metadata": {
    "_cell_guid": "22c2f8aa-8696-45cd-8717-18eed327b317",
    "_uuid": "7d516a7b-b484-4763-ac3f-6f0af6e3b357",
    "execution": {
     "iopub.execute_input": "2024-05-06T13:48:44.080178Z",
     "iopub.status.busy": "2024-05-06T13:48:44.079343Z",
     "iopub.status.idle": "2024-05-06T13:48:44.094178Z",
     "shell.execute_reply": "2024-05-06T13:48:44.093232Z"
    },
    "papermill": {
     "duration": 0.037437,
     "end_time": "2024-05-06T13:48:44.096254",
     "exception": false,
     "start_time": "2024-05-06T13:48:44.058817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Converting `FeatureEncoder` output into model input & output, train/test splitting\n",
    "\n",
    "def series_to_lstm_inputs(data, token, n_in=1, n_out=1):\n",
    "    \"\"\"\n",
    "    Converts a date-indexed feature array into sequences used as LSTM inputs and their corresponding outputs.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or 2D Numpy array.\n",
    "        n_in: Number of observations in each LSTM sequence (x).\n",
    "        n_out: Number of future observations as target LSTM output (y).\n",
    "    Returns:\n",
    "        x, y\n",
    "    \"\"\"\n",
    "    x, y, token_list = [], [], []\n",
    "    for i in range(len(data) - n_in - n_out + 1):\n",
    "        x.append(data[i:(i+n_in), :])\n",
    "        y.append(data[i + n_in:i + n_in + n_out, 1:4].flatten()) # columns 1 to 3 => 'Close', 'High' and 'Low' \n",
    "        token_list.append(token)\n",
    "    \n",
    "    return np.array(x), np.array(y), np.array(token_list)\n",
    "\n",
    "\n",
    "def divide_into_sets(features, trim=0.1, periods=5, test_size=0.2):\n",
    "    data = features[int(len(features) * trim):int(len(features) * (1 - trim))]\n",
    "    batch_size = len(data) // periods\n",
    "    test = []\n",
    "    train = []\n",
    "    for i in range(periods):\n",
    "        batch = data[batch_size * i:batch_size * (i+1)]\n",
    "        train.append(batch[:int(batch_size * (1 - test_size))])\n",
    "        test.append(batch[int(batch_size * (1 - test_size)):])\n",
    "        \n",
    "    return train, test\n",
    "\n",
    "\n",
    "def sets_to_inputs(train_sets, test_sets, train_tokens, test_tokens, days_in=5, days_out=1):\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    train_token_list = []\n",
    "    for i, train_set in enumerate(train_sets):\n",
    "        if train_tokens is None:\n",
    "            x, y, _ = series_to_lstm_inputs(train_set, 0, days_in, days_out)\n",
    "        else:\n",
    "            x, y, tokens = series_to_lstm_inputs(train_set, train_tokens[i], days_in, days_out)\n",
    "            train_token_list.extend(tokens)\n",
    "        train_x.extend(x)\n",
    "        train_y.extend(y)\n",
    "\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    test_token_list = []\n",
    "    for i, test_set in enumerate(test_sets):\n",
    "        if test_tokens is None:\n",
    "            x, y, _ = series_to_lstm_inputs(test_set, 0, days_in, days_out)\n",
    "        else:\n",
    "            x, y, tokens = series_to_lstm_inputs(test_set, test_tokens[i], days_in, days_out)\n",
    "            test_token_list.extend(tokens)\n",
    "        test_x.extend(x)\n",
    "        test_y.extend(y)\n",
    "    \n",
    "    return np.array(train_x), np.array(train_y), np.array(test_x), np.array(test_y), np.array(train_token_list), np.array(test_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d22933d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:48:44.136586Z",
     "iopub.status.busy": "2024-05-06T13:48:44.135864Z",
     "iopub.status.idle": "2024-05-06T13:49:22.526375Z",
     "shell.execute_reply": "2024-05-06T13:49:22.525430Z"
    },
    "papermill": {
     "duration": 38.431958,
     "end_time": "2024-05-06T13:49:22.547520",
     "exception": false,
     "start_time": "2024-05-06T13:48:44.115562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of datasets: 150\n"
     ]
    }
   ],
   "source": [
    "# def load_datasets(number, training_limit='2022-06-12', check_split=False):\n",
    "#     split_date = None\n",
    "#     if check_split:\n",
    "#         split_date = training_limit\n",
    "#     datasets = {s[\"Token\"].iloc[0]: s for _, s in zip(range(number), filter_invalid(csv_nasdaq_datasets(), split_date=training_limit))}\n",
    "#     train_datasets = {s[\"Token\"].iloc[0]: s for s in only_before(datasets.values(), training_limit)}\n",
    "#     validation_datasets = {s[\"Token\"].iloc[0]: s for s in only_after(datasets.values(), training_limit)}\n",
    "#     fear_greed_index_data = load_fear_greed_index_data()\n",
    "#     return train_datasets, validation_datasets, datasets, fear_greed_index_data\n",
    "\n",
    "\n",
    "class DatasetLoader:\n",
    "    def __init__(self, number, training_limit='2022-06-12', check_split=False):\n",
    "        self.number = number\n",
    "        self.training_limit = training_limit\n",
    "        self.check_split = check_split\n",
    "        self.datasets = self.load_datasets()\n",
    "        self.train_datasets, self.validation_datasets = self.split_datasets()\n",
    "        self.fear_greed_index_data = load_fear_greed_index_data()\n",
    "\n",
    "    def load_datasets(self):\n",
    "        split_date = self.training_limit if self.check_split else None\n",
    "        datasets = {s[\"Token\"].iloc[0]: s for _, s in zip(range(self.number), filter_invalid(csv_nasdaq_datasets(), split_date=split_date))}\n",
    "        return datasets\n",
    "\n",
    "    def split_datasets(self):\n",
    "        train_datasets = {s[\"Token\"].iloc[0]: s for s in only_before(self.datasets.values(), self.training_limit)}\n",
    "        validation_datasets = {s[\"Token\"].iloc[0]: s for s in only_after(self.datasets.values(), self.training_limit)}\n",
    "        return train_datasets, validation_datasets\n",
    "\n",
    "    def get_datasets(self):\n",
    "        return self.train_datasets, self.validation_datasets, self.datasets, self.fear_greed_index_data\n",
    "\n",
    "dataset_loader = DatasetLoader(number=150, training_limit='2022-06-12', check_split=True)\n",
    "print(f\"Num of datasets: {len(dataset_loader.get_datasets()[2])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "755c04ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:49:22.588086Z",
     "iopub.status.busy": "2024-05-06T13:49:22.587471Z",
     "iopub.status.idle": "2024-05-06T13:49:22.601997Z",
     "shell.execute_reply": "2024-05-06T13:49:22.601126Z"
    },
    "papermill": {
     "duration": 0.036875,
     "end_time": "2024-05-06T13:49:22.603957",
     "exception": false,
     "start_time": "2024-05-06T13:49:22.567082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Converting token datasets into model input & output\n",
    "\n",
    "def datasets_to_training_data(datasets, days_in, days_out, extractor):    \n",
    "    encoder = FeatureEncoder(extractor)\n",
    "    encoder.fit(datasets)\n",
    "        \n",
    "    train_sets = []\n",
    "    test_sets = []\n",
    "    train_tokens = []\n",
    "    test_tokens = []\n",
    "    \n",
    "    for token, dataset in datasets.items():\n",
    "        features = encoder.encode(dataset)\n",
    "        \n",
    "        nan_count = np.count_nonzero(np.isnan(features))\n",
    "        if nan_count > 0:\n",
    "            print(f\"NaNs in {token}: {nan_count}\")\n",
    "            continue\n",
    "            \n",
    "        train, test = divide_into_sets(features)\n",
    "        train_sets.extend(train)\n",
    "        test_sets.extend(test)\n",
    "        \n",
    "        train_tokens.extend([extractor.token_to_index[token]] * len(train))\n",
    "        test_tokens.extend([extractor.token_to_index[token]] * len(test))\n",
    "    return *sets_to_inputs(train_sets, test_sets, train_tokens, test_tokens, days_in, days_out), encoder\n",
    "\n",
    "\n",
    "def datasets_to_validation_data(datasets, days_in, days_out, encoder):\n",
    "    features_sets = []\n",
    "    tokens = []\n",
    "    \n",
    "    for token, dataset in datasets.items():\n",
    "        features = encoder.encode(dataset)\n",
    "        nan_count = np.count_nonzero(np.isnan(features))\n",
    "        if nan_count > 0:\n",
    "            print(f\"NaNs in {token}: {nan_count}\")\n",
    "            continue\n",
    "        features_sets.append(features)\n",
    "        \n",
    "        tokens.extend([encoder._feature_extractor.token_to_index[token]] * len(features))\n",
    "    tokens = np.array(tokens)\n",
    "\n",
    "        \n",
    "    _, _, x, y, _, tokens = sets_to_inputs([], features_sets, [], tokens, days_in, days_out)\n",
    "    return x, y, tokens\n",
    "\n",
    "\n",
    "def datasets_to_encoded_data(datasets, days_in, days_out, extractor): \n",
    "    \"\"\"\n",
    "    Does the same thing as `datasets_to_training_data`, but does not divide data into train/test sets.\n",
    "    Used in auto parameter search (training data is divided by the optimizer).\n",
    "    \"\"\"\n",
    "    encoder = FeatureEncoder(extractor)\n",
    "    encoder.fit(datasets)\n",
    "        \n",
    "    train_sets = []\n",
    "    train_tokens = []\n",
    "    \n",
    "    for token, dataset in datasets.items():\n",
    "        features = encoder.encode(dataset)\n",
    "        \n",
    "        nan_count = np.count_nonzero(np.isnan(features))\n",
    "        if nan_count > 0:\n",
    "            print(f\"NaNs in {token}: {nan_count}\")\n",
    "            continue\n",
    "\n",
    "        train_sets.extend([features])\n",
    "        train_tokens.extend([extractor.token_to_index[token]] * len([features]))\n",
    "    \n",
    "    x, y, _, _, tokens, _ = sets_to_inputs(train_sets, [], train_tokens, None, days_in, days_out)\n",
    "    return x, y, tokens, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b4666e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:49:22.644408Z",
     "iopub.status.busy": "2024-05-06T13:49:22.643702Z",
     "iopub.status.idle": "2024-05-06T13:49:22.651286Z",
     "shell.execute_reply": "2024-05-06T13:49:22.650391Z"
    },
    "papermill": {
     "duration": 0.029977,
     "end_time": "2024-05-06T13:49:22.653367",
     "exception": false,
     "start_time": "2024-05-06T13:49:22.623390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_weight(x, y, eps=1):\n",
    "    # Range: eps --- eps+1\n",
    "    return np.mean(np.abs(np.mean(x[:, 1:4], axis=0) - np.mean(y.reshape((days_out, 3)), axis=0))) + eps\n",
    "\n",
    "\n",
    "def calculate_weights(xs, ys, eps, show=False):\n",
    "    weights = np.array([calculate_weight(x, y, eps) for x, y in zip(xs, ys)])\n",
    "    weights /= np.mean(weights) # normalize to not disturb learning rate\n",
    "\n",
    "    if show:\n",
    "        plt.plot(range(len(weights)), sorted(weights))\n",
    "        plt.show()\n",
    "        print(f\"Range: {min(weights)} to {max(weights)}\")\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40600cd7",
   "metadata": {
    "_cell_guid": "0eb8ad79-6bde-424c-8014-960cda4ebd10",
    "_uuid": "a4cd9876-07d3-4e02-9005-2efe929765a7",
    "execution": {
     "iopub.execute_input": "2024-05-06T13:49:22.693420Z",
     "iopub.status.busy": "2024-05-06T13:49:22.693062Z",
     "iopub.status.idle": "2024-05-06T13:50:14.295103Z",
     "shell.execute_reply": "2024-05-06T13:50:14.294338Z"
    },
    "papermill": {
     "duration": 51.624585,
     "end_time": "2024-05-06T13:50:14.297496",
     "exception": false,
     "start_time": "2024-05-06T13:49:22.672911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "days_in = 10\n",
    "days_out = 5\n",
    "\n",
    "# train_datasets, validation_datasets, full_datasets, fear_greed_data = load_datasets(30, training_limit='2021-12-12')\n",
    "train_datasets, validation_datasets, full_datasets, fear_greed_data = dataset_loader.get_datasets()\n",
    "extractor = FeatureExtractor(full_datasets, fear_greed_data)\n",
    "train_x, train_y, test_x, test_y, train_token, test_token, encoder = datasets_to_training_data(train_datasets, days_in, days_out, extractor)\n",
    "validation_x, validation_y, validation_token = datasets_to_validation_data(validation_datasets, days_in, days_out, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4927690",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:50:14.338302Z",
     "iopub.status.busy": "2024-05-06T13:50:14.337977Z",
     "iopub.status.idle": "2024-05-06T13:50:17.629155Z",
     "shell.execute_reply": "2024-05-06T13:50:17.628243Z"
    },
    "papermill": {
     "duration": 3.313495,
     "end_time": "2024-05-06T13:50:17.631312",
     "exception": false,
     "start_time": "2024-05-06T13:50:14.317817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open: (-0.04766429721533205, 0.04973194395634328)\n",
      "Close: (-0.045751607567268995, 0.048106684297454974)\n",
      "High: (-0.0410959154652657, 0.04353815291152492)\n",
      "Low: (-0.044005358758616, 0.044354847920177)\n",
      "Volume: (9.971146201009942, 16.069461477776276)\n",
      "Feer-Gread index: (14.0, 78.0)\n",
      "575930 values\n",
      "Mean: 0.4966654056061715\n",
      "Stdev: 0.36784506126851096\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Input data inspection\n",
    "\n",
    "encoder.print_scaler_ranges()\n",
    "\n",
    "column = 0\n",
    "values = []\n",
    "for dataset in train_datasets.values():\n",
    "    values.extend(encoder.encode(dataset)[:, column])\n",
    "    \n",
    "print(f\"{len(values)} values\")\n",
    "print(f\"Mean: {np.mean(values)}\")\n",
    "print(f\"Stdev: {np.std(values)}\")\n",
    "    \n",
    "t = [i for i in range(len(values))]\n",
    "plt.plot(t, sorted(values))\n",
    "plt.title(\"Feature distribution\")\n",
    "#plt.ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49c00f62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:50:17.671586Z",
     "iopub.status.busy": "2024-05-06T13:50:17.671273Z",
     "iopub.status.idle": "2024-05-06T13:50:17.687262Z",
     "shell.execute_reply": "2024-05-06T13:50:17.686365Z"
    },
    "papermill": {
     "duration": 0.038336,
     "end_time": "2024-05-06T13:50:17.689295",
     "exception": false,
     "start_time": "2024-05-06T13:50:17.650959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Input data analysis\n",
    "\n",
    "# Fear Greed Index Over Time plot\n",
    "def plot_feer_gread_index_over_time(fear_greed_data):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fear_greed_data[\"Date\"], fear_greed_data[\"Fear Greed\"])\n",
    "    plt.title('Fear Greed Index Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Fear Greed Index')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'feer_gread_index_over_time.png')\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot the Fear Greed Index over time with a rolling mean.\n",
    "def plot_rolling_mean_fear_greed_index(fear_greed_data, window=30):\n",
    "    fear_greed_data_indexed = fear_greed_data.copy()\n",
    "    fear_greed_data_indexed.set_index('Date', inplace=True)\n",
    "    \n",
    "    rolling_mean = fear_greed_data_indexed['Fear Greed'].rolling(window=window).mean()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fear_greed_data_indexed.index, fear_greed_data_indexed['Fear Greed'], label='Fear Greed Index', color='blue', alpha=0.2)\n",
    "    plt.plot(rolling_mean.index, rolling_mean, label=f'Rolling Mean ({window} days)', color='red')\n",
    "    plt.title('Fear Greed Index with Rolling Mean')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Fear Greed Index')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'rolling_mean_fear_greed_index.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72b8c70e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:50:17.729733Z",
     "iopub.status.busy": "2024-05-06T13:50:17.729429Z",
     "iopub.status.idle": "2024-05-06T13:50:17.740424Z",
     "shell.execute_reply": "2024-05-06T13:50:17.739559Z"
    },
    "papermill": {
     "duration": 0.033578,
     "end_time": "2024-05-06T13:50:17.742411",
     "exception": false,
     "start_time": "2024-05-06T13:50:17.708833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_close_price_over_time(token, dataset):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(dataset['Date'], dataset['Close'], label='Close Price')\n",
    "    plt.title(f'Close Price Over Time - {token}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'close_price_over_time_{token}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_volume_distribution(token, dataset):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(dataset['Volume'], bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    plt.title(f'Volume Distribution - {token}')\n",
    "    plt.xlabel('Volume')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'volume_distribution_{token}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_rolling_mean_close_price(token, dataset, window=30):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    rolling_mean = dataset['Close'].rolling(window=window).mean()\n",
    "    plt.plot(dataset['Date'], dataset['Close'], label='Close Price', alpha=0.4)\n",
    "    plt.plot(dataset['Date'], rolling_mean, label=f'{window}-Day Rolling Mean', color='red')\n",
    "    plt.title(f'Close Price with {window}-Day Rolling Mean - {token}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'rolling_mean_close_price_{token}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "041ff1ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:50:17.782091Z",
     "iopub.status.busy": "2024-05-06T13:50:17.781839Z",
     "iopub.status.idle": "2024-05-06T13:50:17.813089Z",
     "shell.execute_reply": "2024-05-06T13:50:17.812326Z"
    },
    "papermill": {
     "duration": 0.053383,
     "end_time": "2024-05-06T13:50:17.814984",
     "exception": false,
     "start_time": "2024-05-06T13:50:17.761601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_traded_value_over_time(token, traded_value_by_day):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(traded_value_by_day['Date'], traded_value_by_day['Traded Value'], label='Traded Value')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Traded Value')\n",
    "    plt.title(f'Traded Value Over Time - {token}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.savefig(f'traded_value_over_time_{token}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_rolling_mean_traded_value(token, traded_value_by_day, window=30):\n",
    "    rolling_mean = traded_value_by_day['Traded Value'].rolling(window=window).mean()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(traded_value_by_day['Date'], traded_value_by_day['Traded Value'], label='Traded Value', alpha=0.4)\n",
    "    plt.plot(traded_value_by_day['Date'], rolling_mean, label=f'Rolling Mean ({window} days)', color='red')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Rolling Mean Traded Value')\n",
    "    plt.title(f'Rolling Mean ({window} days) Traded Value - {token}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.savefig(f'rolling_mean_traded_value_{token}.png')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_corelation_close_price_and_traded_value(token, traded_value_by_day, dataset, window=30):\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Close Price', color=color)\n",
    "    ax1.plot(dataset['Date'], dataset['Close'], color=color, label='Close Price')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()  \n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Traded Value', color=color)  \n",
    "    ax2.plot(traded_value_by_day['Date'], traded_value_by_day['Traded Value'], color=color, label='Traded Value', alpha=0.5)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.set_yscale('log')\n",
    "    plt.title(f'Correlation Between Close Price and Traded Value Over Time - {token}')\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f'corelation_close_price_and_traded_value_{token}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.clf()\n",
    "    \n",
    "    rolling_mean_close_price = dataset['Close'].rolling(window=window).mean()\n",
    "    rolling_mean_traded_value_by_day = traded_value_by_day['Traded Value'].rolling(window=window).mean()\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Date', fontsize=18)\n",
    "    ax1.set_ylabel('Close Price', color=color, fontsize=18)\n",
    "    ax1.plot(dataset['Date'], rolling_mean_close_price, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()  \n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Traded Value', color=color, fontsize=18)  \n",
    "    ax2.plot(traded_value_by_day['Date'], rolling_mean_traded_value_by_day, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.set_yscale('log')\n",
    "#     plt.title(f'Close Price and Traded Value Over Time ({window}-Day Rolling Mean) - {token}', fontsize=14)\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f'corelation_close_price_and_traded_value_rolling_{token}.png')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_corelation_close_price_and_traded_volume(token, dataset, window=30):\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Date', fontsize=18)\n",
    "    ax1.set_ylabel('Close Price', color=color, fontsize=18)\n",
    "    ax1.plot(dataset['Date'], dataset['Close'], color=color, label='Close Price')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()  \n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Traded Volume', color=color, fontsize=18)  \n",
    "    ax2.plot(dataset['Date'], dataset['Volume'], color=color, label='Traded Volume', alpha=0.5)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.set_yscale('log')\n",
    "#     plt.title(f'Correlation Between Close Price and Traded Volume Over Time - {token}')\n",
    "    fig.tight_layout()\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.savefig(f'corelation_close_price_and_traded_volume_{token}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.clf()\n",
    "    \n",
    "    rolling_mean_close_price = dataset['Close'].rolling(window=window).mean()\n",
    "    rolling_mean_traded_volume_by_day = dataset['Volume'].rolling(window=window).mean()\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Date', fontsize=18)\n",
    "    ax1.set_ylabel('Close Price', color=color, fontsize=18)\n",
    "    ax1.plot(dataset['Date'], rolling_mean_close_price, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color, labelsize=16)\n",
    "\n",
    "    ax2 = ax1.twinx()  \n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Traded Volume', color=color, fontsize=18)  \n",
    "    ax2.plot(dataset['Date'], rolling_mean_traded_volume_by_day, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color, labelsize=16)\n",
    "    ax2.set_yscale('log')\n",
    "#     plt.title(f'Close Price and Traded Volume Over Time ({window}-Day Rolling Mean) - {token}', fontsize=18)\n",
    "    ax1.tick_params(axis='x', labelsize=16)\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f'plot_close_price_and_traded_volume_rolling_{token}.png')\n",
    "    plt.show()\n",
    "    \n",
    "def analyze_traded_value_by_day(token, dataset, window=30):\n",
    "    traded_value_by_day = pd.DataFrame(columns=['Date', 'Traded Value'])\n",
    "    for _, row in dataset.iterrows():\n",
    "        traded_value = row[\"Open\"] * row[\"Volume\"]\n",
    "        traded_value_by_day = pd.concat([traded_value_by_day, \n",
    "                                         pd.DataFrame({'Date': [row[\"Date\"]], 'Traded Value': [traded_value]})], ignore_index=True)\n",
    "    plot_traded_value_over_time(token, traded_value_by_day)\n",
    "    plot_rolling_mean_traded_value(token, traded_value_by_day, window)\n",
    "    plot_corelation_close_price_and_traded_value(token, traded_value_by_day, dataset, window)\n",
    "    plot_corelation_close_price_and_traded_volume(token, dataset, window)\n",
    "    \n",
    "def analyze_daily_returns(token, dataset):\n",
    "    daily_return = dataset['Adjusted Close'].pct_change()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(dataset['Date'], daily_return, label=f'Daily Return for {token}', linestyle='--', marker='o')\n",
    "    plt.title(f'Daily Returns for {token}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Daily Return')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'daily_returns_{token}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.clf()\n",
    "    \n",
    "    daily_return = daily_return.dropna()\n",
    "    daily_return = daily_return[np.isfinite(daily_return)]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    daily_return.hist(bins=50)\n",
    "    plt.xlabel('Daily Return')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.title(f'Daily Return Histogram for {token}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'daily_returns_histogram_{token}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5163f30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:50:17.897314Z",
     "iopub.status.busy": "2024-05-06T13:50:17.896987Z",
     "iopub.status.idle": "2024-05-06T13:50:17.903079Z",
     "shell.execute_reply": "2024-05-06T13:50:17.902111Z"
    },
    "papermill": {
     "duration": 0.070617,
     "end_time": "2024-05-06T13:50:17.905059",
     "exception": false,
     "start_time": "2024-05-06T13:50:17.834442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_datasets_data(full_datasets, count=4, window=30):\n",
    "    datasets_list = list(full_datasets.items())\n",
    "    if count > len(datasets_list):\n",
    "        count = len(datasets_list)\n",
    "    for token, dataset in datasets_list[:count]:\n",
    "        plot_close_price_over_time(token, dataset)\n",
    "        plot_rolling_mean_close_price(token, dataset, window)\n",
    "        plot_volume_distribution(token, dataset)\n",
    "        analyze_traded_value_by_day(token, dataset, window)\n",
    "        analyze_daily_returns(token, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "884e5d6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:50:17.945430Z",
     "iopub.status.busy": "2024-05-06T13:50:17.945128Z",
     "iopub.status.idle": "2024-05-06T13:50:18.677369Z",
     "shell.execute_reply": "2024-05-06T13:50:18.676614Z"
    },
    "papermill": {
     "duration": 0.754836,
     "end_time": "2024-05-06T13:50:18.679652",
     "exception": false,
     "start_time": "2024-05-06T13:50:17.924816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_correlation_heatmap(data, tokens, title):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    heatmap = sns.heatmap(data.corr(), annot=True, cmap='summer', annot_kws={\"size\": 10})\n",
    "    plt.xticks(ticks=range(len(tokens)), labels=tokens, rotation=45, ha='left', fontsize=13)\n",
    "    plt.yticks(ticks=range(len(tokens)), labels=tokens, va='top', fontsize=13)\n",
    "    cbar = heatmap.collections[0].colorbar\n",
    "    cbar.ax.tick_params(labelsize=11)\n",
    "#     plt.title(title)\n",
    "    plt.savefig(f'{title}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_correlation_for_token_list(tokens, title, start=\"2018-01-01\", end=\"2022-12-12\"): #2023-12-01\n",
    "    first_dataset = load_dataset(tokens[0])\n",
    "    first_dataset = first_dataset.loc[(first_dataset['Date'] >= start) & (first_dataset['Date'] <= end)]\n",
    "    first_dataset.reset_index(drop=True, inplace=True)\n",
    "    closing_df = pd.DataFrame(index=first_dataset.index)\n",
    "    for token in tokens:\n",
    "        dataset = load_dataset(token)\n",
    "        dataset = dataset.loc[(dataset['Date'] >= start) & (dataset['Date'] <= end)]\n",
    "        dataset.reset_index(drop=True, inplace=True)\n",
    "        closing_df[token] = dataset['Adjusted Close']\n",
    "#     plot_correlation_heatmap(closing_df.pct_change(), tokens, 'Correlation of stock return for ' + title)\n",
    "    plot_correlation_heatmap(closing_df, tokens, 'Correlation of stock closing price for ' + title)\n",
    "\n",
    "def analyze_correlation_between_tokens():\n",
    "    tech_tokens = ['AMZN', 'AAPL', 'NVDA', 'MSFT']\n",
    "    bank_tokens = ['ISTR', 'AMNB', 'WTFC', 'FRBA']\n",
    "    healthcare_tokens = ['XENE', 'REGN', 'VRTX', 'AMGN']\n",
    "    energy_tokens = ['MGEE', 'PLUG', 'WWD', 'LFUS']\n",
    "    telecommunications_tokens = ['CSCO', 'CMCSA', 'CHTR', 'LBRDA']\n",
    "    \n",
    "    plot_correlation_for_token_list(tech_tokens, 'technology tokens')\n",
    "#     plot_correlation_for_token_list(bank_tokens, 'bank tokens')\n",
    "    plot_correlation_for_token_list(healthcare_tokens, 'healthcare tokens')\n",
    "#     plot_correlation_for_token_list(energy_tokens, 'energy tokens')\n",
    "#     plot_correlation_for_token_list(telecommunications_tokens, 'telecommunications tokens')\n",
    "    plot_correlation_for_token_list(tech_tokens[-3:] + bank_tokens[-3:] + healthcare_tokens[-3:] + energy_tokens[-3:] + telecommunications_tokens[-3:], 'technology, bank, healthcare, energy, telecommunications tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f7f2c60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:50:18.721824Z",
     "iopub.status.busy": "2024-05-06T13:50:18.721525Z",
     "iopub.status.idle": "2024-05-06T13:50:18.742480Z",
     "shell.execute_reply": "2024-05-06T13:50:18.741621Z"
    },
    "papermill": {
     "duration": 0.044612,
     "end_time": "2024-05-06T13:50:18.744439",
     "exception": false,
     "start_time": "2024-05-06T13:50:18.699827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_average_volume_by_day_of_week(datasets):\n",
    "    average_volume_by_day_list = []\n",
    "    \n",
    "    for token, dataset in datasets:\n",
    "        dataset['Day of Week'] = dataset['Date'].dt.dayofweek\n",
    "        average_volume_by_day_token = dataset.groupby('Day of Week')['Volume'].mean()\n",
    "        average_volume_by_day_list.append(pd.DataFrame({token: average_volume_by_day_token}))\n",
    "        \n",
    "    average_volume_by_day = pd.concat(average_volume_by_day_list, axis=1)\n",
    "    average_volume_by_day['Average'] = average_volume_by_day.mean(axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(13, 11.5))\n",
    "    plt.bar(average_volume_by_day.index, average_volume_by_day['Average'])\n",
    "#     plt.title('Average Volume by Day of Week', fontsize=20)\n",
    "    plt.xlabel('Day of Week', fontsize=18)\n",
    "    plt.ylabel('Average Volume', fontsize=18)\n",
    "    plt.xticks(range(5), ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'], fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.savefig('average_volume_by_day_of_week.png')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_average_volume_by_day_of_year(datasets):\n",
    "    average_volume_by_day_list = []\n",
    "    \n",
    "    for token, dataset in datasets:\n",
    "        dataset['Day of Week'] = dataset['Date'].dt.dayofyear\n",
    "        average_volume_by_day_token = dataset.groupby('Day of Week')['Volume'].mean()\n",
    "        average_volume_by_day_list.append(pd.DataFrame({token: average_volume_by_day_token}))\n",
    "        \n",
    "    average_volume_by_day = pd.concat(average_volume_by_day_list, axis=1)\n",
    "    average_volume_by_day['Average'] = average_volume_by_day.mean(axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(average_volume_by_day.index, average_volume_by_day['Average'])\n",
    "    plt.title('Average Volume by Day of Year', fontsize=14)\n",
    "    plt.xlabel('Day of Year', fontsize=14)\n",
    "    plt.ylabel('Average Volume', fontsize=14)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.savefig(f'average_volume_by_day_of_year.png')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_average_volume_by_month(datasets):\n",
    "    average_volume_by_month_list = []\n",
    "    \n",
    "    for token, dataset in datasets:\n",
    "        dataset['Month'] = dataset['Date'].dt.month\n",
    "        average_volume_by_month_token = dataset.groupby('Month')['Volume'].mean()\n",
    "        average_volume_by_month_list.append(pd.DataFrame({token: average_volume_by_month_token}))\n",
    "        \n",
    "    average_volume_by_month = pd.concat(average_volume_by_month_list, axis=1)\n",
    "    average_volume_by_month['Average'] = average_volume_by_month.mean(axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(13, 11.5))\n",
    "    plt.bar(average_volume_by_month.index, average_volume_by_month['Average'])\n",
    "#     plt.title('Average Day Volume by Month', fontsize=20)\n",
    "    plt.xlabel('Month', fontsize=18)\n",
    "    plt.ylabel('Average Volume', fontsize=18)\n",
    "    plt.xticks(range(1, 13), ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'], rotation=30, fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.savefig(f'average_volume_by_month.png')\n",
    "    plt.show()\n",
    "    \n",
    "def analyze_average_daily_returns(datasets):\n",
    "    daily_returns = {}\n",
    "\n",
    "    for token, dataset in datasets:\n",
    "        daily_return = dataset['Adjusted Close'].pct_change()\n",
    "        for date, return_value in zip(dataset['Date'], daily_return):\n",
    "            if date not in daily_returns:\n",
    "                daily_returns[date] = 0\n",
    "            daily_returns[date] += return_value\n",
    "\n",
    "    average_daily_returns = {date: total_return / len(datasets) for date, total_return in daily_returns.items()}\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(list(average_daily_returns.values()), bins=50, edgecolor='black')\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.xlabel('Average Daily Return', fontsize=18)\n",
    "    plt.ylabel('Frequency', fontsize=18)\n",
    "#     plt.title('Histogram of Average Daily Returns Across All Tokens', fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('average_daily_returns_histogram.png')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def analyze_average_volume(full_datasets):\n",
    "    plot_average_volume_by_day_of_week(list(full_datasets.items()))\n",
    "    plot_average_volume_by_day_of_year(list(full_datasets.items()))\n",
    "    plot_average_volume_by_month(list(full_datasets.items()))\n",
    "    analyze_average_daily_returns(list(full_datasets.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16e8239f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:50:18.784308Z",
     "iopub.status.busy": "2024-05-06T13:50:18.783753Z",
     "iopub.status.idle": "2024-05-06T13:50:18.789129Z",
     "shell.execute_reply": "2024-05-06T13:50:18.788304Z"
    },
    "papermill": {
     "duration": 0.02743,
     "end_time": "2024-05-06T13:50:18.791080",
     "exception": false,
     "start_time": "2024-05-06T13:50:18.763650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_input_data(num_of_datasets_to_load, num_of_datasets_to_show, dataset_loader, window=30):\n",
    "    train_datasets, validation_datasets, full_datasets, fear_greed_data = dataset_loader.get_datasets() # load_datasets(num_of_datasets_to_load)\n",
    "    analyze_datasets_data(full_datasets, num_of_datasets_to_show, window)\n",
    "    analyze_correlation_between_tokens()\n",
    "    analyze_average_volume(full_datasets)\n",
    "\n",
    "if False:\n",
    "    plot_feer_gread_index_over_time(fear_greed_data)\n",
    "    plot_rolling_mean_fear_greed_index(fear_greed_data, 30)\n",
    "    analyze_input_data(100, 4, dataset_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fe81d9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:50:18.830729Z",
     "iopub.status.busy": "2024-05-06T13:50:18.830468Z",
     "iopub.status.idle": "2024-05-06T13:50:18.862804Z",
     "shell.execute_reply": "2024-05-06T13:50:18.862005Z"
    },
    "papermill": {
     "duration": 0.054454,
     "end_time": "2024-05-06T13:50:18.864833",
     "exception": false,
     "start_time": "2024-05-06T13:50:18.810379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import joblib\n",
    "plt.savefig(f'.png')\n",
    "\n",
    "class ModelWrapper:\n",
    "    default_batch_size = 2048\n",
    "    \n",
    "    \"\"\"\n",
    "    Wrapper for Keras and scikit-learn models\n",
    "    \"\"\"\n",
    "    def __init__(self, model, is_keras_model, uses_token_input = False):\n",
    "        self.model = model\n",
    "        self.is_keras_model = is_keras_model\n",
    "        self.uses_token_input = uses_token_input\n",
    "        \n",
    "    def fit(self, train_x, train_y, test_x=None, test_y=None, epochs=30, batch_size=None, sample_weights=None, verbose=False):\n",
    "        if batch_size is None:\n",
    "            batch_size = ModelWrapper.default_batch_size\n",
    "        if not self.uses_token_input:\n",
    "                train_x = train_x[:, :, :-1]\n",
    "                if test_x is not None:\n",
    "                    test_x = test_x[:, :, :-1]\n",
    "                \n",
    "        if self.is_keras_model:\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=epochs//5, restore_best_weights=True)\n",
    "            validation_data = (test_x, test_y) if test_x is not None and test_y is not None else None\n",
    "            return self.model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, validation_data=validation_data, \\\n",
    "                             verbose=(1 if verbose else 0), shuffle=True, callbacks=[early_stopping], sample_weight=sample_weights)\n",
    "        \n",
    "        flat_train_x = train_x.reshape((train_x.shape[0], -1))\n",
    "        if 'sample_weight' in self.model.fit.__code__.co_varnames:\n",
    "            self.model.fit(flat_train_x, train_y, sample_weight=sample_weights)\n",
    "        else:\n",
    "            self.model.fit(flat_train_x, train_y)\n",
    "        return None\n",
    "    \n",
    "    def predict(self, x, batch_size=None, verbose=True):\n",
    "        if batch_size is None:\n",
    "            batch_size = ModelWrapper.default_batch_size\n",
    "        if not self.uses_token_input:\n",
    "                x = x[:, :, :-1]\n",
    "                \n",
    "        if self.is_keras_model:\n",
    "            return self.model.predict(x, batch_size=batch_size, verbose=(1 if verbose else 0))\n",
    "        \n",
    "        flat_x = x.reshape((x.shape[0], -1))\n",
    "        return self.model.predict(flat_x)\n",
    "    \n",
    "    def save(self, file_name):\n",
    "        if self.is_keras_model:\n",
    "            return self.model.save(f\"{file_name}.h5\", save_format=\"h5\", overwrite=True)\n",
    "        \n",
    "        return joblib.dump(self.model, f\"{file_name}.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10a9f6bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:50:18.905277Z",
     "iopub.status.busy": "2024-05-06T13:50:18.905033Z",
     "iopub.status.idle": "2024-05-06T13:50:18.978715Z",
     "shell.execute_reply": "2024-05-06T13:50:18.978028Z"
    },
    "papermill": {
     "duration": 0.096511,
     "end_time": "2024-05-06T13:50:18.980738",
     "exception": false,
     "start_time": "2024-05-06T13:50:18.884227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Trained Model Analysis\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def plot_actual_vs_predicted_prices(prices_before_prediction, actual_prices_percentage_changes, predicted_prices_percentage_changes, index, model_name=''):\n",
    "    actual_prices = actual_prices_percentage_changes[index, :, 0]\n",
    "    predicted_prices = predicted_prices_percentage_changes[index, :, 0]\n",
    "    actual_prices = np.concatenate([prices_before_prediction, actual_prices])\n",
    "    predicted_prices = np.concatenate([prices_before_prediction, predicted_prices])\n",
    "    \n",
    "    actual_prices_percentage_relative = np.cumprod(1 + actual_prices / 100)\n",
    "    predicted_prices_percentage_relative = np.cumprod(1 + predicted_prices / 100)\n",
    "\n",
    "    actual_prices_percentage_relative = np.insert(actual_prices_percentage_relative, 0, 1)\n",
    "    predicted_prices_percentage_relative = np.insert(predicted_prices_percentage_relative, 0, 1)\n",
    "    \n",
    "    fontsize = 17\n",
    "    \n",
    "    plt.figure(figsize=(11.5, 6))\n",
    "    plt.plot(actual_prices_percentage_relative * 100, label='Actual Price Change (%)', color='blue')\n",
    "    plt.plot(predicted_prices_percentage_relative * 100, label='Predicted Price Change (%)', color='orange')\n",
    "    plt.xlabel('Day', fontsize=fontsize)\n",
    "    plt.ylabel('Price (%)', fontsize=fontsize)\n",
    "#     plt.title('Actual vs Predicted Close Prices Changes')\n",
    "    plt.legend()\n",
    "    plt.xticks(np.arange(len(actual_prices) + 1), np.arange(len(actual_prices) + 1), fontsize=fontsize)\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "    plt.legend(fontsize=fontsize)\n",
    "    plt.savefig(f'evaluations{model_name}/actual_vs_predicted_prices_{index}.png')\n",
    "    plt.show()\n",
    "    \n",
    "def calculate_mae(prediction, actual):\n",
    "    return np.mean(np.abs(prediction - actual))\n",
    "\n",
    "def calculate_mse(prediction, actual):\n",
    "    return np.mean((prediction - actual) ** 2)\n",
    "\n",
    "def calculate_rmse(prediction, actual):\n",
    "    return np.sqrt(np.mean((prediction - actual) ** 2))\n",
    "\n",
    "def calculate_errors(predicted_prices, actual_prices):\n",
    "    mae = calculate_mae(predicted_prices, actual_prices)\n",
    "    mse = calculate_mse(predicted_prices, actual_prices)\n",
    "    rmse = calculate_rmse(predicted_prices, actual_prices)\n",
    "    \n",
    "    print(f'Mean Absolute Error (MAE): {mae}')\n",
    "    print(f'Mean Squared Error (MSE): {mse}')\n",
    "    print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "\n",
    "def plot_error_over_time(errors, model_name=''):\n",
    "    plt.figure(figsize=(11.5, 6))\n",
    "    fontsize = 17\n",
    "    days = range(1, len(errors) + 1)\n",
    "    plt.plot(days, errors, label='Error', color='red')\n",
    "    plt.xlabel('Day', fontsize=fontsize)\n",
    "    plt.ylabel('Error', fontsize=fontsize)\n",
    "#     plt.title('Average Error Over Time for Close Price')\n",
    "    plt.xticks(days, fontsize=fontsize)\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "    plt.legend(fontsize=fontsize)\n",
    "    plt.savefig(f'evaluations{model_name}/error_over_time.png')\n",
    "    plt.show()\n",
    "    \n",
    "def error_prediction(model, error_train_x, error_test_x, error_train_y, error_test_y, batch_size, verbose):\n",
    "    predictions_train = model.predict(error_train_x, batch_size=batch_size, verbose=verbose)\n",
    "    predictions_test = model.predict(error_test_x, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    decoded_predictions_train = encoder.decode_many(predictions_train)\n",
    "    decoded_predictions_test = encoder.decode_many(predictions_test)\n",
    "    decoded_error_train_y = encoder.decode_many(error_train_y)\n",
    "    decoded_error_test_y = encoder.decode_many(error_test_y)\n",
    "    \n",
    "    errors_train = np.abs(decoded_predictions_train - decoded_error_train_y)\n",
    "    errors_test = np.abs(decoded_predictions_test - decoded_error_test_y)\n",
    "    \n",
    "    errors_train = errors_train.reshape(errors_train.shape[0], -1)\n",
    "    \n",
    "    if isinstance(error_train_x, list):\n",
    "        error_train_x = error_train_x[0]\n",
    "    \n",
    "    error_model = KNeighborsRegressor(n_neighbors=20, weights=\"uniform\", p=2)\n",
    "    error_model.fit(error_train_x.reshape((error_train_x.shape[0], -1)), errors_train)\n",
    "    \n",
    "    predicted_errors = error_model.predict(error_test_x.reshape((error_test_x.shape[0], -1)))\n",
    "    \n",
    "    error_prediction_error = mean_squared_error(errors_test.reshape(errors_test.shape[0], -1), predicted_errors)\n",
    "    error_threshold_percent = 30\n",
    "    error_threshold_absolute = error_threshold_percent / 100 * np.abs(errors_test.reshape(errors_test.shape[0], -1))\n",
    "    error_threshold_absolute = np.max([np.mean(error_threshold_absolute),np.median(error_threshold_absolute)])\n",
    "    good_predictions = np.abs(errors_test.reshape(errors_test.shape[0], -1) - predicted_errors) < error_threshold_absolute\n",
    "    good_prediction_percentage = np.mean(good_predictions) * 100\n",
    "    \n",
    "    return good_prediction_percentage\n",
    "\n",
    "def plot_error_distribution(errors, model_name=''):\n",
    "    fontsize = 17\n",
    "    fig, ax = plt.subplots(figsize=(11.5, 6))\n",
    "    ax.hist(errors.flatten(), bins=40, color='blue', alpha=0.7)\n",
    "    ax.set_xlabel('Error', fontsize=fontsize)\n",
    "    ax.set_ylabel('Frequency', fontsize=fontsize)\n",
    "#     plt.hist(errors.flatten(), bins=40, color='blue', alpha=0.7, figsize=(11, 6))\n",
    "#     plt.xlabel('Error', fontsize=13)\n",
    "#     plt.ylabel('Frequency', fontsize=13)\n",
    "#     plt.title('Error Distribution')\n",
    "    plt.xticks(fontsize=fontsize)\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "    plt.savefig(f'evaluations{model_name}/error_distribution.png')\n",
    "    plt.show()\n",
    "\n",
    "def analyze_trained_model(model, encoder, validation_x, validation_y, batch_size=2048, verbose=False, print_analysis=True, model_name=''):\n",
    "    if print_analysis and model_name != '':\n",
    "        model_name = f\"/{model_name}\"\n",
    "        os.makedirs(f\"/kaggle/working/evaluations{model_name}\", exist_ok=True)\n",
    "    \n",
    "    predictions = model.predict(validation_x, batch_size=batch_size, verbose=verbose)\n",
    "    predicted_prices = encoder.decode_many(predictions)\n",
    "    actual_prices = encoder.decode_many(validation_y)\n",
    "    \n",
    "    non_relative_errors = predicted_prices - actual_prices\n",
    "    if print_analysis:\n",
    "        plot_error_distribution(non_relative_errors, model_name)\n",
    "    \n",
    "    errors = np.abs(predicted_prices - actual_prices)\n",
    "    \n",
    "    total_errors_per_stock = np.mean(errors, axis=(1, 2))\n",
    "    index_of_min_error_stock = np.argmin(total_errors_per_stock)\n",
    "    \n",
    "    if isinstance(validation_x, list):\n",
    "        if print_analysis:\n",
    "            plot_actual_vs_predicted_prices(encoder._close_scaler.descale(validation_x[0][index_of_min_error_stock, :, 1]), actual_prices, predicted_prices, index_of_min_error_stock, model_name)\n",
    "            plot_actual_vs_predicted_prices(encoder._close_scaler.descale(validation_x[0][0, :, 1]), actual_prices, predicted_prices, 0, model_name)\n",
    "            plot_actual_vs_predicted_prices(encoder._close_scaler.descale(validation_x[0][-1, :, 1]), actual_prices, predicted_prices, validation_x.shape[0]-1, model_name)\n",
    "        \n",
    "        split_index = len(validation_x[0]) // 2\n",
    "        error_train_x = [validation_x[0][:split_index], validation_x[1][:split_index]]\n",
    "        error_test_x = [validation_x[0][split_index:], validation_x[1][split_index:]]\n",
    "    else:\n",
    "        if print_analysis:\n",
    "            plot_actual_vs_predicted_prices(encoder._close_scaler.descale(validation_x[index_of_min_error_stock, :, 1]), actual_prices, predicted_prices, index_of_min_error_stock, model_name)\n",
    "            plot_actual_vs_predicted_prices(encoder._close_scaler.descale(validation_x[0, :, 1]), actual_prices, predicted_prices, 0, model_name)\n",
    "            plot_actual_vs_predicted_prices(encoder._close_scaler.descale(validation_x[-1, :, 1]), actual_prices, predicted_prices, validation_x.shape[0]-1, model_name)\n",
    "        \n",
    "        split_index = len(validation_x) // 2\n",
    "        error_train_x, error_test_x = validation_x[:split_index], validation_x[split_index:]\n",
    "        \n",
    "    error_train_y, error_test_y = validation_y[:split_index], validation_y[split_index:]\n",
    "    \n",
    "    good_prediction_percentage = error_prediction(model, error_train_x, error_test_x, error_train_y, error_test_y, batch_size, verbose)\n",
    "    \n",
    "    daily_averages = np.mean(errors[:, :, 0], axis=0)\n",
    "    if print_analysis:\n",
    "        plot_error_over_time(daily_averages, model_name)\n",
    "        \n",
    "        calculate_errors(predicted_prices, actual_prices)\n",
    "        print('Good error predictions percentage:', good_prediction_percentage)\n",
    "    \n",
    "    return non_relative_errors, daily_averages, good_prediction_percentage\n",
    "\n",
    "def analyze_trained_models_for_tokens(non_relative_errors, daily_averages, good_prediction_percentage, title=''):\n",
    "    if title != '':\n",
    "        title = f\"/{title}\"\n",
    "        os.makedirs(f\"/kaggle/working/evaluations{title}\", exist_ok=True)\n",
    "    plot_error_distribution(non_relative_errors, title)\n",
    "    plot_error_over_time(daily_averages, title)\n",
    "    print('Good error predictions percentage:', good_prediction_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff3f6ed3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:50:19.021781Z",
     "iopub.status.busy": "2024-05-06T13:50:19.021202Z",
     "iopub.status.idle": "2024-05-06T13:50:19.031783Z",
     "shell.execute_reply": "2024-05-06T13:50:19.030942Z"
    },
    "papermill": {
     "duration": 0.033538,
     "end_time": "2024-05-06T13:50:19.033879",
     "exception": false,
     "start_time": "2024-05-06T13:50:19.000341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Evaluating model performance on validation data\n",
    "\n",
    "def calculate_score(actual, expected):\n",
    "    \"\"\"\n",
    "    Returns weighted average of mean absolute error of predicted %-changes\n",
    "    \"\"\"\n",
    "    \n",
    "    error_score = 1e10\n",
    "    \n",
    "    def abs_error(actual_value, expected_value):\n",
    "        return np.mean(abs(actual_value - expected_value))\n",
    "    \n",
    "    def single_prediction_error(actual_value, expected_value):\n",
    "        weights = [0.7**i for i in range(len(actual_value))]\n",
    "        return sum(abs_error(a, e) * w for a, e, w in zip(actual_value, expected_value, weights, strict=True)) / sum(weights)\n",
    "    \n",
    "    if np.count_nonzero(np.isnan(actual)):\n",
    "        return error_score\n",
    "    \n",
    "    if any(a.shape != e.shape for a, e in zip(actual, expected, strict=True)):\n",
    "        return error_score\n",
    "    \n",
    "    return sum(single_prediction_error(a, e) for a, e in zip(actual, expected, strict=True)) / len(actual)\n",
    "\n",
    "def evaluate_trained_model(model, encoder, validation_x, validation_y, batch_size=2048, verbose=False, analyze_model=True, print_analysis=True, model_name=''):\n",
    "    predictions = model.predict(validation_x, batch_size=batch_size, verbose=verbose)\n",
    "    if analyze_model:\n",
    "        non_relative_errors, daily_averages, good_prediction_percentage = analyze_trained_model(model, encoder, validation_x, validation_y, batch_size, verbose, print_analysis, model_name)\n",
    "        if not print_analysis:\n",
    "            return calculate_score(encoder.decode_many(predictions), encoder.decode_many(validation_y)), non_relative_errors, daily_averages, good_prediction_percentage\n",
    "    return calculate_score(encoder.decode_many(predictions), encoder.decode_many(validation_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35157567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:50:19.073550Z",
     "iopub.status.busy": "2024-05-06T13:50:19.073215Z",
     "iopub.status.idle": "2024-05-06T13:50:19.082462Z",
     "shell.execute_reply": "2024-05-06T13:50:19.081795Z"
    },
    "papermill": {
     "duration": 0.031333,
     "end_time": "2024-05-06T13:50:19.084468",
     "exception": false,
     "start_time": "2024-05-06T13:50:19.053135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.losses import MeanAbsoluteError\n",
    "\n",
    "def make_lstm_model_without_embedding(days_in, days_out, vocabulary_size, embedding_dim,\n",
    "                    dropout_1=0.32755, neurons_1=29,\n",
    "                    dropout_2=0.36570, neurons_2=64,\n",
    "                    dropout_3=0.70769, neurons_3=88,\n",
    "                    neurons_4=46,\n",
    "                    neurons_5=259,\n",
    "                    learning_rate=0.09167, inv_beta_1=0.41481, inv_beta_2=0.11718,\n",
    "                    show=False):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(neurons_1, input_shape=(days_in, FeatureEncoder.num_of_features), return_sequences=True, dropout=dropout_1, recurrent_dropout=dropout_1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(neurons_2, return_sequences=True, dropout=dropout_2, recurrent_dropout=dropout_2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(neurons_3, dropout=dropout_3, recurrent_dropout=dropout_3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(neurons_4, activation='relu'))\n",
    "    model.add(Dense(neurons_5, activation='relu'))\n",
    "    model.add(Dense(3 * days_out))\n",
    "    model.compile(loss=MeanAbsoluteError(), optimizer=Adam(learning_rate=learning_rate, beta_1=1-inv_beta_1, beta_2=1-inv_beta_2))\n",
    "    \n",
    "    if show:\n",
    "        model.summary()\n",
    "        \n",
    "    return ModelWrapper(model, is_keras_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11e1f887",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:50:19.124173Z",
     "iopub.status.busy": "2024-05-06T13:50:19.123904Z",
     "iopub.status.idle": "2024-05-06T13:50:19.136424Z",
     "shell.execute_reply": "2024-05-06T13:50:19.135577Z"
    },
    "papermill": {
     "duration": 0.034456,
     "end_time": "2024-05-06T13:50:19.138261",
     "exception": false,
     "start_time": "2024-05-06T13:50:19.103805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "def make_lstm_model(days_in, days_out, vocabulary_size, embedding_dim,\n",
    "                    dropout_1=0.12120, neurons_1=50,\n",
    "                    dropout_2=0.79941, neurons_2=31,\n",
    "                    dropout_3=0.35324, neurons_3=38,\n",
    "                    neurons_4=179,\n",
    "                    neurons_5=297,\n",
    "                    learning_rate=0.00938, inv_beta_1=0.20869, inv_beta_2=0.1172,\n",
    "                    show=False):\n",
    "    \n",
    "    input_data = Input(shape=(days_in, FeatureEncoder.num_of_features+1))\n",
    "    input_features = Lambda(lambda x: x[:, :, :-1])(input_data)\n",
    "    token_index_input = Lambda(lambda x: x[:, -1:, -1])(input_data)\n",
    "    \n",
    "    embedded_token = Embedding(input_dim=vocabulary_size, output_dim=165)(token_index_input)\n",
    "    embedded_token = Flatten()(embedded_token)\n",
    "    embedded_token = RepeatVector(days_in)(embedded_token)\n",
    "    \n",
    "    concatenated_inputs = concatenate([input_features[:,:,1:], embedded_token], axis=-1)\n",
    "    \n",
    "    lstm_layer_1 = LSTM(neurons_1, return_sequences=True, dropout=dropout_1, recurrent_dropout=dropout_1)(concatenated_inputs)\n",
    "    lstm_layer_1_output = BatchNormalization()(lstm_layer_1)\n",
    "    lstm_layer_2 = LSTM(neurons_2, return_sequences=True, dropout=dropout_2, recurrent_dropout=dropout_2)(lstm_layer_1_output)\n",
    "    lstm_layer_2_output = BatchNormalization()(lstm_layer_2)\n",
    "    lstm_layer_3 = LSTM(neurons_3, dropout=dropout_3, recurrent_dropout=dropout_3)(lstm_layer_2_output)\n",
    "\n",
    "    lstm_output = BatchNormalization()(lstm_layer_3)\n",
    "\n",
    "    dense_layer_1 = Dense(neurons_4, activation='relu')(lstm_output)\n",
    "    dense_layer_2 = Dense(neurons_5, activation='relu')(dense_layer_1)\n",
    "    output_layer = Dense(3 * days_out)(dense_layer_2)\n",
    "\n",
    "    model = Model(inputs=input_data, outputs=output_layer)\n",
    "\n",
    "    model.compile(loss=MeanAbsoluteError(), optimizer=Adam(learning_rate=learning_rate, beta_1=1-inv_beta_1, beta_2=1-inv_beta_2))\n",
    "\n",
    "    if show:\n",
    "        model.summary()\n",
    "\n",
    "    return ModelWrapper(model, is_keras_model=True, uses_token_input=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "759f88f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:50:19.179860Z",
     "iopub.status.busy": "2024-05-06T13:50:19.178946Z",
     "iopub.status.idle": "2024-05-06T13:50:19.190236Z",
     "shell.execute_reply": "2024-05-06T13:50:19.189426Z"
    },
    "papermill": {
     "duration": 0.034091,
     "end_time": "2024-05-06T13:50:19.192089",
     "exception": false,
     "start_time": "2024-05-06T13:50:19.157998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_transformer_model(days_in, days_out, vocabulary_size, embedding_dim,\n",
    "                           dropout_1=0.2, neurons_1=79,\n",
    "                           dropout_2=0.5, neurons_2=46,\n",
    "                           dropout_3=0.5, neurons_3=23,\n",
    "                           neurons_4=28,\n",
    "                           neurons_5=354,\n",
    "                           learning_rate=0.02023, inv_beta_1=0.099093, inv_beta_2=0.01387, num_heads=8,\n",
    "                           show=False):\n",
    "\n",
    "    input_data = Input(shape=(days_in, FeatureEncoder.num_of_features+1))\n",
    "    input_features = Lambda(lambda x: x[:, :, :-1])(input_data)\n",
    "    token_index_input = Lambda(lambda x: x[:, -1:, -1])(input_data)\n",
    "\n",
    "    embedded_token = Embedding(input_dim=vocabulary_size, output_dim=111)(token_index_input)\n",
    "    embedded_token = Flatten()(embedded_token)\n",
    "    embedded_token = RepeatVector(days_in)(embedded_token)\n",
    "\n",
    "    concatenated_inputs = concatenate([input_features[:,:,1:], embedded_token], axis=-1)\n",
    "\n",
    "    transformer_block = MultiHeadAttention(num_heads=num_heads, key_dim=neurons_1)(concatenated_inputs, concatenated_inputs)\n",
    "    transformer_block = Dropout(dropout_1)(transformer_block)\n",
    "    transformer_block = LayerNormalization(epsilon=1e-6)(concatenated_inputs + transformer_block)\n",
    "\n",
    "    transformer_output = GlobalAveragePooling1D()(transformer_block)\n",
    "    transformer_output = BatchNormalization()(transformer_output)\n",
    "\n",
    "    dense_layer_1 = Dense(neurons_4, activation='relu')(transformer_output)\n",
    "    dense_layer_2 = Dense(neurons_5, activation='relu')(dense_layer_1)\n",
    "    output_layer = Dense(3 * days_out)(dense_layer_2)\n",
    "\n",
    "    model = Model(inputs=input_data, outputs=output_layer)\n",
    "\n",
    "    model.compile(loss=MeanAbsoluteError(), optimizer=Adam(learning_rate=learning_rate, beta_1=1-inv_beta_1, beta_2=1-inv_beta_2))\n",
    "\n",
    "    if show:\n",
    "        model.summary()\n",
    "\n",
    "    return ModelWrapper(model, is_keras_model=True, uses_token_input=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a70bd474",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:50:19.232798Z",
     "iopub.status.busy": "2024-05-06T13:50:19.232510Z",
     "iopub.status.idle": "2024-05-06T13:50:19.241277Z",
     "shell.execute_reply": "2024-05-06T13:50:19.240446Z"
    },
    "papermill": {
     "duration": 0.03137,
     "end_time": "2024-05-06T13:50:19.243177",
     "exception": false,
     "start_time": "2024-05-06T13:50:19.211807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, Flatten, Input\n",
    "\n",
    "def make_neural_network_model(days_in, days_out, vocabulary_size, embedding_dim,\n",
    "                              dropout_1=0.11541, neurons_1=25,\n",
    "                              neurons_2=56,\n",
    "                              neurons_3=77,\n",
    "                              neurons_4=40,\n",
    "                              neurons_5=228,\n",
    "                              learning_rate=0.09167, inv_beta_1=0.41461, inv_beta_2=0.11718,\n",
    "                              show=False):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(days_in, FeatureEncoder.num_of_features)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(neurons_1, activation='relu'))\n",
    "    model.add(Dropout(dropout_1))\n",
    "    model.add(Dense(neurons_2, activation='relu'))\n",
    "    model.add(Dense(neurons_3, activation='relu'))\n",
    "    model.add(Dense(neurons_4, activation='relu'))\n",
    "    model.add(Dense(neurons_5, activation='relu'))\n",
    "    model.add(Dense(3 * days_out))\n",
    "    model.compile(loss=MeanAbsoluteError(), optimizer=Adam(learning_rate=learning_rate, beta_1=1-inv_beta_1, beta_2=1-inv_beta_2))\n",
    "    \n",
    "    if show:\n",
    "        model.summary()\n",
    "        \n",
    "    return ModelWrapper(model, is_keras_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6886005f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:50:19.283467Z",
     "iopub.status.busy": "2024-05-06T13:50:19.283145Z",
     "iopub.status.idle": "2024-05-06T13:50:19.351776Z",
     "shell.execute_reply": "2024-05-06T13:50:19.350859Z"
    },
    "papermill": {
     "duration": 0.091547,
     "end_time": "2024-05-06T13:50:19.354044",
     "exception": false,
     "start_time": "2024-05-06T13:50:19.262497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "\n",
    "def make_decision_tree_model(days_in, days_out, vocabulary_size = 0, embedding_dim = 0, max_depth=56, min_samples_split=1340):\n",
    "    model = DecisionTreeRegressor(max_depth=max_depth, min_samples_split=min_samples_split)\n",
    "    return ModelWrapper(model, is_keras_model=False)\n",
    "\n",
    "\n",
    "def make_gradient_boosting_model(days_in, days_out, vocabulary_size = 0, embedding_dim = 0, n_estimators=10, max_depth=8, min_samples_split=633, subsample=0.54919):\n",
    "    model = MultiOutputRegressor(GradientBoostingRegressor(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, subsample=subsample))\n",
    "    return ModelWrapper(model, is_keras_model=False)\n",
    "\n",
    "\n",
    "def make_extra_trees_model(days_in, days_out, vocabulary_size = 0, embedding_dim = 0, n_estimators=29, max_depth=14, min_samples_split=1799):\n",
    "    model = ExtraTreesRegressor(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split)\n",
    "    return ModelWrapper(model, is_keras_model=False)\n",
    "\n",
    "\n",
    "def make_adaboost_model(base, n_estimators=100, learning_rate=1):\n",
    "    \"\"\"\n",
    "    Wraps existing model into AdaBoost model\n",
    "    \"\"\"\n",
    "    model = AdaBoostRegressor(estimator=base, n_estimators=n_estimators, learning_rate=learning_rate)\n",
    "    return ModelWrapper(model, is_keras_model=False)\n",
    "\n",
    "\n",
    "def make_random_forest_model(days_in, days_out, vocabulary_size = 0, embedding_dim = 0, n_estimators=92, max_depth=5, min_samples_split=1455):\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split)\n",
    "    return ModelWrapper(model, is_keras_model=False)\n",
    "\n",
    "\n",
    "def make_knn_model(days_in, days_out, vocabulary_size = 0, embedding_dim = 0, n_neighbors=55, weights=\"uniform\", p=1):\n",
    "    model = KNeighborsRegressor(n_neighbors=n_neighbors, weights=weights, p=p)\n",
    "    return ModelWrapper(model, is_keras_model=False)\n",
    "\n",
    "\n",
    "def make_linear_regression_model(days_in, days_out, vocabulary_size, embedding_dim):\n",
    "    model = LinearRegression()\n",
    "    return ModelWrapper(model, is_keras_model=False)\n",
    "\n",
    "\n",
    "def make_elasticnet_model(days_in, days_out, vocabulary_size = 0, embedding_dim = 0, alpha=.00099, l1_ratio=0.71480):\n",
    "    model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "    return ModelWrapper(model, is_keras_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b05173d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:50:19.396679Z",
     "iopub.status.busy": "2024-05-06T13:50:19.396133Z",
     "iopub.status.idle": "2024-05-06T13:50:19.404593Z",
     "shell.execute_reply": "2024-05-06T13:50:19.403672Z"
    },
    "papermill": {
     "duration": 0.032413,
     "end_time": "2024-05-06T13:50:19.406504",
     "exception": false,
     "start_time": "2024-05-06T13:50:19.374091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NaiveModel:\n",
    "    def __init__(self, days_out):\n",
    "        self._days_out = days_out\n",
    "        self.is_keras_model = False\n",
    "        self.uses_token_input = False\n",
    "        \n",
    "    def fit(self, train_x, train_y, test_x, test_y, epochs=30, batch_size=2048, sample_weights=None, verbose=False):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, x, batch_size=2048, verbose=True):\n",
    "        y  = []\n",
    "        for sample in x:\n",
    "            last_day = sample[-1]\n",
    "            target_features = last_day[1:4]\n",
    "            y.append(np.array([target_features]*self._days_out))\n",
    "        \n",
    "        return np.array(y)\n",
    "            \n",
    "    def save(self, file_name):\n",
    "        if self.is_keras_model:\n",
    "            return self.model.save(f\"{file_name}.h5\", save_format=\"h5\")\n",
    "        \n",
    "        return joblib.dump(self.model, f\"{file_name}.gz\")\n",
    "\n",
    "            \n",
    "def make_naive_model(days_in, days_out, vocabulary_size = 0, embedding_dim = 0):\n",
    "    return NaiveModel(days_out)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "332d697d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:50:19.446432Z",
     "iopub.status.busy": "2024-05-06T13:50:19.446151Z",
     "iopub.status.idle": "2024-05-06T13:50:57.175879Z",
     "shell.execute_reply": "2024-05-06T13:50:57.174945Z"
    },
    "papermill": {
     "duration": 37.772138,
     "end_time": "2024-05-06T13:50:57.197915",
     "exception": false,
     "start_time": "2024-05-06T13:50:19.425777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation: 2.26%\n"
     ]
    }
   ],
   "source": [
    "batch = 2048\n",
    "\n",
    "model = make_extra_trees_model(days_in, days_out, n_estimators=10, max_depth=3)\n",
    "weights = calculate_weights(train_x, train_y, 0.02)\n",
    "train_x = np.concatenate((train_x, np.repeat(np.expand_dims(np.expand_dims(train_token, axis=-1), axis=1), 10, axis=1)), axis=-1)\n",
    "test_x = np.concatenate((test_x, np.repeat(np.expand_dims(np.expand_dims(test_token, axis=-1), axis=1), 10, axis=1)), axis=-1)\n",
    "history = model.fit(train_x, train_y, test_x, test_y, epochs=50, batch_size=batch, sample_weights=weights, verbose=True)\n",
    "\n",
    "validation_x = np.concatenate((validation_x, np.repeat(np.expand_dims(np.expand_dims(validation_token, axis=-1), axis=1), 10, axis=1)), axis=-1)\n",
    "evaluation = evaluate_trained_model(model, encoder, validation_x, validation_y, batch_size=batch, verbose=True, analyze_model=False)\n",
    "print(f\"\\nEvaluation: {(evaluation*100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f750d568",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:50:57.238535Z",
     "iopub.status.busy": "2024-05-06T13:50:57.237861Z",
     "iopub.status.idle": "2024-05-06T13:50:57.244730Z",
     "shell.execute_reply": "2024-05-06T13:50:57.243929Z"
    },
    "papermill": {
     "duration": 0.029257,
     "end_time": "2024-05-06T13:50:57.246565",
     "exception": false,
     "start_time": "2024-05-06T13:50:57.217308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Save model metadata\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def get_scaler_props(scaler):\n",
    "    min_in, max_in = scaler.get_range()\n",
    "    return {\n",
    "        \"min\": float(min_in),\n",
    "        \"max\": float(max_in)\n",
    "    }\n",
    "\n",
    "\n",
    "def serialize_configuration(model, encoder, path):\n",
    "    with open(path, \"w\") as file:\n",
    "        json.dump({\n",
    "            \"is_keras_model\": model.is_keras_model,\n",
    "            \"open\": get_scaler_props(encoder._open_scaler),\n",
    "            \"close\": get_scaler_props(encoder._close_scaler),\n",
    "            \"high\": get_scaler_props(encoder._high_scaler),\n",
    "            \"low\": get_scaler_props(encoder._low_scaler),\n",
    "            \"volume\": get_scaler_props(encoder._volume_scaler),\n",
    "            \"fear_greed_index\": get_scaler_props(encoder._fear_greed_index_scaler)\n",
    "        }, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f6b3209",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:50:57.286549Z",
     "iopub.status.busy": "2024-05-06T13:50:57.286272Z",
     "iopub.status.idle": "2024-05-06T13:50:57.305576Z",
     "shell.execute_reply": "2024-05-06T13:50:57.304754Z"
    },
    "papermill": {
     "duration": 0.041734,
     "end_time": "2024-05-06T13:50:57.307566",
     "exception": false,
     "start_time": "2024-05-06T13:50:57.265832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Training & evaluation methods to use in automatic hyperparameter search\n",
    "\n",
    "class EvaluationData:\n",
    "    \"\"\"\n",
    "    Used to cache test data between evaluations since loading datasets takes time\n",
    "    \"\"\"\n",
    "    def __init__(self, train_datasets, validation_datasets, train_x, train_y, train_token, test_x, test_y, test_token, validation_x, validation_y, validation_token, encoder, days_in, days_out, vocab_size):\n",
    "        self.train_datasets = train_datasets\n",
    "        self.validation_datasets = validation_datasets\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.train_token = train_token\n",
    "        self.test_x = test_x\n",
    "        self.test_y = test_y\n",
    "        self.test_token = test_token\n",
    "        self.validation_x = validation_x\n",
    "        self.validation_y = validation_y\n",
    "        self.validation_token = validation_token\n",
    "        self.encoder = encoder\n",
    "        self.days_in = days_in\n",
    "        self.days_out = days_out\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"EvaluationData(train_datasets={self.train_datasets}, validation_datasets={self.validation_datasets}, train_x={self.train_x}, train_y={self.train_y}, train_token={self.train_token}, test_x={self.test_x}, test_y={self.test_y}, test_token={self.test_token}, validation_x={self.validation_x}, validation_y={self.validation_y}, validation_token={self.validation_token}, encoder={self.encoder}, days_in={self.days_in}, days_out={self.days_out}, vocab_size={self.vocab_size})\"\n",
    "\n",
    "def plot_learning_curve(history, model_name=''):\n",
    "    loss = history.history['loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, loss, label='Training Loss', color='blue')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{model_name}_learning_curve.png')\n",
    "    plt.show()\n",
    "        \n",
    "def prepare_evaluation_data(days_in, days_out, dataset_loader, number_of_datasets=15):\n",
    "    train_datasets, validation_datasets, full_datasets, fear_greed_data = dataset_loader.get_datasets() # load_datasets(number_of_datasets)\n",
    "    extractor = FeatureExtractor(full_datasets, fear_greed_data)\n",
    "    train_x, train_y, test_x, test_y, train_token, test_token, encoder = datasets_to_training_data(train_datasets, days_in, days_out, extractor)\n",
    "    validation_x, validation_y, validation_token = datasets_to_validation_data(validation_datasets, days_in, days_out, encoder)\n",
    "    \n",
    "    vocab_size = len(extractor.token_to_index)\n",
    "    return EvaluationData(train_datasets, validation_datasets, train_x, train_y, train_token, test_x, test_y, test_token, validation_x, validation_y, validation_token, encoder, days_in, days_out, vocab_size)\n",
    "\n",
    "\n",
    "def evaluate_performance(model_creator, evaluation_data, epochs=30, use_weights=True, verbose=False, model_name=''):\n",
    "    model = model_creator(evaluation_data.days_in, evaluation_data.days_out, evaluation_data.vocab_size, evaluation_data.vocab_size//2)\n",
    "\n",
    "    weights = calculate_weights(evaluation_data.train_x, evaluation_data.train_y, 0.02) if use_weights else None\n",
    "    batch_size = ModelWrapper.default_batch_size\n",
    "\n",
    "    train_x_with_token_input = np.concatenate((evaluation_data.train_x, np.repeat(np.expand_dims(np.expand_dims(evaluation_data.train_token, axis=-1), axis=1), 10, axis=1)), axis=-1)\n",
    "    test_x_with_token_input = np.concatenate((evaluation_data.test_x, np.repeat(np.expand_dims(np.expand_dims(evaluation_data.test_token, axis=-1), axis=1), 10, axis=1)), axis=-1)\n",
    "    history = model.fit(train_x_with_token_input, evaluation_data.train_y, test_x_with_token_input, evaluation_data.test_y, epochs=epochs, batch_size=batch_size, \\\n",
    "                        sample_weights=weights, verbose=verbose)\n",
    "\n",
    "    if history is not None:\n",
    "#         print(history)\n",
    "        plot_learning_curve(history, model_name=model_name)\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"\\nEvaluation:\")\n",
    "    validation_x_with_token_input = np.concatenate((evaluation_data.validation_x, np.repeat(np.expand_dims(np.expand_dims(evaluation_data.validation_token, axis=-1), axis=1), 10, axis=1)), axis=-1)\n",
    "    evaluation = evaluate_trained_model(model, evaluation_data.encoder, validation_x_with_token_input, evaluation_data.validation_y, batch_size=batch_size, verbose=verbose, model_name=model_name)\n",
    "    \n",
    "    if model_name != \"Naive\":\n",
    "        save_name = \"models/\" + model_name\n",
    "        try:\n",
    "            serialize_configuration(model, encoder, f\"{save_name}_config.json\")\n",
    "            model.save(save_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error at model save: {e}\")\n",
    "    return evaluation, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cfdcc70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:50:57.348843Z",
     "iopub.status.busy": "2024-05-06T13:50:57.348586Z",
     "iopub.status.idle": "2024-05-06T13:51:48.708845Z",
     "shell.execute_reply": "2024-05-06T13:51:48.708051Z"
    },
    "papermill": {
     "duration": 51.383342,
     "end_time": "2024-05-06T13:51:48.711273",
     "exception": false,
     "start_time": "2024-05-06T13:50:57.327931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = prepare_evaluation_data(days_in, days_out, dataset_loader, 100)\n",
    "models = {\n",
    "    \"Naive\": make_naive_model,\n",
    "    \"LSTM_without_embedding\": make_lstm_model_without_embedding,\n",
    "    \"LSTM\": make_lstm_model,\n",
    "    \"Transformer\": make_transformer_model,\n",
    "    \"KNN\": make_knn_model,\n",
    "    \"ElasticNet\": make_elasticnet_model,\n",
    "    \"Neural network\": make_neural_network_model,\n",
    "    \"Decision tree\": make_decision_tree_model,\n",
    "    \"Gradient boosting\": lambda i, o, v, e: make_gradient_boosting_model(i, o, v, e),\n",
    "    \"Extra trees\": lambda i, o, v, e: make_extra_trees_model(i, o, v, e, n_estimators=10, max_depth=4),\n",
    "    \"Random forest\": lambda i, o, v, e: make_random_forest_model(i, o, v, e, n_estimators=10, max_depth=4),\n",
    "    \"Linear regression\": make_linear_regression_model\n",
    "}\n",
    "\n",
    "if False:\n",
    "    for model_name, creator in models.items():\n",
    "        print(f\"{model_name}:\")\n",
    "        result, _ = evaluate_performance(creator, data, 50, model_name = model_name)\n",
    "        print(f\"{model_name}: {result*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c78d52c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:51:48.752932Z",
     "iopub.status.busy": "2024-05-06T13:51:48.752612Z",
     "iopub.status.idle": "2024-05-06T13:51:48.771175Z",
     "shell.execute_reply": "2024-05-06T13:51:48.770284Z"
    },
    "papermill": {
     "duration": 0.041577,
     "end_time": "2024-05-06T13:51:48.773298",
     "exception": false,
     "start_time": "2024-05-06T13:51:48.731721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_evaluation_data_for_tokens(days_in, days_out, dataset_loader, number_of_datasets=15):\n",
    "    train_datasets, validation_datasets, full_datasets, fear_greed_data = dataset_loader.get_datasets() # load_datasets(number_of_datasets, check_split=True)\n",
    "    extractor = FeatureExtractor(full_datasets, fear_greed_data)\n",
    "    evaluation_data_dict = {}\n",
    "    \n",
    "    for token in validation_datasets:\n",
    "        try:\n",
    "            train_x, train_y, test_x, test_y, train_token, test_token, encoder = datasets_to_training_data({token: train_datasets[token]}, days_in, days_out, extractor)\n",
    "            if test_x.shape == (0,):\n",
    "                test_x = test_x.reshape(0, 10, 10)\n",
    "                test_y = test_y.reshape(0, 15)\n",
    "            validation_x, validation_y, validation_token = datasets_to_validation_data({token: validation_datasets[token]}, days_in, days_out, encoder)\n",
    "            vocab_size = len(extractor.token_to_index)\n",
    "            evaluation_data = EvaluationData({token: dataset}, {token: validation_datasets[token]}, train_x, train_y, train_token, test_x, test_y, test_token, validation_x, validation_y, validation_token, encoder, days_in, days_out, vocab_size)\n",
    "            evaluation_data_dict[token] = evaluation_data\n",
    "        except:\n",
    "            pass\n",
    "    return evaluation_data_dict\n",
    "\n",
    "\n",
    "def evaluate_performance_for_model_per_token(models, evaluation_data_dict, epochs=30, use_weights=True, verbose=False, analyze_all_models=False):\n",
    "    evaluation_dict = {}\n",
    "    all_non_relative_errors = []\n",
    "    daily_averages_list = []\n",
    "    good_prediction_percentage_list = []\n",
    "    best_models_dict = {}\n",
    "    for token, evaluation_data in evaluation_data_dict.items():\n",
    "        evaluation_for_token = 1000\n",
    "        best_model_for_token = None\n",
    "        best_model_for_token_name = ''\n",
    "        \n",
    "        weights = calculate_weights(evaluation_data.train_x, evaluation_data.train_y, 0.02) if use_weights else None\n",
    "        batch_size = ModelWrapper.default_batch_size\n",
    "        train_x_with_token_input = np.concatenate((evaluation_data.train_x, np.repeat(np.expand_dims(np.expand_dims(evaluation_data.train_token, axis=-1), axis=1), 10, axis=1)), axis=-1)\n",
    "        test_x_with_token_input = np.concatenate((evaluation_data.test_x, np.repeat(np.expand_dims(np.expand_dims(evaluation_data.test_token, axis=-1), axis=1), 10, axis=1)), axis=-1)\n",
    "        validation_x_with_token_input = np.concatenate((evaluation_data.validation_x, np.repeat(np.expand_dims(np.expand_dims(evaluation_data.validation_token, axis=-1), axis=1), 10, axis=1)), axis=-1)\n",
    "            \n",
    "        for model_name, model_creator in models.items():\n",
    "            model = model_creator(evaluation_data.days_in, evaluation_data.days_out, evaluation_data.vocab_size, evaluation_data.vocab_size//2)\n",
    "            history = model.fit(train_x_with_token_input, evaluation_data.train_y, test_x_with_token_input, evaluation_data.test_y, epochs=epochs, batch_size=batch_size, \\\n",
    "                                sample_weights=weights, verbose=verbose)\n",
    "            if verbose:\n",
    "                print(\"\\nEvaluation:\")\n",
    "            evaluation = evaluate_trained_model(model, evaluation_data.encoder, validation_x_with_token_input, evaluation_data.validation_y, \\\n",
    "                                            batch_size=batch_size, verbose=verbose, analyze_model=False, print_analysis=False)\n",
    "            if evaluation < evaluation_for_token:\n",
    "                evaluation_for_token = evaluation\n",
    "                best_model_for_token = model\n",
    "                best_model_for_token_name = model_name\n",
    "        \n",
    "        evaluation, non_relative_errors, daily_averages, good_prediction_percentage = evaluate_trained_model(best_model_for_token, \\\n",
    "                                            evaluation_data.encoder, validation_x_with_token_input, evaluation_data.validation_y, \\\n",
    "                                            batch_size=batch_size, verbose=verbose, analyze_model=True, print_analysis=False)\n",
    "        \n",
    "        all_non_relative_errors.extend(non_relative_errors)\n",
    "        daily_averages_list.append(daily_averages)\n",
    "        good_prediction_percentage_list.append(good_prediction_percentage)\n",
    "        if analyze_all_models:\n",
    "            print(f'Evaluation for {token}: {evaluation_for_token*100:.2f}%')\n",
    "        evaluation_dict[token] = evaluation_for_token\n",
    "        best_models_dict[token] = best_model_for_token_name\n",
    "    analyze_trained_models_for_tokens(np.array(all_non_relative_errors), np.mean(np.array(daily_averages_list), axis=0), \\\n",
    "                                        np.mean(good_prediction_percentage_list), title=\"models_per_token\")\n",
    "    return evaluation_dict, best_models_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ea23ea5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:51:48.813425Z",
     "iopub.status.busy": "2024-05-06T13:51:48.813124Z",
     "iopub.status.idle": "2024-05-06T13:51:48.824168Z",
     "shell.execute_reply": "2024-05-06T13:51:48.823291Z"
    },
    "papermill": {
     "duration": 0.03327,
     "end_time": "2024-05-06T13:51:48.825980",
     "exception": false,
     "start_time": "2024-05-06T13:51:48.792710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_models_per_tokens(dataset_loader, print_all_tokens_evaluation=True):\n",
    "    data_for_models_per_token = prepare_evaluation_data_for_tokens(days_in, days_out, dataset_loader, 12)\n",
    "    models = {\n",
    "        \"Naive\": make_naive_model,\n",
    "        \"LSTM_without_embedding\": make_lstm_model_without_embedding,\n",
    "        \"KNN\": make_knn_model,\n",
    "        \"ElasticNet\": make_elasticnet_model,\n",
    "        \"Neural network\": make_neural_network_model,\n",
    "        \"Decision tree\": make_decision_tree_model,\n",
    "        \"Gradient boosting\": lambda i, o, v, e: make_gradient_boosting_model(i, o, v, e, n_estimators=10, max_depth=4),\n",
    "        \"Extra trees\": lambda i, o, v, e: make_extra_trees_model(i, o, v, e, n_estimators=10, max_depth=4),\n",
    "        \"Random forest\": lambda i, o, v, e: make_random_forest_model(i, o, v, e, n_estimators=10, max_depth=4),\n",
    "        \"Linear regression\": make_linear_regression_model,\n",
    "    }\n",
    "    \n",
    "    print(\"Models Per Token:\")\n",
    "    for i in range(1):\n",
    "        results, best_models = evaluate_performance_for_model_per_token(models, data_for_models_per_token, 50, analyze_all_models=print_all_tokens_evaluation)\n",
    "        avg_result = 0\n",
    "        all_results = []\n",
    "        \n",
    "        for token, model_name in best_models.items():\n",
    "            print(f\"Best model for {token}: {model_name}\")\n",
    "            print(f\"Evaluation for {token}: {results[token]*100:.2f}%\")\n",
    "        \n",
    "        for token, result in results.items():\n",
    "            all_results.append(result)\n",
    "            avg_result += result\n",
    "        \n",
    "        print(\"Results for set of models per tokens:\")\n",
    "        avg_result /= len(results.items())\n",
    "        print(f\"Average: {avg_result*100:.2f}%\")\n",
    "    \n",
    "        median_result = np.median(all_results)\n",
    "        print(f\"Median: {median_result*100:.2f}%\")\n",
    "        \n",
    "        # average for the results without top and bottom 10%, if a sufficient number of sets\n",
    "        num_to_exclude = int(len(all_results) * 0.1)\n",
    "        if num_to_exclude > 0:\n",
    "            trimmed_results = sorted(all_results)[num_to_exclude:-num_to_exclude]\n",
    "            trimmed_avg_result = np.mean(trimmed_results)\n",
    "            print(f\"Trimmed average: {trimmed_avg_result*100:.2f}%\")   \n",
    "   \n",
    "if False:\n",
    "    train_and_evaluate_models_per_tokens(dataset_loader, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cfc518db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:51:48.866731Z",
     "iopub.status.busy": "2024-05-06T13:51:48.866454Z",
     "iopub.status.idle": "2024-05-06T13:51:48.894831Z",
     "shell.execute_reply": "2024-05-06T13:51:48.893996Z"
    },
    "papermill": {
     "duration": 0.050864,
     "end_time": "2024-05-06T13:51:48.896636",
     "exception": false,
     "start_time": "2024-05-06T13:51:48.845772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data_for_tokens_sets(dataset_loader, number_of_datasets=15):\n",
    "    train_datasets, validation_datasets, full_datasets, fear_greed_data = dataset_loader.get_datasets() # load_datasets(number_of_datasets, check_split=True)\n",
    "    return train_datasets, validation_datasets, full_datasets, fear_greed_data\n",
    "\n",
    "def prepare_evaluation_data_for_tokens_sets(days_in, days_out, number_of_sets, train_datasets, validation_datasets, full_datasets, fear_greed_data, token_sets = None):\n",
    "    extractor = FeatureExtractor(full_datasets, fear_greed_data)\n",
    "    evaluation_data_dict = {}\n",
    "    token_evaluation_data_dict = {}\n",
    "    \n",
    "    tokens_list = list(set(validation_datasets.keys()).intersection(set(train_datasets.keys())))\n",
    "    if token_sets is None:\n",
    "        tokens_per_set = len(tokens_list) // number_of_sets\n",
    "        token_sets = [tokens_list[i:i+tokens_per_set] for i in range(0, len(tokens_list), tokens_per_set)]\n",
    "\n",
    "    for token_set in token_sets:\n",
    "        train_dataset_for_tokens_set = {}\n",
    "        validation_dataset_for_tokens_set = {}\n",
    "        \n",
    "        for token in token_set:\n",
    "            train_dataset_for_tokens_set[token] = train_datasets[token]\n",
    "            validation_dataset_for_tokens_set[token] = validation_datasets[token]\n",
    "            train_x, train_y, test_x, test_y, train_token, test_token, encoder = datasets_to_training_data({token: train_datasets[token]}, days_in, days_out, extractor)\n",
    "            validation_x, validation_y, validation_token = datasets_to_validation_data({token: validation_datasets[token]}, days_in, days_out, encoder)\n",
    "            vocab_size = len(extractor.token_to_index)\n",
    "            evaluation_data = EvaluationData(train_dataset_for_tokens_set, validation_dataset_for_tokens_set, train_x, train_y, train_token, test_x, test_y, test_token, validation_x, validation_y, validation_token, encoder, days_in, days_out, vocab_size)\n",
    "            token_evaluation_data_dict[token] = evaluation_data\n",
    "\n",
    "        train_x, train_y, test_x, test_y, train_token, test_token, encoder = datasets_to_training_data(train_dataset_for_tokens_set, days_in, days_out, extractor)\n",
    "        validation_x, validation_y, validation_token = datasets_to_validation_data(validation_dataset_for_tokens_set, days_in, days_out, encoder)\n",
    "        vocab_size = len(extractor.token_to_index)\n",
    "        \n",
    "        evaluation_data = EvaluationData(train_dataset_for_tokens_set, validation_dataset_for_tokens_set, train_x, train_y, train_token, test_x, test_y, test_token, validation_x, validation_y, validation_token, encoder, days_in, days_out, vocab_size)\n",
    "        \n",
    "        key = ','.join(token_set)\n",
    "        evaluation_data_dict[key] = evaluation_data\n",
    "    \n",
    "    return evaluation_data_dict, token_evaluation_data_dict\n",
    "\n",
    "def evaluate_performance_for_model_per_token_iteration(model_creator, evaluation_data_dict, token_evaluation_data_dict, epochs=30, use_weights=True, verbose=False, analyze_all_models=False, reasemble_sets=True, print_all_tokens_evaluation=False, model_name=''):\n",
    "    models_list = []\n",
    "    best_evaluations = {}\n",
    "    best_models = {}\n",
    "    all_non_relative_errors_dict = {}\n",
    "    daily_averages_dict = {}\n",
    "    good_prediction_percentage_dict = {}\n",
    "    \n",
    "    for idx, (token_set, evaluation_data) in enumerate(evaluation_data_dict.items()):\n",
    "        model = model_creator(evaluation_data.days_in, evaluation_data.days_out, evaluation_data.vocab_size, evaluation_data.vocab_size//2)\n",
    "        weights = calculate_weights(evaluation_data.train_x, evaluation_data.train_y, 0.02) if use_weights else None\n",
    "        batch_size = ModelWrapper.default_batch_size\n",
    "        train_x_with_token_input = np.concatenate((evaluation_data.train_x, np.repeat(np.expand_dims(np.expand_dims(evaluation_data.train_token, axis=-1), axis=1), 10, axis=1)), axis=-1)\n",
    "        test_x_with_token_input = np.concatenate((evaluation_data.test_x, np.repeat(np.expand_dims(np.expand_dims(evaluation_data.test_token, axis=-1), axis=1), 10, axis=1)), axis=-1)\n",
    "        history = model.fit(train_x_with_token_input, evaluation_data.train_y, test_x_with_token_input, evaluation_data.test_y, epochs=epochs, batch_size=batch_size, sample_weights=weights, verbose=verbose)\n",
    "        models_list.append((model, history))\n",
    "        \n",
    "        if reasemble_sets == False:\n",
    "            tokens = token_set.split(',')\n",
    "            for token in tokens:\n",
    "                validation_x_with_token_input = np.concatenate((token_evaluation_data_dict[token].validation_x, np.repeat(np.expand_dims(np.expand_dims(token_evaluation_data_dict[token].validation_token, axis=-1), axis=1), 10, axis=1)), axis=-1)\n",
    "                evaluation, non_relative_errors, daily_averages, good_prediction_percentage = evaluate_trained_model(model, \\\n",
    "                                token_evaluation_data_dict[token].encoder, validation_x_with_token_input, token_evaluation_data_dict[token].validation_y, \\\n",
    "                                batch_size=batch_size, verbose=verbose, analyze_model=True, print_analysis=False)\n",
    "                best_models[token] = idx\n",
    "                best_evaluations[token] = evaluation\n",
    "                all_non_relative_errors_dict[token] = non_relative_errors\n",
    "                daily_averages_dict[token] = daily_averages\n",
    "                good_prediction_percentage_dict[token] = good_prediction_percentage\n",
    "    \n",
    "    if reasemble_sets:\n",
    "        for token_set in evaluation_data_dict.keys():\n",
    "            tokens = token_set.split(',')\n",
    "            for token in tokens:\n",
    "                best_evaluations[token] = 100\n",
    "                for idx, (model, _) in enumerate(models_list):\n",
    "                    validation_x_with_token_input = np.concatenate((token_evaluation_data_dict[token].validation_x, np.repeat(np.expand_dims(np.expand_dims(token_evaluation_data_dict[token].validation_token, axis=-1), axis=1), 10, axis=1)), axis=-1)\n",
    "                    evaluation, non_relative_errors, daily_averages, good_prediction_percentage = evaluate_trained_model(model, \\\n",
    "                                    token_evaluation_data_dict[token].encoder, validation_x_with_token_input, \\\n",
    "                                    token_evaluation_data_dict[token].validation_y, batch_size=batch_size, verbose=verbose, \\\n",
    "                                    analyze_model=True, print_analysis=False)\n",
    "                    if evaluation < best_evaluations[token]:\n",
    "                        best_models[token] = idx\n",
    "                        best_evaluations[token] = evaluation\n",
    "                        all_non_relative_errors_dict[token] = non_relative_errors\n",
    "                        daily_averages_dict[token] = daily_averages\n",
    "                        good_prediction_percentage_dict[token] = good_prediction_percentage\n",
    "                    \n",
    "    new_token_sets = {}\n",
    "    for i in set(best_models.values()):\n",
    "        new_token_sets[i] = []\n",
    "    \n",
    "    for token_set in evaluation_data_dict.keys():\n",
    "        tokens = token_set.split(',')\n",
    "        for token in tokens:\n",
    "            if print_all_tokens_evaluation:\n",
    "                print(f'Best Evaluation for {token}: {best_evaluations[token]*100:.2f}% from model: {best_models[token]}')\n",
    "            new_token_sets[best_models[token]].append(token)\n",
    "    analyze_trained_models_for_tokens(np.array(list(all_non_relative_errors_dict.values())), np.mean(np.array(list(daily_averages_dict.values())), axis=0), \\\n",
    "                                      np.mean(list(good_prediction_percentage_dict.values())), title=model_name)\n",
    "    \n",
    "    return best_evaluations, list(map(lambda x: x[1], models_list)), new_token_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9fb9c9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:51:48.937140Z",
     "iopub.status.busy": "2024-05-06T13:51:48.936893Z",
     "iopub.status.idle": "2024-05-06T13:51:49.196939Z",
     "shell.execute_reply": "2024-05-06T13:51:49.196209Z"
    },
    "papermill": {
     "duration": 0.282722,
     "end_time": "2024-05-06T13:51:49.198991",
     "exception": false,
     "start_time": "2024-05-06T13:51:48.916269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def calculate_correlation_for_tokens(tokens, start=\"2018-01-01\", end=\"2023-12-01\"):\n",
    "    closing_df = pd.DataFrame()\n",
    "    \n",
    "    for token in tokens:\n",
    "        dataset = load_dataset(token)\n",
    "        dataset = dataset.loc[(dataset['Date'] >= start) & (dataset['Date'] <= end)]\n",
    "        dataset.reset_index(drop=True, inplace=True)\n",
    "        closing_df[token] = dataset['Adjusted Close']\n",
    "        \n",
    "#     return closing_df.pct_change().corr()\n",
    "    return closing_df.corr()\n",
    "\n",
    "def analyze_elbow_method_for_kmeans_clustering(K_range, sum_of_squared_distances):\n",
    "    plt.figure(figsize=(11.5, 6))\n",
    "    fontsize = 17\n",
    "    plt.plot(K_range, sum_of_squared_distances, 'bx-')\n",
    "    plt.xlabel('Number of clusters', fontsize=fontsize)\n",
    "    plt.ylabel('Inertia', fontsize=fontsize)\n",
    "    plt.xticks(fontsize=fontsize)\n",
    "    plt.yticks([500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500], fontsize=fontsize)\n",
    "#     plt.title('The Elbow Method using Inertia')\n",
    "    plt.savefig(f'clustering/elbow_method_for_kmeans_clustering.png')\n",
    "    plt.show()\n",
    "\n",
    "def group_tokens_by_correlation(correlation_matrix, num_groups, analyze_clustering=False):\n",
    "    tokens = correlation_matrix.columns.tolist()\n",
    "    correlation_array = correlation_matrix.values\n",
    "    \n",
    "    if num_groups == 0:\n",
    "        sum_of_squared_distances = []\n",
    "        K_range = range(1, min(len(tokens), 15) + 1)\n",
    "        for k in K_range:\n",
    "            kmeans = KMeans(n_clusters=k)\n",
    "            kmeans.fit(correlation_array)\n",
    "            sum_of_squared_distances.append(kmeans.inertia_)\n",
    "            \n",
    "            if analyze_clustering:\n",
    "                y_kmeans = kmeans.fit_predict(correlation_array)\n",
    "                plt.scatter(correlation_array[:, 0], correlation_array[:, 1], c=y_kmeans)\n",
    "                plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=100, c='red')\n",
    "                plt.title('K-means clustering (k={})'.format(k))\n",
    "                plt.savefig(f'clustering/clustering_for_{k}_clusters.png')\n",
    "                plt.show()\n",
    "\n",
    "        elbow_point_index = np.argmin(np.diff(np.diff(sum_of_squared_distances)))\n",
    "        num_groups = elbow_point_index + 1\n",
    "        \n",
    "        if analyze_clustering:\n",
    "            analyze_elbow_method_for_kmeans_clustering(K_range, sum_of_squared_distances)\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=num_groups)\n",
    "    else:\n",
    "        kmeans = KMeans(n_clusters=num_groups)\n",
    "    kmeans.fit(correlation_array)\n",
    "\n",
    "    groups = {}\n",
    "    for token, label in zip(tokens, kmeans.labels_):\n",
    "        if label not in groups:\n",
    "            groups[label] = []\n",
    "        groups[label].append(token)\n",
    "\n",
    "    return (list(groups.values()), num_groups)\n",
    "\n",
    "def split_tokens_into_sets(token_groups, number_of_sets):\n",
    "    token_sets = [set() for _ in range(number_of_sets)]\n",
    "\n",
    "    for group in token_groups:\n",
    "        min_set_idx = np.argmin([len(token_set) for token_set in token_sets])\n",
    "        token_sets[min_set_idx].update(group)\n",
    "\n",
    "    return token_sets\n",
    "\n",
    "def plot_correlation_matrix(correlation_matrix):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='summer', fmt=\".2f\")\n",
    "    plt.title('Correlation of stock return')\n",
    "    plt.xlabel('Token')\n",
    "    plt.ylabel('Token')\n",
    "    plt.savefig(f'models_for_tokens_sets_correlation_matrix.png')\n",
    "    plt.show()\n",
    "\n",
    "def prepare_token_sets_based_on_correlation(tokens, number_of_sets, analyze_clustering=False):\n",
    "    if analyze_clustering:\n",
    "        os.makedirs(\"/kaggle/working/clustering\", exist_ok=True)\n",
    "    correlation_matrix = calculate_correlation_for_tokens(tokens)\n",
    "    token_groups, number_of_sets = group_tokens_by_correlation(correlation_matrix, number_of_sets, analyze_clustering=analyze_clustering)\n",
    "    plot_correlation_matrix(correlation_matrix)\n",
    "    token_sets = split_tokens_into_sets(token_groups, number_of_sets)\n",
    "    print(f'Number of token sets defined by correlation: {number_of_sets}')\n",
    "    print(f'Token sets defined by correlation: {token_sets}')\n",
    "    return token_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d19869a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:51:49.240657Z",
     "iopub.status.busy": "2024-05-06T13:51:49.240101Z",
     "iopub.status.idle": "2024-05-06T13:51:49.251784Z",
     "shell.execute_reply": "2024-05-06T13:51:49.250978Z"
    },
    "papermill": {
     "duration": 0.034589,
     "end_time": "2024-05-06T13:51:49.253685",
     "exception": false,
     "start_time": "2024-05-06T13:51:49.219096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_models_per_token_sets(dataset_loader, number_of_datasets=12, number_of_innitial_sets=3, iterations=1, predefine_sets=False, reasemble_sets=True):\n",
    "    train_datasets, validation_datasets, full_datasets, fear_greed_data = prepare_data_for_tokens_sets(dataset_loader, number_of_datasets)\n",
    "    if predefine_sets:\n",
    "        innitial_token_sets = prepare_token_sets_based_on_correlation(train_datasets.keys(), number_of_innitial_sets)\n",
    "    else:\n",
    "        innitial_token_sets = None\n",
    "        \n",
    "    models = {\n",
    "        \"Naive\": make_naive_model,\n",
    "        \"LSTM_without_embedding\": make_lstm_model_without_embedding,\n",
    "        \"KNN\": make_knn_model,\n",
    "        \"ElasticNet\": make_elasticnet_model,\n",
    "        \"Neural network\": make_neural_network_model,\n",
    "        \"Decision tree\": make_decision_tree_model,\n",
    "        \"Gradient boosting\": lambda i, o, v, e: make_gradient_boosting_model(i, o, v, e, n_estimators=10, max_depth=4),\n",
    "        \"Extra trees\": lambda i, o, v, e: make_extra_trees_model(i, o, v, e, n_estimators=10, max_depth=4),\n",
    "        \"Random forest\": lambda i, o, v, e: make_random_forest_model(i, o, v, e, n_estimators=10, max_depth=4),\n",
    "        \"Linear regression\": make_linear_regression_model,\n",
    "    }\n",
    "    for model_name, creator in models.items():\n",
    "        token_sets = innitial_token_sets\n",
    "        print(f\"{model_name}:\")\n",
    "        for i in range(iterations):\n",
    "            evaluation_data_dict, token_evaluation_data_dict = prepare_evaluation_data_for_tokens_sets(days_in, days_out, number_of_innitial_sets, train_datasets, validation_datasets, full_datasets, fear_greed_data, token_sets)\n",
    "            print(f\"Iteration {i + 1}:\")\n",
    "            best_token_assignment, _, new_token_sets = evaluate_performance_for_model_per_token_iteration(creator, evaluation_data_dict, token_evaluation_data_dict, 50, analyze_all_models=False, reasemble_sets=reasemble_sets, model_name=\"models_per_token_sets_\" + model_name + f\"_{i}\")\n",
    "            token_sets = list(new_token_sets.values())\n",
    "            print(f'Token sets:')\n",
    "            print(new_token_sets)\n",
    "            avg_for_all_result = 0\n",
    "            for key, tokens in new_token_sets.items():\n",
    "                avg_result = 0\n",
    "                for token in tokens:\n",
    "                    avg_result += best_token_assignment[token]\n",
    "                avg_result /= len(tokens)\n",
    "                avg_for_all_result += avg_result\n",
    "                print(f'Average for model {key}: {avg_result*100:.2f}%')\n",
    "            avg_for_all_result /= len(new_token_sets.items())\n",
    "            print(f'Average for all {model_name} models: {avg_for_all_result*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4bed3b19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:51:49.293528Z",
     "iopub.status.busy": "2024-05-06T13:51:49.293245Z",
     "iopub.status.idle": "2024-05-06T13:51:49.297359Z",
     "shell.execute_reply": "2024-05-06T13:51:49.296603Z"
    },
    "papermill": {
     "duration": 0.02621,
     "end_time": "2024-05-06T13:51:49.299256",
     "exception": false,
     "start_time": "2024-05-06T13:51:49.273046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    print('With 1 iteration:')\n",
    "    train_and_evaluate_models_per_token_sets(dataset_loader, 12, 3, 1)\n",
    "#     print('With 2 iterations:')\n",
    "#     train_and_evaluate_models_per_token_sets(dataset_loader, 12, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c10640d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:51:49.339227Z",
     "iopub.status.busy": "2024-05-06T13:51:49.338969Z",
     "iopub.status.idle": "2024-05-06T13:51:49.342923Z",
     "shell.execute_reply": "2024-05-06T13:51:49.342124Z"
    },
    "papermill": {
     "duration": 0.026084,
     "end_time": "2024-05-06T13:51:49.344754",
     "exception": false,
     "start_time": "2024-05-06T13:51:49.318670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    print('With 1 iteration:')\n",
    "    train_and_evaluate_models_per_token_sets(dataset_loader, 12, 3, 1, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5edea5df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:51:49.385239Z",
     "iopub.status.busy": "2024-05-06T13:51:49.384971Z",
     "iopub.status.idle": "2024-05-06T13:51:49.401993Z",
     "shell.execute_reply": "2024-05-06T13:51:49.401076Z"
    },
    "papermill": {
     "duration": 0.039759,
     "end_time": "2024-05-06T13:51:49.403908",
     "exception": false,
     "start_time": "2024-05-06T13:51:49.364149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_performance_for_models_per_token_sets(model_creators, evaluation_data_dict, token_evaluation_data_dict, epochs=30, use_weights=True, verbose=False, analyze_all_models=False, print_all_tokens_evaluation=False):\n",
    "    models_list = []\n",
    "    best_evaluations = {}\n",
    "    best_models = {}\n",
    "    \n",
    "    for idx, (token_set, evaluation_data) in enumerate(evaluation_data_dict.items()):\n",
    "        models_evaluations = {}\n",
    "        tokens = token_set.split(',')\n",
    "        for model_name, model_creator in model_creators.items():\n",
    "            model = model_creator(evaluation_data.days_in, evaluation_data.days_out, evaluation_data.vocab_size, evaluation_data.vocab_size//2)\n",
    "            weights = calculate_weights(evaluation_data.train_x, evaluation_data.train_y, 0.02) if use_weights else None\n",
    "            batch_size = ModelWrapper.default_batch_size\n",
    "            try:\n",
    "                train_x_with_token_input = np.concatenate((evaluation_data.train_x, np.repeat(np.expand_dims(np.expand_dims(evaluation_data.train_token, axis=-1), axis=1), 10, axis=1)), axis=-1)\n",
    "                test_x_with_token_input = np.concatenate((evaluation_data.test_x, np.repeat(np.expand_dims(np.expand_dims(evaluation_data.test_token, axis=-1), axis=1), 10, axis=1)), axis=-1)\n",
    "                history = model.fit(train_x_with_token_input, evaluation_data.train_y, test_x_with_token_input, evaluation_data.test_y, epochs=epochs, batch_size=batch_size, sample_weights=weights, verbose=verbose)\n",
    "            except:\n",
    "                pass\n",
    "            average_evaluation = 0\n",
    "            all_non_relative_errors = []\n",
    "            daily_averages_list = []\n",
    "            good_prediction_percentage_list = []\n",
    "            for token in tokens:\n",
    "                validation_x_with_token_input = np.concatenate((token_evaluation_data_dict[token].validation_x, np.repeat(np.expand_dims(np.expand_dims(token_evaluation_data_dict[token].validation_token, axis=-1), axis=1), 10, axis=1)), axis=-1)\n",
    "                evaluation, non_relative_errors, daily_averages, good_prediction_percentage = evaluate_trained_model(model, \\\n",
    "                            token_evaluation_data_dict[token].encoder, validation_x_with_token_input, \\\n",
    "                            token_evaluation_data_dict[token].validation_y, batch_size=batch_size, verbose=verbose, \\\n",
    "                            analyze_model=True, print_analysis=False)\n",
    "                all_non_relative_errors.extend(non_relative_errors)\n",
    "                daily_averages_list.append(daily_averages)\n",
    "                good_prediction_percentage_list.append(good_prediction_percentage)\n",
    "                average_evaluation += evaluation\n",
    "            average_evaluation /= len(tokens)\n",
    "            models_evaluations[model_name] = (model, history, average_evaluation, all_non_relative_errors, \\\n",
    "                                              daily_averages_list, good_prediction_percentage_list)\n",
    "        best_model_name, (best_model, best_history, best_evaluation, all_non_relative_errors, daily_averages_list, \\\n",
    "                          good_prediction_percentage_list) = min(models_evaluations.items(), key=lambda x: x[1][2])\n",
    "        for token in tokens:\n",
    "            best_models[token] = (f'{idx} - ' + best_model_name, best_history)\n",
    "            best_evaluations[token] = (best_evaluation, all_non_relative_errors, daily_averages_list, good_prediction_percentage_list)\n",
    "                    \n",
    "    new_token_sets = {}\n",
    "    for model in set(best_models.values()):\n",
    "        new_token_sets[model[0]] = []\n",
    "    \n",
    "    for token_set in evaluation_data_dict.keys():\n",
    "        tokens = token_set.split(',')\n",
    "        for token in tokens:\n",
    "            if print_all_tokens_evaluation:\n",
    "                print(f'Best Evaluation for {token}: {best_evaluations[token]*100:.2f}% from model: {best_models[token]}')\n",
    "            new_token_sets[best_models[token][0]].append(token)\n",
    "    \n",
    "    return best_evaluations, list(map(lambda x: x[1], models_list)), new_token_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2714975e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:51:49.444453Z",
     "iopub.status.busy": "2024-05-06T13:51:49.443885Z",
     "iopub.status.idle": "2024-05-06T13:51:49.457209Z",
     "shell.execute_reply": "2024-05-06T13:51:49.456547Z"
    },
    "papermill": {
     "duration": 0.035431,
     "end_time": "2024-05-06T13:51:49.459030",
     "exception": false,
     "start_time": "2024-05-06T13:51:49.423599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_models_per_token_sets_by_correlation(dataset_loader, number_of_datasets=12, analyze_clustering=False):\n",
    "    train_datasets, validation_datasets, full_datasets, fear_greed_data = prepare_data_for_tokens_sets(dataset_loader, number_of_datasets)\n",
    "    \n",
    "    token_sets = prepare_token_sets_based_on_correlation(train_datasets.keys(), 0, analyze_clustering=analyze_clustering)\n",
    "        \n",
    "    models = {\n",
    "        \"Naive\": make_naive_model,\n",
    "        \"LSTM_without_embedding\": make_lstm_model_without_embedding,\n",
    "        \"KNN\": make_knn_model,\n",
    "        \"ElasticNet\": make_elasticnet_model,\n",
    "        \"Neural network\": make_neural_network_model,\n",
    "        \"Decision tree\": make_decision_tree_model,\n",
    "        \"Gradient boosting\": lambda i, o, v, e: make_gradient_boosting_model(i, o, v, e, n_estimators=10, max_depth=4),\n",
    "        \"Extra trees\": lambda i, o, v, e: make_extra_trees_model(i, o, v, e, n_estimators=10, max_depth=4),\n",
    "        \"Random forest\": lambda i, o, v, e: make_random_forest_model(i, o, v, e, n_estimators=10, max_depth=4),\n",
    "        \"Linear regression\": make_linear_regression_model,\n",
    "    }\n",
    "    \n",
    "    evaluation_data_dict, token_evaluation_data_dict = prepare_evaluation_data_for_tokens_sets(days_in, days_out, 0, train_datasets, validation_datasets, full_datasets, fear_greed_data, token_sets)\n",
    "    best_token_assignment, _, new_token_sets = evaluate_performance_for_models_per_token_sets(models, evaluation_data_dict, token_evaluation_data_dict, 50, analyze_all_models=False)\n",
    "    token_sets = list(new_token_sets.values())\n",
    "    print(f'Token sets:')\n",
    "    print(new_token_sets)\n",
    "    avg_for_all_result = 0\n",
    "    avg_good_prediction_percentage = 0\n",
    "    number_of_tokens = 0\n",
    "    daily_averages_for_all = []\n",
    "    non_relative_errors_for_all = []\n",
    "    for model_name, tokens in new_token_sets.items():\n",
    "        print(f\"{model_name}:\")\n",
    "        avg_result = 0\n",
    "        evaluation_data = best_token_assignment[tokens[0]]\n",
    "        analyze_trained_models_for_tokens(np.array(evaluation_data[1]), np.mean(np.array(evaluation_data[2]), axis=0), \\\n",
    "                                      np.mean(evaluation_data[3]), title=\"models_per_token_sets_by_correlation\"+model_name)\n",
    "        non_relative_errors_for_all.extend(evaluation_data[1])\n",
    "        daily_averages_for_all.extend(evaluation_data[2])\n",
    "        \n",
    "        for token in tokens:\n",
    "            avg_result += best_token_assignment[token][0]\n",
    "            avg_for_all_result += best_token_assignment[token][0]\n",
    "            avg_good_prediction_percentage += np.mean(best_token_assignment[token][3])\n",
    "            number_of_tokens += 1\n",
    "        avg_result /= len(tokens)\n",
    "        print(f'Average for model {model_name}: {avg_result*100:.2f}%')\n",
    "    avg_for_all_result /= number_of_tokens\n",
    "    avg_good_prediction_percentage /= number_of_tokens\n",
    "    print(f'Average for all models: {avg_for_all_result*100:.2f}%')\n",
    "    print(f'Average good error predictions percentage for all models:', avg_good_prediction_percentage)\n",
    "    print('All models:')\n",
    "    analyze_trained_models_for_tokens(np.array(non_relative_errors_for_all), np.mean(np.array(daily_averages_for_all), axis=0), \\\n",
    "                                      avg_good_prediction_percentage, title='models_per_token_sets_by_correlation_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "85e4028a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:51:49.500016Z",
     "iopub.status.busy": "2024-05-06T13:51:49.499735Z",
     "iopub.status.idle": "2024-05-06T13:51:49.503757Z",
     "shell.execute_reply": "2024-05-06T13:51:49.502929Z"
    },
    "papermill": {
     "duration": 0.027424,
     "end_time": "2024-05-06T13:51:49.505714",
     "exception": false,
     "start_time": "2024-05-06T13:51:49.478290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    train_and_evaluate_models_per_token_sets_by_correlation(dataset_loader, 12, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53af9cc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:51:49.545995Z",
     "iopub.status.busy": "2024-05-06T13:51:49.545747Z",
     "iopub.status.idle": "2024-05-06T13:51:49.823197Z",
     "shell.execute_reply": "2024-05-06T13:51:49.822449Z"
    },
    "papermill": {
     "duration": 0.300499,
     "end_time": "2024-05-06T13:51:49.825606",
     "exception": false,
     "start_time": "2024-05-06T13:51:49.525107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Auto hyperparameter optimization\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Dimension, Real, Integer, Categorical\n",
    "\n",
    "\n",
    "class SearchWrapper:\n",
    "    \"\"\"\n",
    "    Model wrapper used in scikit-learn auto optimization \n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.params = kwargs.copy()\n",
    "        \n",
    "        creator = kwargs.pop(\"model_creator\", None)\n",
    "        if creator is None:\n",
    "            raise ValueError(\"model_creator was not specified\")\n",
    "        \n",
    "        self.model = creator(**kwargs)\n",
    "    \n",
    "    def fit(self, train_x, train_y):\n",
    "        self.model.fit(train_x, train_y, epochs=30)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x, verbose=False)\n",
    "    \n",
    "    def get_params(self, deep=False):\n",
    "        return self.params\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        return SearchWrapper(**params)\n",
    "\n",
    "\n",
    "def score_output(expected, predicted, encoder):\n",
    "    error_score = 1e10\n",
    "    \n",
    "    if np.count_nonzero(np.isnan(predicted)):\n",
    "        return error_score\n",
    "    \n",
    "    if predicted.shape != expected.shape:\n",
    "        return error_score\n",
    "    \n",
    "    score = calculate_score(encoder.decode_many(predicted), encoder.decode_many(expected))\n",
    "    if np.isnan(score):\n",
    "        print(\"calculate_score returned NaN\", flush=True)\n",
    "        \n",
    "    return score\n",
    "\n",
    "\n",
    "def perform_random_search(model_creator, parameters_dict, iterations, datasets, days_in, days_out, extractor=None, splits=4):\n",
    "    if extractor is None:\n",
    "        extractor = FeatureExtractor(datasets)\n",
    "        \n",
    "    search_x, search_y, search_encoder = datasets_to_encoded_data(datasets, days_in, days_out, extractor)\n",
    "    search_x = np.concatenate((search_x, np.repeat(np.expand_dims(np.expand_dims(tokens, axis=-1), axis=1), 10, axis=1)), axis=-1)\n",
    "\n",
    "    param_dist = {\n",
    "        \"model_creator\": [model_creator],\n",
    "        \"days_in\": [days_in],\n",
    "        \"days_out\": [days_out],\n",
    "        **parameters_dict\n",
    "    }\n",
    "\n",
    "    split = TimeSeriesSplit(n_splits=splits)\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=SearchWrapper(model_creator=lambda **kwargs: None),\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=iterations,\n",
    "        cv=split,\n",
    "        scoring=make_scorer(score_output, encoder=search_encoder, greater_is_better=False),\n",
    "        n_jobs=4,\n",
    "        refit=False,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "    random_search.fit(search_x, search_y)\n",
    "    \n",
    "    del random_search.best_params_[\"model_creator\"]\n",
    "    del random_search.best_params_[\"days_in\"]\n",
    "    del random_search.best_params_[\"days_out\"]\n",
    "    \n",
    "    return random_search\n",
    "\n",
    "\n",
    "def perform_grid_search(model_creator, parameters_dict, datasets, days_in, days_out, extractor=None, splits=4):\n",
    "    if extractor is None:\n",
    "        extractor = FeatureExtractor(datasets)\n",
    "        \n",
    "    search_x, search_y, search_encoder = datasets_to_encoded_data(datasets, days_in, days_out, extractor)\n",
    "    search_x = np.concatenate((search_x, np.repeat(np.expand_dims(np.expand_dims(tokens, axis=-1), axis=1), 10, axis=1)), axis=-1)\n",
    "\n",
    "    param_dist = {\n",
    "        \"model_creator\": [model_creator],\n",
    "        \"days_in\": [days_in],\n",
    "        \"days_out\": [days_out],\n",
    "        **parameters_dict\n",
    "    }\n",
    "\n",
    "    split = TimeSeriesSplit(n_splits=splits)\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=SearchWrapper(model_creator=lambda **kwargs: None),\n",
    "        param_grid=param_dist,\n",
    "        cv=split,\n",
    "        scoring=make_scorer(score_output, encoder=search_encoder, greater_is_better=False),\n",
    "        n_jobs=4,\n",
    "        refit=False,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "    grid_search.fit(search_x, search_y)\n",
    "    \n",
    "    del grid_search.best_params_[\"model_creator\"]\n",
    "    del grid_search.best_params_[\"days_in\"]\n",
    "    del grid_search.best_params_[\"days_out\"]\n",
    "    \n",
    "    return grid_search\n",
    "\n",
    "\n",
    "def perform_bayes_search(model_creator, spaces_dict, iterations, datasets, days_in, days_out, vocabulary_size,\n",
    "                         extractor=None, splits=4):\n",
    "    if extractor is None:\n",
    "        extractor = FeatureExtractor(datasets)\n",
    "        \n",
    "    search_x, search_y, tokens, search_encoder = datasets_to_encoded_data(datasets, days_in, days_out, extractor)\n",
    "    search_x = np.concatenate((search_x, np.repeat(np.expand_dims(np.expand_dims(tokens, axis=-1), axis=1), 10, axis=1)), axis=-1)\n",
    "\n",
    "    param_dist = {\n",
    "        \"model_creator\": [model_creator],\n",
    "        \"days_in\": [days_in],\n",
    "        \"days_out\": [days_out],\n",
    "        \"vocabulary_size\": [vocabulary_size],\n",
    "        **spaces_dict\n",
    "    }\n",
    "\n",
    "    split = TimeSeriesSplit(n_splits=splits)\n",
    "    bayes_search = BayesSearchCV(\n",
    "        estimator=SearchWrapper(model_creator=lambda **kwargs: None),\n",
    "        search_spaces=param_dist,\n",
    "        n_iter=iterations,\n",
    "        cv=split,\n",
    "        scoring=make_scorer(score_output, encoder=search_encoder, greater_is_better=False),\n",
    "        n_jobs=1,\n",
    "        refit=False,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "    bayes_search.fit(search_x, search_y)\n",
    "    \n",
    "    del bayes_search.best_params_[\"model_creator\"]\n",
    "    del bayes_search.best_params_[\"days_in\"]\n",
    "    del bayes_search.best_params_[\"days_out\"]\n",
    "    del bayes_search.best_params_[\"vocabulary_size\"]\n",
    "    \n",
    "    return bayes_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "34624786",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:51:49.866893Z",
     "iopub.status.busy": "2024-05-06T13:51:49.866570Z",
     "iopub.status.idle": "2024-05-06T13:51:49.887713Z",
     "shell.execute_reply": "2024-05-06T13:51:49.886872Z"
    },
    "papermill": {
     "duration": 0.043794,
     "end_time": "2024-05-06T13:51:49.889466",
     "exception": false,
     "start_time": "2024-05-06T13:51:49.845672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_normalized_lstm_model(days_in, days_out, vocabulary_size, embedding_dim,\n",
    "                               dropout_1=0.2, neurons_1=0.2,\n",
    "                               dropout_2=0.5, neurons_2=0.2,\n",
    "                               dropout_3=0.5, neurons_3=0.1,\n",
    "                               neurons_4=0.1,\n",
    "                               neurons_5=0.2,\n",
    "                               learning_rate=0.001, inv_beta_1=0.1, inv_beta_2=0.001,\n",
    "                               target_params_count=200000):\n",
    "    weight = 1000\n",
    "    model = make_lstm_model_without_embedding(days_in, days_out, 0, 0,\n",
    "                            dropout_1, int(neurons_1*weight), \n",
    "                            dropout_2, int(neurons_2*weight), \n",
    "                            dropout_3, int(neurons_3*weight), \n",
    "                            int(neurons_4*weight), \n",
    "                            int(neurons_5*weight), \n",
    "                            learning_rate, inv_beta_1, inv_beta_2)\n",
    "    ratio = model.model.count_params() / target_params_count\n",
    "    \n",
    "    weight /= np.sqrt(ratio)\n",
    "    return make_lstm_model_without_embedding(days_in, days_out, 0, 0,\n",
    "                           dropout_1, int(neurons_1*weight), \n",
    "                           dropout_2, int(neurons_2*weight), \n",
    "                           dropout_3, int(neurons_3*weight), \n",
    "                           int(neurons_4*weight), \n",
    "                           int(neurons_5*weight), \n",
    "                           learning_rate, inv_beta_1, inv_beta_2)\n",
    "\n",
    "def make_normalized_lstm_model_with_embedding(days_in, days_out, vocabulary_size, embedding_dim,\n",
    "                               dropout_1=0.2, neurons_1=0.2,\n",
    "                               dropout_2=0.5, neurons_2=0.2,\n",
    "                               dropout_3=0.5, neurons_3=0.1,\n",
    "                               neurons_4=0.1,\n",
    "                               neurons_5=0.2,\n",
    "                               learning_rate=0.001, inv_beta_1=0.1, inv_beta_2=0.001,\n",
    "                               target_params_count=200000):\n",
    "    weight = 1000\n",
    "    model = make_lstm_model(days_in, days_out, vocabulary_size, embedding_dim,\n",
    "                            dropout_1, int(neurons_1*weight), \n",
    "                            dropout_2, int(neurons_2*weight), \n",
    "                            dropout_3, int(neurons_3*weight), \n",
    "                            int(neurons_4*weight), \n",
    "                            int(neurons_5*weight), \n",
    "                            learning_rate, inv_beta_1, inv_beta_2)\n",
    "    ratio = model.model.count_params() / target_params_count\n",
    "    \n",
    "    weight /= np.sqrt(ratio)\n",
    "    return make_lstm_model(days_in, days_out, 0, 0,\n",
    "                           dropout_1, int(neurons_1*weight), \n",
    "                           dropout_2, int(neurons_2*weight), \n",
    "                           dropout_3, int(neurons_3*weight), \n",
    "                           int(neurons_4*weight), \n",
    "                           int(neurons_5*weight), \n",
    "                           learning_rate, inv_beta_1, inv_beta_2)\n",
    "\n",
    "def make_normalized_transformer_model(days_in, days_out, vocabulary_size, embedding_dim,\n",
    "                               dropout_1=0.2, neurons_1=0.2,\n",
    "                               dropout_2=0.5, neurons_2=0.2,\n",
    "                               dropout_3=0.5, neurons_3=0.1,\n",
    "                               neurons_4=0.1,\n",
    "                               neurons_5=0.2,\n",
    "                               learning_rate=0.001, inv_beta_1=0.1, inv_beta_2=0.001, num_heads=8,\n",
    "                               target_params_count=200000):\n",
    "    weight = 1000\n",
    "    model = make_transformer_model(days_in, days_out, 0, 0,\n",
    "                            dropout_1, int(neurons_1*weight), \n",
    "                            dropout_2, int(neurons_2*weight), \n",
    "                            dropout_3, int(neurons_3*weight), \n",
    "                            int(neurons_4*weight), \n",
    "                            int(neurons_5*weight), \n",
    "                            learning_rate, inv_beta_1, inv_beta_2, num_heads)\n",
    "    ratio = model.model.count_params() / target_params_count\n",
    "    \n",
    "    weight /= np.sqrt(ratio)\n",
    "    return make_transformer_model(days_in, days_out, vocabulary_size, embedding_dim,\n",
    "                           dropout_1, int(neurons_1*weight), \n",
    "                           dropout_2, int(neurons_2*weight), \n",
    "                           dropout_3, int(neurons_3*weight), \n",
    "                           int(neurons_4*weight), \n",
    "                           int(neurons_5*weight), \n",
    "                           learning_rate, inv_beta_1, inv_beta_2, num_heads)\n",
    "\n",
    "def make_normalized_neural_network_model(days_in, days_out, vocabulary_size, embedding_dim,\n",
    "                                         dropout_1=0.5, neurons_1=0.1,\n",
    "                                         neurons_2=0.1,\n",
    "                                         neurons_3=0.1,\n",
    "                                         neurons_4=0.1,\n",
    "                                         neurons_5=0.1,\n",
    "                                         learning_rate=0.001, inv_beta_1=0.1, inv_beta_2=0.001,\n",
    "                                         target_params_count=200000):\n",
    "    weight = 1000\n",
    "    model = make_neural_network_model(days_in, days_out, 0, 0, dropout_1,\n",
    "                                      int(neurons_1*weight), \n",
    "                                      int(neurons_2*weight), \n",
    "                                      int(neurons_3*weight), \n",
    "                                      int(neurons_4*weight), \n",
    "                                      int(neurons_5*weight), \n",
    "                                      learning_rate, inv_beta_1, inv_beta_2)\n",
    "    ratio = model.model.count_params() / target_params_count\n",
    "    \n",
    "    weight /= np.sqrt(ratio)\n",
    "    return make_neural_network_model(days_in, days_out, 0, 0, dropout_1,\n",
    "                                      int(neurons_1*weight), \n",
    "                                      int(neurons_2*weight), \n",
    "                                      int(neurons_3*weight), \n",
    "                                      int(neurons_4*weight), \n",
    "                                      int(neurons_5*weight), \n",
    "                                      learning_rate, inv_beta_1, inv_beta_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "784b129d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:51:49.930515Z",
     "iopub.status.busy": "2024-05-06T13:51:49.930013Z",
     "iopub.status.idle": "2024-05-06T13:51:50.001051Z",
     "shell.execute_reply": "2024-05-06T13:51:50.000348Z"
    },
    "papermill": {
     "duration": 0.094141,
     "end_time": "2024-05-06T13:51:50.003309",
     "exception": false,
     "start_time": "2024-05-06T13:51:49.909168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Bayes optimization\n",
    "\n",
    "from time import time\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    \"Neural network\": (\n",
    "        lambda **params: make_normalized_neural_network_model(**params, target_params_count=100000), \n",
    "        {\n",
    "            \"embedding_dim\": [0],\n",
    "            \"dropout_1\": Real(0.1, 0.9), \n",
    "            \"neurons_1\": Real(0.05, 1),\n",
    "            \"neurons_2\": Real(0.05, 1),\n",
    "            \"neurons_3\": Real(0.05, 1),\n",
    "            \"neurons_4\": Real(0.05, 1),\n",
    "            \"neurons_5\": Real(0.05, 1),\n",
    "            \"learning_rate\": Real(0.0001, 1, prior=\"log-uniform\"),\n",
    "            \"inv_beta_1\": Real(0.001, 0.5, prior=\"log-uniform\"), \n",
    "            \"inv_beta_2\": Real(0.001, 0.5, prior=\"log-uniform\")\n",
    "        }),\n",
    "    \"LSTM\": (\n",
    "        lambda **params: make_normalized_lstm_model(**params, target_params_count=100000), \n",
    "        {\n",
    "            \"embedding_dim\": [0],\n",
    "            \"dropout_1\": Real(0.1, 0.5), \n",
    "            \"neurons_1\": Real(0.05, 0.3),\n",
    "            \"dropout_2\": Real(0.1, 0.9), \n",
    "            \"neurons_2\": Real(0.05, 0.3),\n",
    "            \"dropout_3\": Real(0.1, 0.9), \n",
    "            \"neurons_3\": Real(0.05, 0.3),\n",
    "            \"neurons_4\": Real(0.05, 1),\n",
    "            \"neurons_5\": Real(0.05, 1),\n",
    "            \"learning_rate\": Real(0.0001, 1, prior=\"log-uniform\"),\n",
    "            \"inv_beta_1\": Real(0.001, 0.5, prior=\"log-uniform\"), \n",
    "            \"inv_beta_2\": Real(0.001, 0.5, prior=\"log-uniform\")\n",
    "        }),\n",
    "    \"LSTM Embedding\": (\n",
    "        lambda **params: make_normalized_lstm_model_with_embedding(**params, target_params_count=100000), \n",
    "        {\n",
    "            \"embedding_dim\": Integer(16, 256),\n",
    "            \"dropout_1\": Real(0.1, 0.5), \n",
    "            \"neurons_1\": Real(0.05, 0.3),\n",
    "            \"dropout_2\": Real(0.1, 0.9), \n",
    "            \"neurons_2\": Real(0.05, 0.3),\n",
    "            \"dropout_3\": Real(0.1, 0.9), \n",
    "            \"neurons_3\": Real(0.05, 0.3),\n",
    "            \"neurons_4\": Real(0.05, 1),\n",
    "            \"neurons_5\": Real(0.05, 1),\n",
    "            \"learning_rate\": Real(0.0001, 1, prior=\"log-uniform\"),\n",
    "            \"inv_beta_1\": Real(0.001, 0.5, prior=\"log-uniform\"), \n",
    "            \"inv_beta_2\": Real(0.001, 0.5, prior=\"log-uniform\")\n",
    "        }),\n",
    "    \"Transformer\": (\n",
    "        lambda **params: make_normalized_transformer_model(**params, target_params_count=100000), \n",
    "        {\n",
    "            \"embedding_dim\": Integer(16, 256),\n",
    "            \"dropout_1\": Real(0.1, 0.5), \n",
    "            \"neurons_1\": Real(0.05, 0.3),\n",
    "            \"dropout_2\": Real(0.1, 0.9), \n",
    "            \"neurons_2\": Real(0.05, 0.3),\n",
    "            \"dropout_3\": Real(0.1, 0.9), \n",
    "            \"neurons_3\": Real(0.05, 0.3),\n",
    "            \"neurons_4\": Real(0.05, 1),\n",
    "            \"neurons_5\": Real(0.05, 1),\n",
    "            \"learning_rate\": Real(0.0001, 1, prior=\"log-uniform\"),\n",
    "            \"inv_beta_1\": Real(0.001, 0.5, prior=\"log-uniform\"), \n",
    "            \"inv_beta_2\": Real(0.001, 0.5, prior=\"log-uniform\"),\n",
    "            \"num_heads\": Integer(1,10),\n",
    "        }),\n",
    "    \"Decision Tree\": (\n",
    "        make_decision_tree_model, \n",
    "        {\n",
    "            \"embedding_dim\": [0],\n",
    "            \"max_depth\": Integer(3, 500),\n",
    "            \"min_samples_split\": Integer(2, 2000)\n",
    "        }),\n",
    "    \"Gradient boosting\": (\n",
    "       lambda **params: make_gradient_boosting_model(**params, n_estimators=10), \n",
    "       {\n",
    "           \"embedding_dim\": [0],\n",
    "           \"max_depth\": Integer(3, 30),\n",
    "           \"min_samples_split\": Integer(2, 2000),\n",
    "           \"subsample\": Real(0.1, 1)\n",
    "       }),\n",
    "    \"Extra trees\": (\n",
    "        make_extra_trees_model, \n",
    "        {\n",
    "            \"embedding_dim\": [0],\n",
    "            \"n_estimators\": Integer(3, 100),\n",
    "            \"max_depth\": Integer(3, 20),\n",
    "            \"min_samples_split\": Integer(2, 2000)\n",
    "        }),\n",
    "    \"Random forest\": (\n",
    "       make_random_forest_model, \n",
    "       {\n",
    "           \"embedding_dim\": [0],\n",
    "           \"n_estimators\": Integer(3, 100),\n",
    "           \"max_depth\": Integer(3, 20),\n",
    "           \"min_samples_split\": Integer(2, 2000)\n",
    "       }),\n",
    "    \"KNN\": (\n",
    "        make_knn_model, \n",
    "        {\n",
    "            \"embedding_dim\": [0],\n",
    "            \"n_neighbors\": Integer(2, 100),\n",
    "            \"weights\": Categorical([\"uniform\", \"distance\"]),\n",
    "            \"p\": Integer(1, 2)\n",
    "        }),\n",
    "    \"ElasticNet\": (\n",
    "        make_elasticnet_model, \n",
    "        {\n",
    "            \"embedding_dim\": [0],\n",
    "            \"alpha\": Real(0.0001, 1, prior=\"log-uniform\"),\n",
    "            \"l1_ratio\": Real(0, 1)\n",
    "        })\n",
    "}\n",
    "\n",
    "ModelWrapper.default_batch_size = 512\n",
    "\n",
    "results = {}\n",
    "if False:\n",
    "    data = prepare_evaluation_data(days_in, days_out, dataset_loader, number_of_datasets=100)\n",
    "    for name, details in parameters.items():\n",
    "        creator, params = details\n",
    "\n",
    "        print(f\"\\n{name}:\")\n",
    "\n",
    "        start = time()\n",
    "        bayes_results = perform_bayes_search(creator, params, 2, train_datasets, days_in, days_out, data.vocab_size, extractor=extractor, splits=2)\n",
    "\n",
    "        print(f\"Finished in {(time() - start):.2f}s\")\n",
    "        print(f\"Best score: {(-bayes_results.best_score_*100):.2f}%\")\n",
    "\n",
    "        evaluation, _ = evaluate_performance(lambda i, o, v, e: creator(days_in=i, days_out=o, vocabulary_size=v, **bayes_results.best_params_), data, model_name=name)\n",
    "\n",
    "        print(f\"Evaluation score: {(evaluation*100):.2f}%\")\n",
    "        print(f\"Parameters: \\n\\t{bayes_results.best_params_}\")\n",
    "\n",
    "        results[name] = (creator, bayes_results.best_params_, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cefbe64a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:51:50.044237Z",
     "iopub.status.busy": "2024-05-06T13:51:50.043945Z",
     "iopub.status.idle": "2024-05-06T13:51:50.048472Z",
     "shell.execute_reply": "2024-05-06T13:51:50.047583Z"
    },
    "papermill": {
     "duration": 0.027279,
     "end_time": "2024-05-06T13:51:50.050497",
     "exception": false,
     "start_time": "2024-05-06T13:51:50.023218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Creating AdaBoost model for best estimator\n",
    "\n",
    "# TODO: AdaBoostRegressor from scikit-learn only accepts 1D array for y. Maybe implement it manually? \n",
    "# if False:\n",
    "#     best_result = min(results.keys(), key=lambda k: results[k][-1])\n",
    "#     print(f\"Best model: {best_result}\")\n",
    "    \n",
    "#     best_creator, best_params, _ = results[best_result]\n",
    "\n",
    "#     adaboost_model = make_adaboost_model(best_creator(days_in=days_in, days_out=days_out, **best_params), n_estimators=10, learning_rate=1.0)\n",
    "#     history = adaboost_model.fit(train_x, train_y) # TODO: Maybe pass more parameters to base model's `fit()` (how?)\n",
    "\n",
    "#     evaluation = evaluate_trained_model(adaboost_model, encoder, validation_x, validation_y, verbose=True)\n",
    "#     print(f\"AdaBoost evaluation: {(evaluation*100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5db42ee4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:51:50.090447Z",
     "iopub.status.busy": "2024-05-06T13:51:50.090170Z",
     "iopub.status.idle": "2024-05-06T13:51:50.097924Z",
     "shell.execute_reply": "2024-05-06T13:51:50.097040Z"
    },
    "papermill": {
     "duration": 0.0297,
     "end_time": "2024-05-06T13:51:50.099784",
     "exception": false,
     "start_time": "2024-05-06T13:51:50.070084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Ensemble\n",
    "\n",
    "class AverageEnsemble:\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        self.is_keras_model = False\n",
    "        \n",
    "    def fit(self, train_x, train_y, test_x, test_y, epochs=30, batch_size=None, sample_weights=None, verbose=False):\n",
    "        for name, model in self.models.items():\n",
    "            if verbose:\n",
    "                print(f\"Fitting {name}\")\n",
    "            model.fit(train_x, train_y, test_x, test_y, epochs=epochs, batch_size=batch_size, sample_weights=sample_weights, verbose=verbose)\n",
    "    \n",
    "    def predict(self, x, batch_size=None, verbose=True):\n",
    "        predictions = [model.predict(x, batch_size=batch_size, verbose=False) for model in self.models.values()]\n",
    "        return sum(predictions) / len(predictions)\n",
    "    \n",
    "    def save(self, file_name):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e99c13fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:51:50.139867Z",
     "iopub.status.busy": "2024-05-06T13:51:50.139585Z",
     "iopub.status.idle": "2024-05-06T13:51:50.159723Z",
     "shell.execute_reply": "2024-05-06T13:51:50.158876Z"
    },
    "papermill": {
     "duration": 0.042764,
     "end_time": "2024-05-06T13:51:50.161883",
     "exception": false,
     "start_time": "2024-05-06T13:51:50.119119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def errors_per_prediction(actual, expected):\n",
    "    return np.array([calculate_score([a], [e]) for a, e in zip(actual, expected, strict=True)])\n",
    "\n",
    "\n",
    "class AdaptiveEnsemble:\n",
    "    \"\"\"\n",
    "    Ensemble that uses a classifier to predict which model has the best answer for a given input.\n",
    "    'best_model_classifier' must be a scikit-learn-like classifier model (i.e. 'DecisionTreeClassifier').\n",
    "    \"\"\"\n",
    "    def __init__(self, models, best_model_classifier, encoder):\n",
    "        self.models = {i: (name, model) for i, (name, model) in zip(range(len(models)), models.items())}\n",
    "        self.best_model_classifier = best_model_classifier\n",
    "        self.is_keras_model = False\n",
    "        self.encoder = encoder\n",
    "        \n",
    "    def fit(self, train_x, train_y, test_x, test_y, epochs=30, batch_size=None, sample_weights=None, verbose=False):\n",
    "        for name, model in self.models.values():\n",
    "            if verbose:\n",
    "                print(f\"Fitting {name}\")\n",
    "            model.fit(train_x, train_y, test_x, test_y, epochs=epochs, batch_size=batch_size, sample_weights=sample_weights, verbose=verbose)\n",
    "            \n",
    "        decoded_y = self.encoder.decode_many(test_y)\n",
    "        \n",
    "        predictions = dict()\n",
    "        for index in self.models.keys():\n",
    "            name, model = self.models[index]\n",
    "            if verbose:\n",
    "                print(f\"Making predictions for {name}\")\n",
    "            predictions[index] = model.predict(test_x, batch_size=batch_size, verbose=verbose)\n",
    "            \n",
    "        errors = {index: errors_per_prediction(self.encoder.decode_many(predictions[index]), decoded_y) for index in self.models.keys()}\n",
    "        \n",
    "        best_per_sample = []\n",
    "        for i in range(len(decoded_y)):\n",
    "            sample_errors = {index: errors[index][i] for index in errors.keys()}\n",
    "            best_per_sample.append(min(sample_errors.keys(), key=lambda k: sample_errors[k]))\n",
    "            \n",
    "        best_model_classifier_x = [test_x.reshape((test_x.shape[0], -1))]\n",
    "        best_model_classifier_x.extend([predictions[i] for i in sorted(predictions.keys())])\n",
    "        stacked_x = np.column_stack(best_model_classifier_x)\n",
    "        \n",
    "        self.best_model_classifier.fit(stacked_x, best_per_sample)        \n",
    "    \n",
    "    def predict(self, x, batch_size=None, verbose=True):\n",
    "        predictions = dict()\n",
    "        for index in sorted(self.models.keys()):\n",
    "            _, model = self.models[index]\n",
    "            predictions[index] = model.predict(x, batch_size=batch_size, verbose=verbose)\n",
    "            \n",
    "        best_model_classifier_x = [x.reshape((x.shape[0], -1))]\n",
    "        best_model_classifier_x.extend([predictions[i] for i in sorted(predictions.keys())])\n",
    "        stacked_x = np.column_stack(best_model_classifier_x)\n",
    "        probabilities = self.best_model_classifier.predict_proba(stacked_x)\n",
    "        \n",
    "        return np.array([\n",
    "            sum([predictions[index][i] * probabilities[i][index] for index in sorted(predictions.keys())]) / sum(probabilities[i]) \n",
    "            for i in range(len(x))\n",
    "        ])\n",
    "    \n",
    "    def calculate_bias(self, x, batch_size=None):\n",
    "        predictions = dict()\n",
    "        for index in sorted(self.models.keys()):\n",
    "            _, model = self.models[index]\n",
    "            predictions[index] = model.predict(x, batch_size=batch_size, verbose=False)\n",
    "            \n",
    "        best_model_classifier_x = [x.reshape((x.shape[0], -1))]\n",
    "        best_model_classifier_x.extend([predictions[i] for i in sorted(predictions.keys())])\n",
    "        stacked_x = np.column_stack(best_model_classifier_x)\n",
    "        probabilities = self.best_model_classifier.predict_proba(stacked_x)\n",
    "        \n",
    "        return [sum([probabilities[i][index] for i in range(len(probabilities))]) / len(probabilities) for index in sorted(self.models.keys())]\n",
    "    \n",
    "    def save(self, file_name):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "48cf8e70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:51:50.201892Z",
     "iopub.status.busy": "2024-05-06T13:51:50.201604Z",
     "iopub.status.idle": "2024-05-06T13:51:50.212655Z",
     "shell.execute_reply": "2024-05-06T13:51:50.211764Z"
    },
    "papermill": {
     "duration": 0.033169,
     "end_time": "2024-05-06T13:51:50.214518",
     "exception": false,
     "start_time": "2024-05-06T13:51:50.181349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "if False:\n",
    "    model_names = list(results.keys())\n",
    "    model_names.sort(key=lambda name: results[name][-1])\n",
    "    ensemble_models = {name: results[name][0](days_in=days_in, days_out=days_out, vocabulary_size=data.vocab_size, **results[name][1]) for name in model_names[:3]}\n",
    "    print(f\"Models for ensemble: {list(ensemble_models.keys())}\")\n",
    "\n",
    "    data = prepare_evaluation_data(days_in, days_out, dataset_loader, 30)\n",
    "\n",
    "    ensemble_model = AdaptiveEnsemble(ensemble_models, DecisionTreeClassifier(max_depth=100), data.encoder)\n",
    "    data_train_x = np.concatenate((data.train_x, np.repeat(np.expand_dims(np.expand_dims(data.train_token, axis=-1), axis=1), 10, axis=1)), axis=-1)\n",
    "    data_test_x = np.concatenate((data.test_x, np.repeat(np.expand_dims(np.expand_dims(data.test_token, axis=-1), axis=1), 10, axis=1)), axis=-1)\n",
    "    ensemble_model.fit(data_train_x, data.train_y, data_test_x, data.test_y, batch_size=2048, verbose=False)\n",
    "    \n",
    "    data_validation_x = np.concatenate((data.validation_x, np.repeat(np.expand_dims(np.expand_dims(data.validation_token, axis=-1), axis=1), 10, axis=1)), axis=-1)\n",
    "    evaluation = evaluate_trained_model(ensemble_model, data.encoder, data_validation_x, data.validation_y, verbose=False)\n",
    "    print(f\"\\nEvaluation: {(evaluation*100):.2f}%\")\n",
    "\n",
    "    print(\"\\nIndividual models:\")\n",
    "    for name, model in ensemble_models.items():\n",
    "        evaluation = evaluate_trained_model(model, data.encoder, data_validation_x, data.validation_y, verbose=False)\n",
    "        print(f\"\\t{name}: {(evaluation*100):.2f}%\")\n",
    "\n",
    "    print(f\"\\nBias: {ensemble_model.calculate_bias(data_validation_x, batch_size=2048)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cb4b3887",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:51:50.254557Z",
     "iopub.status.busy": "2024-05-06T13:51:50.254081Z",
     "iopub.status.idle": "2024-05-06T13:51:51.758450Z",
     "shell.execute_reply": "2024-05-06T13:51:51.757673Z"
    },
    "papermill": {
     "duration": 1.526719,
     "end_time": "2024-05-06T13:51:51.760673",
     "exception": false,
     "start_time": "2024-05-06T13:51:50.233954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Autogluon\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "from autogluon.timeseries.utils.forecast import get_forecast_horizon_index_ts_dataframe\n",
    "from autogluon.timeseries.metrics.abstract import TimeSeriesScorer\n",
    "from autogluon.timeseries.splitter import ExpandingWindowSplitter\n",
    "\n",
    "\n",
    "def create_autogluon_dataframe(datasets, extractor=None, encoder=None):\n",
    "    if extractor is None:\n",
    "        extractor = FeatureExtractor(datasets)\n",
    "    \n",
    "    if encoder is None:\n",
    "        encoder = FeatureEncoder(extractor)\n",
    "        encoder.fit(datasets)\n",
    "    \n",
    "    series_dataframe, past_covariates, static_features = Features.create_dataframe_for_autogluon(datasets, extractor._general_features)\n",
    "    \n",
    "    data = TimeSeriesDataFrame.from_data_frame(\n",
    "        series_dataframe,\n",
    "        id_column=\"token_id\",\n",
    "        timestamp_column=\"date\",\n",
    "        static_features_df=static_features\n",
    "    )\n",
    "    \n",
    "    past_covariate_values = past_covariates(data.index.get_level_values(\"timestamp\"))\n",
    "    for column_name in past_covariate_values.columns:\n",
    "        data[column_name] = np.array(past_covariate_values[column_name])\n",
    "        \n",
    "    encoder.encode_merged_dataframe_inplace(data)\n",
    "        \n",
    "    data = data.convert_frequency('D')\n",
    "    for column_name in data.columns:\n",
    "        if column_name.endswith(\"_change\"):\n",
    "            data[column_name] = data[column_name].fillna(0)\n",
    "        \n",
    "    data = data.fill_missing_values()\n",
    "        \n",
    "    known_covariates = Features.dates_to_known_covariates(data.index.get_level_values(\"timestamp\"))\n",
    "    for column_name in known_covariates.columns:\n",
    "        data[column_name] = np.array(known_covariates[column_name])\n",
    "\n",
    "    return data, encoder, known_covariates\n",
    "\n",
    "\n",
    "class CustomPredictionScorer(TimeSeriesScorer):\n",
    "    greater_is_better_internal = False\n",
    "    optimum = 0.0\n",
    "    \n",
    "    def __init__(self, scaler):\n",
    "        super().__init__()\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def compute_metric(self, data_future, predictions, target, **kwargs):\n",
    "        return calculate_score([self.scaler.descale(np.array(predictions[\"mean\"]))], [self.scaler.descale(np.array(data_future[target]))])\n",
    "\n",
    "def make_autogluon_predictor(prediction_length, target, scaler, known_covariates):\n",
    "    return TimeSeriesPredictor(\n",
    "        prediction_length=prediction_length, \n",
    "        eval_metric=CustomPredictionScorer(scaler), \n",
    "        target=target,\n",
    "        known_covariates_names=known_covariates\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_autogluon_predictor(predictor, prediction_length, validation_dataframe):\n",
    "    splitter = ExpandingWindowSplitter(\n",
    "        prediction_length=prediction_length, \n",
    "        num_val_windows=len(validation_dataframe)//(len(validation_dataframe.item_ids) * prediction_length) - 1\n",
    "    )\n",
    "    \n",
    "    scores = []\n",
    "    for _, val_split in splitter.split(validation_dataframe):\n",
    "        try:\n",
    "            evaluation = predictor.evaluate(val_split, model=predictor.info()[\"best_model\"])\n",
    "            score = -evaluation[\"CustomPredictionScorer\"]\n",
    "            scores.append(score)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "    return sum(scores)/len(scores), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f23f3820",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:51:51.801720Z",
     "iopub.status.busy": "2024-05-06T13:51:51.801367Z",
     "iopub.status.idle": "2024-05-06T13:51:51.805315Z",
     "shell.execute_reply": "2024-05-06T13:51:51.804516Z"
    },
    "papermill": {
     "duration": 0.026339,
     "end_time": "2024-05-06T13:51:51.807181",
     "exception": false,
     "start_time": "2024-05-06T13:51:51.780842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_autogluon = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aed2b86b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:51:51.847372Z",
     "iopub.status.busy": "2024-05-06T13:51:51.846752Z",
     "iopub.status.idle": "2024-05-06T13:51:51.853555Z",
     "shell.execute_reply": "2024-05-06T13:51:51.852750Z"
    },
    "papermill": {
     "duration": 0.028682,
     "end_time": "2024-05-06T13:51:51.855316",
     "exception": false,
     "start_time": "2024-05-06T13:51:51.826634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_autogluon:\n",
    "    prediction_length = 5\n",
    "\n",
    "    train_datasets, validation_datasets, full_datasets, fear_greed_data = dataset_loader.get_datasets() # load_datasets(250, training_limit='2021-12-12')\n",
    "    extractor = FeatureExtractor(full_datasets, fear_greed_data)\n",
    "\n",
    "    train_data, autogluon_encoder, known_covariates = create_autogluon_dataframe(train_datasets, extractor)\n",
    "\n",
    "    predictors = {\n",
    "        \"close_change\": make_autogluon_predictor(prediction_length, \"close_change\", autogluon_encoder._close_scaler, known_covariates),\n",
    "        \"high_change\": make_autogluon_predictor(prediction_length, \"high_change\", autogluon_encoder._high_scaler, known_covariates),\n",
    "        \"low_change\": make_autogluon_predictor(prediction_length, \"low_change\", autogluon_encoder._low_scaler, known_covariates)\n",
    "    }\n",
    "\n",
    "    for name in predictors.keys():\n",
    "        print(f\"Fitting predictor for {name}...\", flush=True)\n",
    "        predictors[name] = predictors[name].fit(\n",
    "            train_data, \n",
    "            time_limit=20*60,\n",
    "            presets=\"fast_training\", # \"high_quality\",\n",
    "            num_val_windows=3,\n",
    "            refit_full=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fb1ce875",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:51:51.895104Z",
     "iopub.status.busy": "2024-05-06T13:51:51.894833Z",
     "iopub.status.idle": "2024-05-06T13:51:51.900334Z",
     "shell.execute_reply": "2024-05-06T13:51:51.899592Z"
    },
    "papermill": {
     "duration": 0.027782,
     "end_time": "2024-05-06T13:51:51.902343",
     "exception": false,
     "start_time": "2024-05-06T13:51:51.874561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_autogluon:\n",
    "    validation_data, _, _ = create_autogluon_dataframe(validation_datasets, extractor, autogluon_encoder)\n",
    "\n",
    "    evaluations = []\n",
    "    errors_all = []\n",
    "    for name, predictor in predictors.items():\n",
    "        print(f\"Evaluating predictor for {name}...\", flush=True)\n",
    "        evaluation, errors = evaluate_autogluon_predictor(predictor, prediction_length, validation_data)\n",
    "        print(f\"\\nEvaluation: {(np.mean(evaluation) * 100):.2f}%\")\n",
    "        evaluations.append(evaluation)\n",
    "        errors_all.extend(errors)\n",
    "        \n",
    "\n",
    "    print(f\"\\nEvaluation: {(np.mean(evaluations) * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2837dfdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:51:51.942632Z",
     "iopub.status.busy": "2024-05-06T13:51:51.942307Z",
     "iopub.status.idle": "2024-05-06T13:51:52.432255Z",
     "shell.execute_reply": "2024-05-06T13:51:52.431402Z"
    },
    "papermill": {
     "duration": 0.512294,
     "end_time": "2024-05-06T13:51:52.434440",
     "exception": false,
     "start_time": "2024-05-06T13:51:51.922146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Backtesting\n",
    "\n",
    "from MLTradingUtilities.Assets import Assets\n",
    "from MLTradingUtilities.Prediction import Prediction\n",
    "\n",
    "\n",
    "def simulate_parallel(datasets, encoder, model, strategy):\n",
    "    assets = Assets([key for key in datasets.keys()], initial=100)\n",
    "    features = {token: series_to_lstm_inputs(encoder.encode(dataset), days_in, days_out)[0] for token, dataset in datasets.items()}\n",
    "\n",
    "    print(\"Getting predictions...\")\n",
    "    predicted = {token: model.predict(feature, verbose=0, batch_size=2048) for token, feature in features.items()}\n",
    "    decoded = {token: np.array([encoder.decode(prediction.reshape((-1, 3))) for prediction in predictions]) for token, predictions in predicted.items()}\n",
    "    \n",
    "    print(\"Simulating days...\")\n",
    "    days = min(len(f) for f in features.values())\n",
    "    for i in range(days):       \n",
    "        results = {token: decoded_value[i] for token, decoded_value in decoded.items()}\n",
    "        last_prices = {token: (datasets[token]['High'].iloc[i+days_in], datasets[token]['Low'].iloc[i+days_in]) for token in results.keys()}\n",
    "        actions = strategy.apply(Prediction(results), assets, last_prices)\n",
    "        for action in actions:\n",
    "            action.apply(assets, *last_prices[action.token])\n",
    "        yield assets.current_return({token: dataset['Close'].iloc[i+days_in+1] for token, dataset in datasets.items()})\n",
    "        \n",
    "    print(\"\\nPositions at the end:\")\n",
    "    for token, value in assets.stocks.items():\n",
    "        if value == 0:\n",
    "            continue\n",
    "        print(f\"\\t{token}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4719aeca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:51:52.475744Z",
     "iopub.status.busy": "2024-05-06T13:51:52.475440Z",
     "iopub.status.idle": "2024-05-06T13:51:52.720966Z",
     "shell.execute_reply": "2024-05-06T13:51:52.720242Z"
    },
    "papermill": {
     "duration": 0.26851,
     "end_time": "2024-05-06T13:51:52.723419",
     "exception": false,
     "start_time": "2024-05-06T13:51:52.454909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from MLTradingUtilities.HoldStrategy import HoldStrategy\n",
    "\n",
    "if False:\n",
    "    strategy = HoldStrategy()\n",
    "    simulation = [r for r in simulate_parallel(validation_datasets, encoder, model, strategy)]\n",
    "    print(f\"Return: {simulation[-1]*100}%\")\n",
    "\n",
    "    t = [r for r in range(len(simulation))]\n",
    "    plt.plot(t, simulation)\n",
    "    plt.title(\"Return over time\")\n",
    "    plt.xlabel(\"Day\")\n",
    "    plt.xlabel(\"Return\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1c40c970",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T13:51:52.763951Z",
     "iopub.status.busy": "2024-05-06T13:51:52.763652Z",
     "iopub.status.idle": "2024-05-06T13:51:52.775715Z",
     "shell.execute_reply": "2024-05-06T13:51:52.775040Z"
    },
    "papermill": {
     "duration": 0.0341,
     "end_time": "2024-05-06T13:51:52.777478",
     "exception": false,
     "start_time": "2024-05-06T13:51:52.743378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    model.save(\"model\")\n",
    "    serialize_configuration(model, encoder, \"config.json\")\n",
    "    os.listdir(\"/kaggle/working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b21a129",
   "metadata": {
    "papermill": {
     "duration": 0.019004,
     "end_time": "2024-05-06T13:51:52.815652",
     "exception": false,
     "start_time": "2024-05-06T13:51:52.796648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1125174,
     "sourceId": 4861155,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3816952,
     "sourceId": 6614310,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4615160,
     "sourceId": 7866438,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 339.387815,
   "end_time": "2024-05-06T13:51:55.379998",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-06T13:46:15.992183",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
